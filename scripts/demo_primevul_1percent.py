#!/usr/bin/env python3
"""
æ¼”ç¤ºç‰ˆæœ¬ï¼šåœ¨Primevul 1%æ•°æ®ä¸Šè¿›è¡Œpromptè¿›åŒ–ï¼Œä½¿ç”¨æ¨¡æ‹ŸLLMå±•ç¤ºå®Œæ•´æµç¨‹
"""

import os
import sys
import json
import time
import random
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, "src")

from evoprompt.data.sampler import sample_primevul_1percent
from evoprompt.data.dataset import PrimevulDataset
from evoprompt.core.prompt_tracker import PromptTracker
from evoprompt.algorithms.differential import DifferentialEvolution
from evoprompt.algorithms.base import Population
from evoprompt.core.evaluator import Evaluator
from evoprompt.metrics.base import AccuracyMetric


class MockLLMClient:
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯ï¼Œæ¨¡æ‹ŸçœŸå®çš„promptè¿›åŒ–è¿‡ç¨‹"""

    def __init__(self):
        self.call_count = 0
        self.conversation_history = []

    def generate(self, prompt: str, **kwargs) -> str:
        self.call_count += 1

        # è®°å½•è°ƒç”¨å†å²
        self.conversation_history.append(
            {
                "call_id": self.call_count,
                "prompt": prompt[:200] + "..." if len(prompt) > 200 else prompt,
                "timestamp": datetime.now().isoformat(),
            }
        )

        # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å“åº”
        prompt_lower = prompt.lower()

        # å¦‚æœæ˜¯è¿›åŒ–ç›¸å…³çš„promptï¼ˆåŒ…å«ä¸¤ä¸ªæˆ–å¤šä¸ªpromptçš„æ¯”è¾ƒ/ç»„åˆï¼‰
        if "prompt 1" in prompt_lower and "prompt 2" in prompt_lower:
            # æ¨¡æ‹Ÿcrossover/mutationæ“ä½œ
            if "combine" in prompt_lower or "crossover" in prompt_lower:
                return self._generate_evolved_prompt("crossover")
            elif "improve" in prompt_lower or "mutant" in prompt_lower:
                return self._generate_evolved_prompt("mutation")

        # å¦‚æœæ˜¯å•ä¸ªpromptçš„æ”¹è¿›
        elif "improve" in prompt_lower or "better" in prompt_lower:
            return self._generate_evolved_prompt("improvement")

        # å¦‚æœæ˜¯ä»£ç åˆ†æï¼ˆè¯„ä¼°é˜¶æ®µï¼‰
        elif "{input}" in prompt or "analyze" in prompt_lower:
            # æ¨¡æ‹Ÿæ¼æ´åˆ†æå“åº”
            code_analysis_keywords = [
                "strcpy",
                "buffer",
                "overflow",
                "unsafe",
                "vulnerable",
                "injection",
            ]
            benign_keywords = ["safe", "secure", "printf", "return", "const"]

            # ç®€å•çš„å…³é”®è¯åŒ¹é…é€»è¾‘
            if any(keyword in prompt_lower for keyword in code_analysis_keywords):
                return "vulnerable" if random.random() > 0.3 else "benign"
            elif any(keyword in prompt_lower for keyword in benign_keywords):
                return "benign" if random.random() > 0.3 else "vulnerable"
            else:
                return "vulnerable" if random.random() > 0.5 else "benign"

        # é»˜è®¤å“åº”
        return "benign"

    def _generate_evolved_prompt(self, operation_type: str) -> str:
        """ç”Ÿæˆè¿›åŒ–åçš„prompt"""

        base_templates = [
            "Analyze this code for security vulnerabilities. Look for {focus_areas}. Respond 'vulnerable' if unsafe, 'benign' if safe:\n\nCode: {{input}}\n\nAssessment:",
            "You are a security expert. Review this code for {focus_areas}. Answer 'vulnerable' or 'benign':\n\n{{input}}\n\nResult:",
            "Examine this code snippet for security issues including {focus_areas}. Classify as 'vulnerable' or 'benign':\n\n{{input}}\n\nClassification:",
            "Security analysis required. Check for {focus_areas} in this code. Reply 'vulnerable' if issues found, 'benign' otherwise:\n\n{{input}}\n\nFinding:",
        ]

        focus_options = [
            "buffer overflows, injection attacks, and memory corruption",
            "unsafe function calls, input validation issues, and race conditions",
            "SQL injection, XSS vulnerabilities, and authentication bypass",
            "memory leaks, null pointer dereference, and bounds checking",
            "cryptographic weaknesses, privilege escalation, and data exposure",
            "path traversal, command injection, and deserialization flaws",
        ]

        # é€‰æ‹©æ¨¡æ¿å’Œç„¦ç‚¹åŒºåŸŸ
        template = random.choice(base_templates)
        focus = random.choice(focus_options)

        return template.format(focus_areas=focus)

    def batch_generate(self, prompts, **kwargs):
        return [self.generate(p, **kwargs) for p in prompts]


def create_large_mock_primevul_data(output_dir: str, total_samples: int = 10000):
    """åˆ›å»ºå¤§è§„æ¨¡æ¨¡æ‹ŸPrimevulæ•°æ®"""
    print(f"ğŸ“Š åˆ›å»ºå¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®: {total_samples} æ ·æœ¬...")

    os.makedirs(output_dir, exist_ok=True)

    # ç”Ÿæˆå„ç§ç±»å‹çš„ä»£ç æ ·æœ¬
    benign_templates = [
        "int safe_function_{i}(int a, int b) {{ return a + b; }}",
        'void print_message_{i}() {{ printf("Hello World %d\\n", {i}); }}',
        'const char* get_version_{i}() {{ return "1.{i}.0"; }}',
        "bool is_valid_{i}(int value) {{ return value > 0 && value < 1000; }}",
        "double calculate_{i}(double x) {{ return x * 2.5 + {i}; }}",
    ]

    vulnerable_templates = [
        'void unsafe_copy_{i}(char* src) {{ char buf[10]; strcpy(buf, src); printf("%s", buf); }}',
        'int process_input_{i}(char* input) {{ char buffer[{i}]; sprintf(buffer, "%s", input); return strlen(buffer); }}',
        'void handle_request_{i}(char* query) {{ system(query); printf("Executed %s\\n", query); }}',
        'char* read_file_{i}(char* filename) {{ FILE* f = fopen(filename, "r"); /* no bounds checking */ char* data = malloc(1000); fread(data, 1, 2000, f); return data; }}',
        'void authenticate_{i}(char* password) {{ if(strcmp(password, "admin{i}") == 0) {{ /* hardcoded password */ access_granted(); }} }}',
    ]

    projects = [
        "Chrome",
        "Firefox",
        "Linux",
        "OpenSSL",
        "Apache",
        "nginx",
        "MySQL",
        "PostgreSQL",
        "MongoDB",
        "Redis",
    ]
    cwes = [
        "CWE-120",
        "CWE-79",
        "CWE-89",
        "CWE-476",
        "CWE-190",
        "CWE-416",
        "CWE-787",
        "CWE-125",
        "CWE-22",
        "CWE-78",
    ]

    mock_data = []

    # ç”Ÿæˆ60%çš„benignæ ·æœ¬
    benign_count = int(total_samples * 0.6)
    for i in range(benign_count):
        template = random.choice(benign_templates)
        func_code = template.format(i=i % 1000)  # é™åˆ¶æ•°å­—å¤§å°

        sample = {
            "idx": i,
            "project": random.choice(projects),
            "commit_id": f"benign_commit_{i:06d}",
            "target": 0,  # benign
            "func": func_code,
            "cwe": [],
            "cve": "None",
            "func_hash": hash(func_code) % (2**31),
        }
        mock_data.append(sample)

    # ç”Ÿæˆ40%çš„vulnerableæ ·æœ¬
    vulnerable_count = total_samples - benign_count
    for i in range(vulnerable_count):
        template = random.choice(vulnerable_templates)
        func_code = template.format(i=i % 1000)

        sample = {
            "idx": benign_count + i,
            "project": random.choice(projects),
            "commit_id": f"vuln_commit_{i:06d}",
            "target": 1,  # vulnerable
            "func": func_code,
            "cwe": [random.choice(cwes)],
            "cve": f"CVE-2024-{10000 + i}",
            "func_hash": hash(func_code) % (2**31),
        }
        mock_data.append(sample)

    # æ‰“ä¹±æ•°æ®
    random.shuffle(mock_data)

    # ä¿å­˜æ•°æ®
    dev_file = Path(output_dir) / "dev.jsonl"
    with open(dev_file, "w", encoding="utf-8") as f:
        for item in mock_data:
            f.write(json.dumps(item) + "\n")

    print("âœ… æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ")
    print(f"   æ€»æ ·æœ¬: {total_samples}")
    print(f"   Benign: {benign_count} ({benign_count/total_samples:.1%})")
    print(f"   Vulnerable: {vulnerable_count} ({vulnerable_count/total_samples:.1%})")
    print(f"   æ–‡ä»¶: {dev_file}")

    return str(dev_file)


def run_demo_evolution():
    """è¿è¡Œæ¼”ç¤ºç‰ˆæœ¬çš„è¿›åŒ–å®éªŒ"""
    print("ğŸ§¬ Primevul 1%æ•°æ® Promptè¿›åŒ–æ¼”ç¤º")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_base = Path("./outputs/demo_primevul_1percent")
    output_base.mkdir(parents=True, exist_ok=True)

    experiment_id = f"demo_primevul_1pct_{timestamp}"
    exp_dir = output_base / experiment_id
    exp_dir.mkdir(exist_ok=True)

    try:
        # 1. åˆ›å»ºå¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®
        mock_data_dir = "./data/mock_primevul_large"
        if not os.path.exists(mock_data_dir):
            create_large_mock_primevul_data(mock_data_dir, total_samples=10000)
        else:
            print("âœ… å‘ç°å·²å­˜åœ¨çš„å¤§è§„æ¨¡æ¨¡æ‹Ÿæ•°æ®")

        # 2. è¿›è¡Œ1%å‡è¡¡é‡‡æ ·
        print("\nğŸ“Š è¿›è¡Œ1%å‡è¡¡é‡‡æ ·...")
        sample_output_dir = "./data/demo_primevul_1percent_sample"

        if not os.path.exists(sample_output_dir):
            sample_result = sample_primevul_1percent(
                primevul_dir=mock_data_dir, output_dir=sample_output_dir, seed=42
            )
        else:
            print("âœ… å‘ç°å·²å­˜åœ¨çš„é‡‡æ ·æ•°æ®")
            # è¯»å–ç»Ÿè®¡ä¿¡æ¯
            stats_file = Path(sample_output_dir) / "sampling_stats.json"
            with open(stats_file, "r") as f:
                stats = json.load(f)
            sample_result = {
                "train_samples": stats.get("sampled_total", 0) * 0.7,
                "dev_samples": stats.get("sampled_total", 0) * 0.3,
                "total_samples": stats.get("sampled_total", 0),
            }

        print(f"âœ… é‡‡æ ·å®Œæˆ: {sample_result['total_samples']} æ€»æ ·æœ¬")
        print(f"   è®­ç»ƒæ ·æœ¬: {int(sample_result['train_samples'])}")
        print(f"   å¼€å‘æ ·æœ¬: {int(sample_result['dev_samples'])}")

        # 3. è®¾ç½®è¿›åŒ–å®éªŒ
        print("\nâš™ï¸ è®¾ç½®è¿›åŒ–å®éªŒ...")

        # åˆ›å»ºæ•°æ®é›†
        dev_file = Path(sample_output_dir) / "dev.txt"
        test_file = Path(sample_output_dir) / "train.txt"

        dev_dataset = PrimevulDataset(str(dev_file), "dev")
        test_dataset = PrimevulDataset(str(test_file), "test")

        print(f"   å¼€å‘é›†æ ·æœ¬: {len(dev_dataset)}")
        print(f"   æµ‹è¯•é›†æ ·æœ¬: {len(test_dataset)}")

        # åˆ›å»ºæ¨¡æ‹ŸLLMå®¢æˆ·ç«¯
        llm_client = MockLLMClient()

        # åˆ›å»ºè¯„ä¼°å™¨
        metric = AccuracyMetric()
        evaluator = Evaluator(dataset=dev_dataset, metric=metric, llm_client=llm_client)

        # åˆ›å»ºpromptè¿½è¸ªå™¨
        tracker = PromptTracker(str(output_base), experiment_id)
        tracker.set_config(
            {
                "algorithm": "de",
                "population_size": 6,
                "max_generations": 4,
                "sample_ratio": 0.01,
                "dataset": "primevul_mock",
            }
        )

        # åˆ›å»ºç®—æ³•
        algorithm = DifferentialEvolution(
            {
                "population_size": 6,
                "max_generations": 4,
                "mutation_factor": 0.5,
                "crossover_probability": 0.8,
            }
        )

        # 4. åˆ›å»ºåˆå§‹prompts
        print("\nğŸ¯ åˆ›å»ºåˆå§‹prompts...")

        initial_prompts = [
            "Analyze this code for security vulnerabilities. Respond 'vulnerable' if unsafe, 'benign' if safe:\n\nCode: {input}\n\nAssessment:",
            "You are a security expert. Examine this code for potential security flaws. Answer 'vulnerable' or 'benign':\n\n{input}\n\nResult:",
            "Review this code for common vulnerabilities like buffer overflows and injection attacks. Classify as 'vulnerable' or 'benign':\n\n{input}\n\nClassification:",
            "Check this code for security issues. Focus on unsafe operations and input validation. Reply 'vulnerable' or 'benign':\n\n{input}\n\nFinding:",
            "Security analysis: Does this code contain exploitable vulnerabilities? Answer 'vulnerable' or 'benign':\n\n{input}\n\nAnalysis:",
            "Examine this code snippet for security weaknesses. Respond with 'vulnerable' if issues found, 'benign' otherwise:\n\n{input}\n\nVerdict:",
        ]

        print(f"   åˆå§‹promptæ•°é‡: {len(initial_prompts)}")

        # è®°å½•åˆå§‹prompts
        for i, prompt in enumerate(initial_prompts):
            tracker.log_prompt(
                prompt=prompt,
                generation=0,
                individual_id=f"initial_{i}",
                operation="initialization",
                metadata={"prompt_type": "manual_design", "index": i},
            )

        # 5. è¿è¡Œè¿›åŒ–è¿‡ç¨‹
        print("\nğŸš€ å¼€å§‹è¿›åŒ–è¿‡ç¨‹...")
        print("   ç®—æ³•: å·®åˆ†è¿›åŒ– (DE)")
        print(f"   ç§ç¾¤å¤§å°: {algorithm.population_size}")
        print(f"   è¿­ä»£æ¬¡æ•°: {algorithm.max_generations}")

        start_time = time.time()

        # åˆå§‹åŒ–ç§ç¾¤
        population = algorithm.initialize_population(initial_prompts)
        print(f"   åˆå§‹ç§ç¾¤åˆ›å»º: {len(population)} ä¸ªä¸ªä½“")

        # è¯„ä¼°åˆå§‹ç§ç¾¤
        print("\nğŸ“Š è¯„ä¼°åˆå§‹ç§ç¾¤...")
        population = algorithm.evaluate_population(population, evaluator)

        # è®°å½•åˆå§‹è¯„ä¼°ç»“æœ
        for i, individual in enumerate(population.individuals):
            tracker.log_prompt(
                prompt=individual.prompt,
                fitness=individual.fitness,
                generation=0,
                individual_id=f"init_eval_{i}",
                operation="initial_evaluation",
                metadata={"population_index": i},
            )

        # æ˜¾ç¤ºåˆå§‹é€‚åº”åº¦
        initial_fitness = [ind.fitness for ind in population.individuals]
        print(f"   åˆå§‹é€‚åº”åº¦: {[f'{f:.3f}' for f in initial_fitness]}")
        print(f"   åˆå§‹æœ€ä½³: {max(initial_fitness):.3f}")

        best_fitness_history = [max(initial_fitness)]

        # è¿›åŒ–å¾ªç¯
        for generation in range(1, algorithm.max_generations + 1):
            print(f"\nğŸ”„ ç¬¬ {generation} ä»£è¿›åŒ–...")

            generation_start = time.time()
            new_individuals = []

            # DEç®—æ³•çš„è¿›åŒ–è¿‡ç¨‹
            for i, target in enumerate(population.individuals):
                print(f"   å¤„ç†ä¸ªä½“ {i+1}/{len(population.individuals)}", end="")

                # é€‰æ‹©ä¸‰ä¸ªä¸åŒçš„ä¸ªä½“ä½œä¸ºçˆ¶ä»£
                candidates = [
                    ind for j, ind in enumerate(population.individuals) if j != i
                ]
                if len(candidates) >= 3:
                    parents = random.sample(candidates, 3)

                    # åˆ›å»ºå˜å¼‚ä¸ªä½“
                    mutant_individuals = algorithm.crossover(parents, llm_client)
                    if mutant_individuals:
                        mutant = mutant_individuals[0]

                        # è¯„ä¼°å˜å¼‚ä¸ªä½“
                        result = evaluator.evaluate(mutant.prompt)
                        mutant.fitness = result.score

                        # è®°å½•å˜å¼‚ä¸ªä½“
                        tracker.log_prompt(
                            prompt=mutant.prompt,
                            fitness=mutant.fitness,
                            generation=generation,
                            individual_id=f"gen{generation}_mutant_{i}",
                            operation="differential_evolution",
                            metadata={
                                "target_fitness": target.fitness,
                                "improvement": mutant.fitness - target.fitness
                                if target.fitness
                                else 0,
                            },
                        )

                        # é€‰æ‹©ä¿ç•™æ›´å¥½çš„ä¸ªä½“
                        if mutant.fitness > target.fitness:
                            new_individuals.append(mutant)
                            print(
                                f" âœ… æ”¹è¿› ({mutant.fitness:.3f} > {target.fitness:.3f})"
                            )
                        else:
                            new_individuals.append(target)
                            print(
                                f" âŒ ä¿æŒ ({mutant.fitness:.3f} <= {target.fitness:.3f})"
                            )
                    else:
                        new_individuals.append(target)
                        print(" âš ï¸ ç”Ÿæˆå¤±è´¥ï¼Œä¿æŒåŸä¸ªä½“")
                else:
                    new_individuals.append(target)
                    print(" âš ï¸ å€™é€‰ä¸è¶³ï¼Œä¿æŒåŸä¸ªä½“")

            # æ›´æ–°ç§ç¾¤
            population = Population(new_individuals)

            # è®°å½•è¿™ä¸€ä»£çš„ç»“æœ
            generation_fitness = [ind.fitness for ind in population.individuals]
            best_fitness = max(generation_fitness)
            best_fitness_history.append(best_fitness)

            generation_time = time.time() - generation_start
            print(f"   ç¬¬{generation}ä»£å®Œæˆ ({generation_time:.1f}ç§’)")
            print(f"   é€‚åº”åº¦: {[f'{f:.3f}' for f in generation_fitness]}")
            print(f"   æœ€ä½³é€‚åº”åº¦: {best_fitness:.3f}")
            print(f"   æ”¹è¿›ç¨‹åº¦: {best_fitness - best_fitness_history[0]:+.3f}")

        # 6. æ€»ç»“ç»“æœ
        total_time = time.time() - start_time
        final_best = population.best()

        print("\nğŸ‰ è¿›åŒ–å®Œæˆ!")
        print(f"   æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"   LLMè°ƒç”¨: {llm_client.call_count} æ¬¡")
        print(f"   æœ€ç»ˆæœ€ä½³é€‚åº”åº¦: {final_best.fitness:.4f}")
        print(f"   æ€»ä½“æ”¹è¿›: {final_best.fitness - best_fitness_history[0]:+.4f}")

        progress_str = " â†’ ".join(f"{f:.3f}" for f in best_fitness_history)
        print(f"   é€‚åº”åº¦å†ç¨‹: {progress_str}")

        # 7. ä¿å­˜è¯¦ç»†ç»“æœ
        print("\nğŸ’¾ ä¿å­˜ç»“æœ...")

        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_results = {
            "experiment_id": experiment_id,
            "best_prompt": final_best.prompt,
            "best_fitness": final_best.fitness,
            "fitness_history": best_fitness_history,
            "total_llm_calls": llm_client.call_count,
            "duration_seconds": total_time,
            "algorithm": "differential_evolution",
            "population_size": algorithm.population_size,
            "generations": algorithm.max_generations,
            "sample_info": {
                "total_samples": sample_result["total_samples"],
                "dev_samples": sample_result["dev_samples"],
                "train_samples": sample_result["train_samples"],
            },
        }

        # ä¿å­˜åˆ°è¿½è¸ªå™¨
        tracker.save_summary(final_results)

        # å¯¼å‡ºtop prompts
        tracker.export_prompts_by_fitness(str(exp_dir / "top_prompts.txt"), top_k=10)

        # ä¿å­˜LLMè°ƒç”¨å†å²
        llm_history_file = exp_dir / "llm_call_history.json"
        with open(llm_history_file, "w", encoding="utf-8") as f:
            json.dump(llm_client.conversation_history, f, indent=2, ensure_ascii=False)

        # 8. å±•ç¤ºæœ€ä½³prompt
        print("\nğŸ† æœ€ä½³è¿›åŒ–prompt:")
        print("=" * 80)
        print(final_best.prompt)
        print("=" * 80)

        # 9. æ–‡ä»¶æ€»ç»“
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        important_files = [
            "experiment_summary.json",
            "prompt_evolution.jsonl",
            "top_prompts.txt",
            "best_prompts.txt",
            "llm_call_history.json",
        ]

        for filename in important_files:
            filepath = exp_dir / filename
            if filepath.exists():
                size = filepath.stat().st_size
                print(f"   âœ… {filename} ({size} bytes)")
            else:
                print(f"   âŒ {filename} (missing)")

        print(f"\nğŸ“‚ å®éªŒç»“æœä¿å­˜åœ¨: {exp_dir}")

        return {
            "success": True,
            "experiment_dir": str(exp_dir),
            "best_fitness": final_best.fitness,
            "improvement": final_best.fitness - best_fitness_history[0],
            "llm_calls": llm_client.call_count,
            "duration": total_time,
        }

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå®éªŒå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return {"success": False, "error": str(e)}


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°
    random.seed(42)

    result = run_demo_evolution()

    if result["success"]:
        print("\nğŸŠ æ¼”ç¤ºå®Œæˆ!")
        print("âœ¨ è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å®Œæ•´çš„Primevul 1%æ•°æ®promptè¿›åŒ–æµç¨‹")
        print("ğŸ”§ åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œåªéœ€è¦è®¾ç½®OPENAI_API_KEYå³å¯è¿è¡ŒçœŸå®å®éªŒ")
        print(f"ğŸ“ˆ é€‚åº”åº¦æ”¹è¿›: {result['improvement']:+.4f}")
        print(f"âš¡ æ•ˆç‡: {result['llm_calls']} LLMè°ƒç”¨ï¼Œ{result['duration']:.1f}ç§’")
    else:
        print(f"\nğŸ’¥ æ¼”ç¤ºå¤±è´¥: {result.get('error', 'Unknown error')}")
        sys.exit(1)
