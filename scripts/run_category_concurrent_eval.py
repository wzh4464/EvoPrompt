#!/usr/bin/env python3
"""å¹¶å‘è¯„ä¼° Primevul æ¯ä¸ª CWE ç±»åˆ«çš„ LLM Prompt

æŒ‰ CWE å¤§ç±»å¹¶å‘è¿è¡Œæ£€æµ‹ï¼Œæ±‡æ€»æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½ã€‚
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any

sys.path.insert(0, "src")

from evoprompt.llm.client import create_llm_client, load_env_vars
from evoprompt.data.dataset import PrimevulDataset
from evoprompt.data.cwe_categories import CWE_MAJOR_CATEGORIES, map_cwe_to_major


def load_dataset_by_category(data_file: str) -> Dict[str, List[Dict]]:
    """æŒ‰ CWE å¤§ç±»åŠ è½½æ•°æ®é›†"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {data_file}")

    dataset = PrimevulDataset(data_file, "eval")
    samples = dataset.get_samples()

    # æŒ‰ç±»åˆ«åˆ†ç»„
    category_samples: Dict[str, List] = {cat: [] for cat in CWE_MAJOR_CATEGORIES}
    category_samples["Benign"] = []

    for sample in samples:
        target = int(sample.target)
        cwe_codes = sample.metadata.get("cwe", [])

        if target == 0:
            category_samples["Benign"].append(sample)
        else:
            category = map_cwe_to_major(cwe_codes) if cwe_codes else "Other"
            if category in category_samples:
                category_samples[category].append(sample)
            else:
                category_samples["Other"].append(sample)

    # æ‰“å°ç»Ÿè®¡
    print("\nğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for cat, samples_list in category_samples.items():
        if samples_list:
            print(f"   {cat}: {len(samples_list)} æ ·æœ¬")

    return category_samples


def evaluate_category(
    category: str,
    samples: List,
    prompt: str,
    llm_client,
    max_samples: int = None
) -> Dict[str, Any]:
    """è¯„ä¼°å•ä¸ªç±»åˆ«"""
    if not samples:
        return {"category": category, "total": 0, "correct": 0, "accuracy": 0.0}

    # é™åˆ¶æ ·æœ¬æ•°
    eval_samples = samples[:max_samples] if max_samples else samples

    correct = 0
    results = []

    for sample in eval_samples:
        code = sample.input_text
        ground_truth = "Benign" if int(sample.target) == 0 else category

        # æ„å»ºæŸ¥è¯¢
        query = prompt.replace("{input}", code)

        try:
            response = llm_client.generate(query, temperature=0.1, max_tokens=50)
            response_lower = response.lower()

            # åˆ¤æ–­é¢„æµ‹ç»“æœ
            if category == "Benign":
                is_correct = "benign" in response_lower or "safe" in response_lower
            else:
                # æ£€æŸ¥æ˜¯å¦é¢„æµ‹ä¸ºè¯¥ç±»åˆ«
                is_correct = category.lower() in response_lower or (
                    "vulnerable" in response_lower and category != "Benign"
                )

            if is_correct:
                correct += 1

            results.append({
                "ground_truth": ground_truth,
                "prediction": response[:100],
                "correct": is_correct
            })

        except Exception as e:
            results.append({
                "ground_truth": ground_truth,
                "prediction": f"ERROR: {e}",
                "correct": False
            })

    accuracy = correct / len(eval_samples) if eval_samples else 0

    return {
        "category": category,
        "total": len(eval_samples),
        "correct": correct,
        "accuracy": accuracy,
        "sample_results": results[:5]  # åªä¿ç•™å‰5ä¸ªç¤ºä¾‹
    }


def run_concurrent_evaluation(
    category_samples: Dict[str, List],
    prompt: str,
    max_workers: int = 8,
    max_samples_per_category: int = 50
) -> Dict[str, Any]:
    """å¹¶å‘è¯„ä¼°æ‰€æœ‰ç±»åˆ«"""

    load_env_vars()

    print(f"\nğŸš€ å¯åŠ¨å¹¶å‘è¯„ä¼° (workers={max_workers}, max_samples={max_samples_per_category})")
    print(f"ğŸ“ ä½¿ç”¨ Prompt:\n{prompt[:200]}...")

    results = {}
    start_time = time.time()

    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºç‹¬ç«‹çš„ LLM å®¢æˆ·ç«¯
    def eval_task(category: str, samples: List):
        client = create_llm_client()
        return evaluate_category(
            category, samples, prompt, client, max_samples_per_category
        )

    # å¹¶å‘æ‰§è¡Œ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}

        for category, samples in category_samples.items():
            if samples:  # åªå¤„ç†æœ‰æ ·æœ¬çš„ç±»åˆ«
                future = executor.submit(eval_task, category, samples)
                futures[future] = category

        # æ”¶é›†ç»“æœ
        for future in as_completed(futures):
            category = futures[future]
            try:
                result = future.result()
                results[category] = result
                print(f"   âœ… {category}: {result['accuracy']:.2%} ({result['correct']}/{result['total']})")
            except Exception as e:
                print(f"   âŒ {category}: å¤±è´¥ - {e}")
                results[category] = {"category": category, "error": str(e)}

    elapsed = time.time() - start_time

    # æ±‡æ€»ç»Ÿè®¡
    total_samples = sum(r.get("total", 0) for r in results.values())
    total_correct = sum(r.get("correct", 0) for r in results.values())
    overall_accuracy = total_correct / total_samples if total_samples > 0 else 0

    summary = {
        "timestamp": datetime.now().isoformat(),
        "elapsed_seconds": elapsed,
        "total_samples": total_samples,
        "total_correct": total_correct,
        "overall_accuracy": overall_accuracy,
        "category_results": results,
        "prompt_used": prompt
    }

    return summary


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”¥ Primevul å¹¶å‘ CWE ç±»åˆ«è¯„ä¼°")
    print("=" * 60)

    # é…ç½®
    DATA_FILE = "./data/primevul_1percent_sample/dev.txt"
    MAX_WORKERS = 8  # å¹¶å‘çº¿ç¨‹æ•°
    MAX_SAMPLES_PER_CATEGORY = 30  # æ¯ç±»æœ€å¤§æ ·æœ¬æ•°

    # é»˜è®¤ prompt (å¯ä»¥æ›¿æ¢ä¸ºä½ çš„ prompt)
    DEFAULT_PROMPT = """You are a security expert. Analyze this code and classify it into one of these CWE major categories:
- Buffer Errors: buffer overflow, out-of-bounds access
- Injection: SQL injection, command injection, XSS
- Memory Management: use-after-free, double-free, memory leak
- Pointer Dereference: null pointer, invalid pointer
- Integer Errors: integer overflow/underflow
- Concurrency Issues: race conditions
- Path Traversal: directory traversal
- Cryptography Issues: weak crypto
- Information Exposure: data leaks
- Other: other security issues
- Benign: no vulnerabilities

Code:
{input}

Category:"""

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(DATA_FILE):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_FILE}")
        print("è¯·å…ˆè¿è¡Œé‡‡æ ·è„šæœ¬ç”Ÿæˆæ•°æ®")
        return 1

    # åŠ è½½æ•°æ®
    category_samples = load_dataset_by_category(DATA_FILE)

    # è¿è¡Œå¹¶å‘è¯„ä¼°
    results = run_concurrent_evaluation(
        category_samples,
        DEFAULT_PROMPT,
        max_workers=MAX_WORKERS,
        max_samples_per_category=MAX_SAMPLES_PER_CATEGORY
    )

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"æ€»æ ·æœ¬æ•°: {results['total_samples']}")
    print(f"æ­£ç¡®æ•°: {results['total_correct']}")
    print(f"æ€»ä½“å‡†ç¡®ç‡: {results['overall_accuracy']:.2%}")
    print(f"è€—æ—¶: {results['elapsed_seconds']:.1f}ç§’")

    print("\nğŸ“ˆ å„ç±»åˆ«å‡†ç¡®ç‡:")
    for cat, res in sorted(results['category_results'].items(),
                           key=lambda x: x[1].get('accuracy', 0), reverse=True):
        if 'accuracy' in res:
            print(f"   {cat:25s}: {res['accuracy']:6.2%} ({res['correct']:3d}/{res['total']:3d})")

    # ä¿å­˜ç»“æœ
    output_file = f"./outputs/category_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs("./outputs", exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
