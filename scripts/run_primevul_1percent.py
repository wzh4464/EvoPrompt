#!/usr/bin/env python3
"""
åœ¨Primevulæ•°æ®é›†çš„1%å‡è¡¡æ ·æœ¬ä¸Šè¿è¡ŒEvoPromptï¼Œè®°å½•å®Œæ•´çš„promptæ›´æ–°è¿‡ç¨‹
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, "src")

from evoprompt.data.sampler import sample_primevul_1percent
from evoprompt.workflows import VulnerabilityDetectionWorkflow
from evoprompt.algorithms.differential import DifferentialEvolution
from evoprompt.algorithms.base import Population


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    import logging

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler("primevul_1percent_evolution.log", encoding="utf-8"),
        ],
    )

    return logging.getLogger(__name__)


def check_api_key():
    """æ£€æŸ¥APIå¯†é’¥ï¼ˆSVENé£æ ¼é…ç½®ï¼‰"""
    # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    from src.evoprompt.llm.client import load_env_vars

    load_env_vars()

    api_key = os.getenv("API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® API_KEY ç¯å¢ƒå˜é‡")
        print("   åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: API_KEY='your-api-key-here'")
        print("   æˆ–è€…è®¾ç½®ç¯å¢ƒå˜é‡: export API_KEY='your-api-key-here'")
        return None

    # æ£€æŸ¥å…¶ä»–å¿…è¦çš„é…ç½®
    api_base = os.getenv("API_BASE_URL", "https://newapi.pockgo.com/v1")
    model_name = os.getenv("MODEL_NAME", "kimi-k2-code")

    print("âœ… SVENé£æ ¼APIé…ç½®æ£€æŸ¥é€šè¿‡:")
    print(f"   API_BASE_URL: {api_base}")
    print(f"   MODEL_NAME: {model_name}")
    print(f"   API_KEY: {api_key[:10]}...")

    return api_key


def prepare_sample_data(primevul_dir: str, output_dir: str):
    """å‡†å¤‡1%å‡è¡¡é‡‡æ ·æ•°æ®"""
    print("ğŸ“Š å‡†å¤‡Primevul 1%å‡è¡¡é‡‡æ ·æ•°æ®...")

    sample_result = sample_primevul_1percent(
        primevul_dir=primevul_dir, output_dir=output_dir, seed=42
    )

    print("âœ… é‡‡æ ·å®Œæˆ!")
    print(f"   è®­ç»ƒæ ·æœ¬: {sample_result['train_samples']}")
    print(f"   å¼€å‘æ ·æœ¬: {sample_result['dev_samples']}")
    print(f"   æ€»æ ·æœ¬: {sample_result['total_samples']}")

    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    stats = sample_result["statistics"]
    print(f"   åŸå§‹æ•°æ®: {stats['total_samples']} æ ·æœ¬")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {stats['sample_ratio']:.1%}")

    for key, value in stats.items():
        if key.startswith("sampled_"):
            label = key.split("_")[1]
            print(f"   æ ‡ç­¾ {label}: {value} æ ·æœ¬")

    return sample_result


def create_detailed_config():
    """åˆ›å»ºè¯¦ç»†çš„å®éªŒé…ç½®"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    config = {
        # å®éªŒæ ‡è¯†
        "experiment_id": f"primevul_1pct_{timestamp}",
        "experiment_name": "Primevul 1% Balanced Sample Evolution",
        "timestamp": timestamp,
        # æ•°æ®é…ç½®
        "dataset": "primevul",
        "sample_ratio": 0.01,
        "balanced_sampling": True,
        # ç®—æ³•é…ç½® - ä½¿ç”¨è¾ƒå°çš„å‚æ•°é€‚åˆå°æ•°æ®é›†
        "algorithm": "de",  # å·®åˆ†è¿›åŒ–
        "population_size": 8,  # è¾ƒå°çš„ç§ç¾¤
        "max_generations": 6,  # è¾ƒå°‘çš„ä»£æ•°
        "mutation_rate": 0.15,
        "crossover_probability": 0.8,
        # LLMé…ç½®
        "llm_type": "gpt-3.5-turbo",
        "max_tokens": 200,
        "temperature": 0.7,
        # è¯„ä¼°é…ç½® - ä½¿ç”¨æ‰€æœ‰æ ·æœ¬
        "sample_size": None,  # ä½¿ç”¨æ‰€æœ‰devæ ·æœ¬
        "test_sample_size": None,  # ä½¿ç”¨æ‰€æœ‰trainæ ·æœ¬ä½œä¸ºæµ‹è¯•
        # è¾“å‡ºé…ç½®
        "output_dir": "./outputs/primevul_1percent",
        "save_population": True,
        "detailed_logging": True,
        # è¿½è¸ªé…ç½®
        "track_every_evaluation": True,
        "save_intermediate_results": True,
        "export_top_k": 10,
        # å¯ç”¨CWEå¤§ç±»æ¨¡å¼
        "use_cwe_major": True,
    }

    return config


def create_custom_initial_prompts():
    """åˆ›å»ºé’ˆå¯¹æ¼æ´æ£€æµ‹çš„åˆå§‹prompté›†åˆ"""
    initial_prompts = [
        # åŸºç¡€å®‰å…¨åˆ†æprompt
        "Analyze the following code for security vulnerabilities. Look for common issues like buffer overflows, injection attacks, memory corruption, and unsafe function usage. Respond 'vulnerable' if you find security issues, 'benign' if the code appears safe.\n\nCode to analyze:\n{input}\n\nSecurity assessment:",
        # ä¸“å®¶è§’è‰²prompt
        "You are a cybersecurity expert conducting a code security review. Examine this code snippet for potential vulnerabilities including but not limited to: SQL injection, buffer overflow, use-after-free, null pointer dereference, and input validation issues. Classify as 'vulnerable' or 'benign'.\n\n{input}\n\nYour expert assessment:",
        # ç»“æ„åŒ–åˆ†æprompt
        "Perform a systematic security analysis of this code:\n1. Check for unsafe function calls\n2. Analyze input validation\n3. Look for memory management issues\n4. Identify potential attack vectors\n\nCode: {input}\n\nBased on your analysis, is this code 'vulnerable' or 'benign'?",
        # CWEå¯¼å‘prompt
        "Review this code for Common Weakness Enumeration (CWE) patterns such as CWE-120 (buffer overflow), CWE-79 (XSS), CWE-89 (SQL injection), CWE-476 (null pointer dereference), and other security weaknesses. Answer 'vulnerable' if any CWE patterns are found, 'benign' otherwise.\n\n{input}\n\nCWE-based assessment:",
        # é˜²å¾¡æ€§prompt
        "As a security-focused code reviewer, examine this code with a defensive mindset. Consider: Are there any unsafe operations? Is input properly validated? Could this code be exploited by an attacker? Respond with 'vulnerable' for unsafe code or 'benign' for secure code.\n\nCode under review:\n{input}\n\nDefensive analysis result:",
        # ç®€æ´å®ç”¨prompt
        "Check this code for security vulnerabilities. Focus on real exploitable issues. Answer 'vulnerable' or 'benign':\n\n{input}\n\nResult:",
        # å¤šå±‚æ¬¡åˆ†æprompt
        "Evaluate this code's security on multiple levels:\n- Syntax level: unsafe functions, operations\n- Logic level: control flow vulnerabilities\n- Data level: input/output handling issues\n\nCode: {input}\n\nOverall security verdict ('vulnerable' or 'benign'):",
        # æ”»å‡»è€…è§†è§’prompt
        "Think like an attacker: could you exploit this code? Look for entry points, unsafe operations, and potential attack surfaces. If you can find a way to exploit it, answer 'vulnerable'. If not, answer 'benign'.\n\n{input}\n\nAttacker's assessment:",
    ]

    return initial_prompts


def run_evolution_with_tracking(config: dict, sample_data_dir: str):
    """è¿è¡Œå¸¦æœ‰è¯¦ç»†è¿½è¸ªçš„è¿›åŒ–è¿‡ç¨‹"""
    print(f"ğŸ§¬ å¼€å§‹Promptè¿›åŒ–å®éªŒ: {config['experiment_id']}")
    if config.get("use_cwe_major"):
        print("ğŸ” å·²å¯ç”¨ CWE å¤§ç±»æ¨¡å¼ï¼šå›ºå®šè¦æ±‚æ¨¡å‹åªè¾“å‡ºå¤§ç±»ï¼ˆæˆ–Benignï¼‰ä½œä¸ºè¯„ä¼°ä¾æ®")

    # åˆ›å»ºè‡ªå®šä¹‰å·¥ä½œæµç¨‹
    workflow = VulnerabilityDetectionWorkflow(config)

    # è®¾ç½®æ•°æ®è·¯å¾„
    config["dev_file"] = str(Path(sample_data_dir) / "dev.txt")
    config["test_file"] = str(Path(sample_data_dir) / "train.txt")  # ä½¿ç”¨trainä½œä¸ºtest

    print("ğŸ“ æ•°æ®æ–‡ä»¶:")
    print(f"   å¼€å‘é›†: {config['dev_file']}")
    print(f"   æµ‹è¯•é›†: {config['test_file']}")

    # éªŒè¯æ•°æ®æ–‡ä»¶
    for file_type, file_path in [
        ("å¼€å‘é›†", config["dev_file"]),
        ("æµ‹è¯•é›†", config["test_file"]),
    ]:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"{file_type}æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        with open(file_path, "r", encoding="utf-8") as f:
            lines = len(f.readlines())
            print(f"   {file_type}: {lines} æ ·æœ¬")

    # ä¿å­˜å®éªŒé…ç½®
    config_file = Path(workflow.exp_dir) / "experiment_config.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    # åˆ›å»ºè‡ªå®šä¹‰åˆå§‹prompts
    initial_prompts = create_custom_initial_prompts()

    # ä¿å­˜åˆå§‹prompts
    initial_prompts_file = Path(workflow.exp_dir) / "initial_prompts.txt"
    with open(initial_prompts_file, "w", encoding="utf-8") as f:
        f.write("Initial Prompts for Primevul 1% Evolution\n")
        f.write("=" * 50 + "\n\n")
        for i, prompt in enumerate(initial_prompts, 1):
            f.write(f"Prompt {i}:\n")
            f.write("-" * 20 + "\n")
            f.write(prompt + "\n\n")

    print(f"ğŸ’¾ åˆå§‹promptså·²ä¿å­˜: {initial_prompts_file}")
    print(f"ğŸ¯ ä½¿ç”¨ {len(initial_prompts)} ä¸ªåˆå§‹prompt")

    # è¿è¡Œè¿›åŒ–
    print("ğŸš€ å¼€å§‹è¿›åŒ–è¿‡ç¨‹...")
    print(f"   ç®—æ³•: {config['algorithm'].upper()}")
    print(f"   ç§ç¾¤å¤§å°: {config['population_size']}")
    print(f"   è¿­ä»£æ¬¡æ•°: {config['max_generations']}")

    start_time = time.time()

    try:
        # å‡†å¤‡æ•°æ®
        dev_dataset, test_dataset = workflow.prepare_data()

        # åˆ›å»ºç»„ä»¶
        llm_client, evaluator, algorithm = workflow.create_components(dev_dataset)

        # è®°å½•åˆå§‹promptsåˆ°è¿½è¸ªå™¨
        for i, prompt in enumerate(initial_prompts):
            workflow.prompt_tracker.log_prompt(
                prompt=prompt,
                generation=0,
                individual_id=f"initial_{i}",
                operation="initialization",
                metadata={"prompt_index": i, "prompt_type": "manual_design"},
            )

        # æ‰‹åŠ¨è¿è¡Œè¿›åŒ–ä»¥è·å¾—æ›´è¯¦ç»†çš„æ§åˆ¶
        population = algorithm.initialize_population(initial_prompts)

        print("ğŸ“Š åˆå§‹ç§ç¾¤è¯„ä¼°...")
        population = algorithm.evaluate_population(population, evaluator)

        # è®°å½•åˆå§‹ç§ç¾¤
        workflow.prompt_tracker.log_population(
            population.individuals, generation=0, operation="initial_evaluation"
        )

        best_fitness_history = []

        # è¿›åŒ–å¾ªç¯
        for generation in range(1, config["max_generations"] + 1):
            print(f"\nğŸ”„ ç¬¬ {generation} ä»£è¿›åŒ–...")

            # è®°å½•å½“å‰æœ€ä½³
            current_best = population.best()
            best_fitness_history.append(current_best.fitness)
            print(f"   å½“å‰æœ€ä½³é€‚åº”åº¦: {current_best.fitness:.4f}")

            # DEç®—æ³•ç‰¹å®šçš„è¿›åŒ–æ­¥éª¤
            if isinstance(algorithm, DifferentialEvolution):
                new_individuals = []

                for i, target_individual in enumerate(population.individuals):
                    print(f"   è¿›åŒ–ä¸ªä½“ {i+1}/{len(population.individuals)}")

                    # é€‰æ‹©ä¸‰ä¸ªä¸åŒçš„ä¸ªä½“
                    candidates = [
                        ind for j, ind in enumerate(population.individuals) if j != i
                    ]
                    if len(candidates) >= 3:
                        parents = algorithm.select_parents(Population(candidates))

                        # åˆ›å»ºè¯•éªŒå‘é‡
                        trial_individuals = algorithm.crossover(parents, llm_client)
                        if trial_individuals:
                            trial = trial_individuals[0]

                            # è¯„ä¼°è¯•éªŒå‘é‡
                            result = evaluator.evaluate(trial.prompt)
                            trial.fitness = result.score

                            # è®°å½•æ–°ä¸ªä½“
                            workflow.prompt_tracker.log_prompt(
                                prompt=trial.prompt,
                                fitness=trial.fitness,
                                generation=generation,
                                individual_id=f"gen{generation}_trial_{i}",
                                operation="differential_evolution",
                                metadata={
                                    "target_fitness": target_individual.fitness,
                                    "parent_ids": [
                                        f"gen{generation-1}_ind_{j}" for j in range(3)
                                    ],
                                },
                            )

                            # é€‰æ‹©ä¿ç•™
                            if trial.fitness > target_individual.fitness:
                                new_individuals.append(trial)
                                print(
                                    f"     âœ… æ¥å—æ–°ä¸ªä½“ (é€‚åº”åº¦: {trial.fitness:.4f} > {target_individual.fitness:.4f})"
                                )
                            else:
                                new_individuals.append(target_individual)
                                print(
                                    f"     âŒ ä¿ç•™åŸä¸ªä½“ (é€‚åº”åº¦: {trial.fitness:.4f} <= {target_individual.fitness:.4f})"
                                )
                        else:
                            new_individuals.append(target_individual)
                    else:
                        new_individuals.append(target_individual)

                population = Population(new_individuals)

            # è®°å½•è¿™ä¸€ä»£çš„ç§ç¾¤
            workflow.prompt_tracker.log_population(
                population.individuals,
                generation=generation,
                operation="generation_complete",
            )

            # ä¿å­˜ä¸­é—´ç»“æœ
            if config.get("save_intermediate_results", True):
                intermediate_file = (
                    Path(workflow.exp_dir) / f"generation_{generation}_results.json"
                )
                intermediate_results = {
                    "generation": generation,
                    "best_fitness": population.best().fitness,
                    "best_prompt": population.best().prompt,
                    "population_fitness": [
                        ind.fitness for ind in population.individuals
                    ],
                    "fitness_history": best_fitness_history,
                }
                with open(intermediate_file, "w", encoding="utf-8") as f:
                    json.dump(intermediate_results, f, indent=2, ensure_ascii=False)

        # æœ€ç»ˆç»“æœ
        final_best = population.best()
        end_time = time.time()
        duration = end_time - start_time

        final_results = {
            "best_prompt": final_best.prompt,
            "best_fitness": final_best.fitness,
            "fitness_history": best_fitness_history,
            "final_population": [ind.prompt for ind in population.individuals],
            "duration_seconds": duration,
            "total_generations": config["max_generations"],
            "algorithm": config["algorithm"],
            "population_size": config["population_size"],
        }

        print("\nğŸ‰ è¿›åŒ–å®Œæˆ!")
        print(f"   è€—æ—¶: {duration:.1f} ç§’")
        print(f"   æœ€ä½³é€‚åº”åº¦: {final_best.fitness:.4f}")
        print(
            f"   é€‚åº”åº¦æå‡: {best_fitness_history[-1] - best_fitness_history[0]:.4f}"
        )

        # ä¿å­˜ç»“æœ
        workflow.save_results(final_results, test_dataset, llm_client)

        return final_results, workflow.exp_dir

    except Exception as e:
        print(f"âŒ è¿›åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback

        traceback.print_exc()
        raise


def analyze_results(exp_dir: Path):
    """åˆ†æå®éªŒç»“æœ"""
    print("\nğŸ“Š åˆ†æå®éªŒç»“æœ...")

    # è¯»å–å®éªŒæ€»ç»“
    summary_file = exp_dir / "experiment_summary.json"
    if summary_file.exists():
        with open(summary_file, "r", encoding="utf-8") as f:
            summary = json.load(f)

        print("ğŸ“‹ å®éªŒæ€»ç»“:")
        print(f"   å®éªŒID: {summary.get('experiment_id', 'N/A')}")
        print(f"   æŒç»­æ—¶é—´: {summary.get('duration_seconds', 0):.1f} ç§’")
        print(f"   æ€»å¿«ç…§æ•°: {summary.get('total_snapshots', 0)}")
        print(f"   æœ€ä½³é€‚åº”åº¦: {summary.get('best_fitness', 0):.4f}")
        print(f"   è¿­ä»£ä»£æ•°: {summary.get('total_generations', 0)}")

    # åˆ†æpromptè¿›åŒ–è½¨è¿¹
    log_file = exp_dir / "prompt_evolution.jsonl"
    if log_file.exists():
        fitness_by_gen = {}
        operation_counts = {}

        with open(log_file, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    data = json.loads(line.strip())

                    # ç»Ÿè®¡é€‚åº”åº¦
                    if data.get("fitness") is not None:
                        gen = data["generation"]
                        if gen not in fitness_by_gen:
                            fitness_by_gen[gen] = []
                        fitness_by_gen[gen].append(data["fitness"])

                    # ç»Ÿè®¡æ“ä½œç±»å‹
                    op = data.get("operation", "unknown")
                    operation_counts[op] = operation_counts.get(op, 0) + 1

        print("\nğŸ“ˆ è¿›åŒ–è½¨è¿¹:")
        for gen in sorted(fitness_by_gen.keys()):
            fitnesses = fitness_by_gen[gen]
            avg_fitness = sum(fitnesses) / len(fitnesses)
            max_fitness = max(fitnesses)
            print(
                f"   ç¬¬{gen}ä»£: å¹³å‡é€‚åº”åº¦ {avg_fitness:.4f}, æœ€ä½³é€‚åº”åº¦ {max_fitness:.4f}"
            )

        print("\nğŸ”§ æ“ä½œç»Ÿè®¡:")
        for op, count in operation_counts.items():
            print(f"   {op}: {count} æ¬¡")

    # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    important_files = [
        "experiment_config.json",
        "initial_prompts.txt",
        "prompt_evolution.jsonl",
        "best_prompts.txt",
        "top_prompts.txt",
        "experiment_summary.json",
        "test_results.json",
    ]

    for filename in important_files:
        filepath = exp_dir / filename
        if filepath.exists():
            size = filepath.stat().st_size
            print(f"   âœ… {filename} ({size} bytes)")
        else:
            print(f"   âŒ {filename} (missing)")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ Primevul 1%æ•°æ® EvoPrompt è¿›åŒ–å®éªŒ")
    print("=" * 60)

    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()

    # æ£€æŸ¥APIå¯†é’¥
    api_key = check_api_key()
    if not api_key:
        return 1

    # é…ç½®è·¯å¾„
    primevul_dir = "./data/primevul/primevul"
    sample_output_dir = "./data/primevul_1percent_sample"

    try:
        # 1. å‡†å¤‡é‡‡æ ·æ•°æ®
        if not os.path.exists(sample_output_dir):
            if not os.path.exists(primevul_dir):
                print(f"âŒ Primevulæ•°æ®ç›®å½•ä¸å­˜åœ¨: {primevul_dir}")
                print("è¯·ç¡®ä¿Primevulæ•°æ®å·²ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®")
                return 1

            prepare_sample_data(primevul_dir, sample_output_dir)
        else:
            print(f"âœ… å‘ç°å·²å­˜åœ¨çš„é‡‡æ ·æ•°æ®: {sample_output_dir}")
            # è¯»å–å·²å­˜åœ¨çš„ç»Ÿè®¡ä¿¡æ¯
            stats_file = Path(sample_output_dir) / "sampling_stats.json"
            if stats_file.exists():
                with open(stats_file, "r", encoding="utf-8") as f:
                    stats = json.load(f)
                print(f"   æ€»æ ·æœ¬: {stats.get('total_samples', 'N/A')}")
                print(f"   é‡‡æ ·æ¯”ä¾‹: {stats.get('sample_ratio', 0):.1%}")

        print()

        # 2. åˆ›å»ºå®éªŒé…ç½®
        config = create_detailed_config()
        # SVENé£æ ¼APIé…ç½®å·²é€šè¿‡.envæ–‡ä»¶åŠ è½½ï¼Œæ— éœ€é¢å¤–é…ç½®
        config["api_key"] = api_key

        print("âš™ï¸ å®éªŒé…ç½®:")
        print(f"   å®éªŒID: {config['experiment_id']}")
        print(f"   ç®—æ³•: {config['algorithm'].upper()}")
        print(f"   ç§ç¾¤å¤§å°: {config['population_size']}")
        print(f"   è¿­ä»£æ¬¡æ•°: {config['max_generations']}")
        print(f"   LLMæ¨¡å‹: {config['llm_type']}")
        print(f"   CWEå¤§ç±»æ¨¡å¼: {config.get('use_cwe_major', False)}")
        print()

        # 3. è¿è¡Œè¿›åŒ–å®éªŒ
        results, exp_dir = run_evolution_with_tracking(config, sample_output_dir)

        # 4. åˆ†æç»“æœ
        analyze_results(exp_dir)

        print("\nâœ… å®éªŒå®Œæˆ!")
        print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {exp_dir}")
        print(f"ğŸ¯ æœ€ä½³prompté€‚åº”åº¦: {results['best_fitness']:.4f}")

        return 0

    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
