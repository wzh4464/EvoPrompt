#!/usr/bin/env python3
"""å¹¶å‘å…¨é‡è¯„ä¼° Primevul æ¯ä¸ª CWE ç±»åˆ«

æ”¯æŒ:
- å…¨é‡ JSONL æ•°æ®
- è‡ªå®šä¹‰ prompt
- å¹¶å‘åŠ é€Ÿ
- è¯¦ç»†ç»“æœè¾“å‡º
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional

sys.path.insert(0, "src")

from evoprompt.llm.client import create_llm_client, load_env_vars
from evoprompt.data.cwe_categories import CWE_MAJOR_CATEGORIES, map_cwe_to_major, canonicalize_category


def load_jsonl_by_category(data_file: str) -> Dict[str, List[Dict]]:
    """ä» JSONL æ–‡ä»¶æŒ‰ CWE å¤§ç±»åŠ è½½æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½ JSONL æ•°æ®: {data_file}")

    category_samples: Dict[str, List[Dict]] = {cat: [] for cat in CWE_MAJOR_CATEGORIES}
    category_samples["Benign"] = []

    total = 0
    with open(data_file, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                item = json.loads(line)
                total += 1

                target = int(item.get("target", 0))
                cwe_codes = item.get("cwe", [])
                if isinstance(cwe_codes, str):
                    cwe_codes = [cwe_codes] if cwe_codes else []

                if target == 0:
                    category_samples["Benign"].append(item)
                else:
                    category = map_cwe_to_major(cwe_codes) if cwe_codes else "Other"
                    if category in category_samples:
                        category_samples[category].append(item)
                    else:
                        category_samples["Other"].append(item)

            except json.JSONDecodeError:
                continue

    print(f"   æ€»æ ·æœ¬æ•°: {total}")
    return category_samples


def evaluate_single_sample(sample, prompt: str, llm_client, expected_category: str) -> Dict:
    """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
    code = sample.input_text
    query = prompt.replace("{input}", code)

    try:
        response = llm_client.generate(query, temperature=0.1, max_tokens=50)

        # è§„èŒƒåŒ–é¢„æµ‹ç»“æœ
        predicted = canonicalize_category(response)
        if predicted is None:
            if any(w in response.lower() for w in ["vulnerable", "vuln", "exploit"]):
                predicted = "Other"
            else:
                predicted = "Benign"

        is_correct = predicted == expected_category

        return {
            "expected": expected_category,
            "predicted": predicted,
            "raw_response": response[:100],
            "correct": is_correct,
            "error": None
        }
    except Exception as e:
        return {
            "expected": expected_category,
            "predicted": None,
            "raw_response": None,
            "correct": False,
            "error": str(e)
        }


def evaluate_category_batch(
    category: str,
    samples: List[Dict],
    prompt: str,
    max_samples: Optional[int] = None,
    batch_size: int = 10
) -> Dict[str, Any]:
    """æ‰¹é‡è¯„ä¼°å•ä¸ªç±»åˆ« (æ”¯æŒ dict æ ¼å¼æ ·æœ¬)"""
    if not samples:
        return {"category": category, "total": 0, "correct": 0, "accuracy": 0.0, "results": []}

    eval_samples = samples[:max_samples] if max_samples else samples
    llm_client = create_llm_client()

    results = []
    correct = 0

    # æ‰¹é‡å¤„ç†
    for i in range(0, len(eval_samples), batch_size):
        batch = eval_samples[i:i + batch_size]
        # æ”¯æŒ dict æ ¼å¼: ä½¿ç”¨ "func" å­—æ®µ
        queries = [prompt.replace("{input}", s.get("func", "")) for s in batch]

        try:
            responses = llm_client.batch_generate(
                queries, temperature=0.1, max_tokens=50, batch_size=batch_size
            )

            for sample, response in zip(batch, responses):
                predicted = canonicalize_category(response) if response != "error" else None
                if predicted is None:
                    if response and any(w in response.lower() for w in ["vulnerable", "vuln"]):
                        predicted = "Other"
                    else:
                        predicted = "Benign"

                is_correct = predicted == category
                if is_correct:
                    correct += 1

                results.append({
                    "expected": category,
                    "predicted": predicted,
                    "correct": is_correct
                })

        except Exception as e:
            # å›é€€åˆ°å•ä¸ªå¤„ç†
            for sample in batch:
                code = sample.get("func", "")
                query = prompt.replace("{input}", code)
                try:
                    response = llm_client.generate(query, temperature=0.1, max_tokens=50)
                    predicted = canonicalize_category(response)
                    if predicted is None:
                        predicted = "Benign"
                    is_correct = predicted == category
                    if is_correct:
                        correct += 1
                    results.append({"expected": category, "predicted": predicted, "correct": is_correct})
                except:
                    results.append({"expected": category, "predicted": None, "correct": False})

    accuracy = correct / len(eval_samples) if eval_samples else 0

    return {
        "category": category,
        "total": len(eval_samples),
        "correct": correct,
        "accuracy": accuracy,
        "results": results
    }


def run_full_concurrent_evaluation(
    data_file: str,
    prompt: str,
    max_workers: int = 8,
    max_samples_per_category: Optional[int] = None,
    output_dir: str = "./outputs"
) -> Dict[str, Any]:
    """å¹¶å‘å…¨é‡è¯„ä¼°"""

    load_env_vars()

    print("=" * 70)
    print("ğŸ”¥ Primevul å¹¶å‘å…¨é‡ CWE ç±»åˆ«è¯„ä¼°")
    print("=" * 70)

    # åŠ è½½æ•°æ®
    category_samples = load_jsonl_by_category(data_file)

    total_samples = sum(len(s) for s in category_samples.values())
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")

    print("\nğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for cat, samples in sorted(category_samples.items(), key=lambda x: len(x[1]), reverse=True):
        if samples:
            print(f"   {cat:25s}: {len(samples):5d} æ ·æœ¬")

    # å¹¶å‘è¯„ä¼°
    print(f"\nğŸš€ å¯åŠ¨å¹¶å‘è¯„ä¼° (workers={max_workers})")
    if max_samples_per_category:
        print(f"   æ¯ç±»æœ€å¤§æ ·æœ¬: {max_samples_per_category}")

    start_time = time.time()
    results = {}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}

        for category, samples in category_samples.items():
            if samples:
                future = executor.submit(
                    evaluate_category_batch,
                    category, samples, prompt, max_samples_per_category
                )
                futures[future] = category

        for future in as_completed(futures):
            category = futures[future]
            try:
                result = future.result()
                results[category] = result
                print(f"   âœ… {category:25s}: {result['accuracy']:6.2%} ({result['correct']:4d}/{result['total']:4d})")
            except Exception as e:
                print(f"   âŒ {category:25s}: å¤±è´¥ - {e}")
                results[category] = {"category": category, "error": str(e)}

    elapsed = time.time() - start_time

    # æ±‡æ€»
    total_evaluated = sum(r.get("total", 0) for r in results.values())
    total_correct = sum(r.get("correct", 0) for r in results.values())
    overall_accuracy = total_correct / total_evaluated if total_evaluated > 0 else 0

    # è®¡ç®— Macro-F1 (ç®€åŒ–ç‰ˆ)
    category_accuracies = [r.get("accuracy", 0) for r in results.values() if r.get("total", 0) > 0]
    macro_accuracy = sum(category_accuracies) / len(category_accuracies) if category_accuracies else 0

    summary = {
        "timestamp": datetime.now().isoformat(),
        "data_file": data_file,
        "elapsed_seconds": elapsed,
        "total_samples": total_evaluated,
        "total_correct": total_correct,
        "overall_accuracy": overall_accuracy,
        "macro_accuracy": macro_accuracy,
        "num_categories": len([r for r in results.values() if r.get("total", 0) > 0]),
        "category_results": {k: {kk: vv for kk, vv in v.items() if kk != "results"}
                            for k, v in results.items()},
        "prompt_used": prompt
    }

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    print("=" * 70)
    print(f"æ€»æ ·æœ¬æ•°: {total_evaluated}")
    print(f"æ­£ç¡®æ•°: {total_correct}")
    print(f"æ€»ä½“å‡†ç¡®ç‡ (Micro): {overall_accuracy:.2%}")
    print(f"å®å¹³å‡å‡†ç¡®ç‡ (Macro): {macro_accuracy:.2%}")
    print(f"è€—æ—¶: {elapsed:.1f}ç§’")
    print(f"ååé‡: {total_evaluated / elapsed:.1f} æ ·æœ¬/ç§’")

    print("\nğŸ“ˆ å„ç±»åˆ«å‡†ç¡®ç‡ (æŒ‰å‡†ç¡®ç‡æ’åº):")
    for cat, res in sorted(results.items(), key=lambda x: x[1].get('accuracy', 0), reverse=True):
        if res.get('total', 0) > 0:
            print(f"   {cat:25s}: {res['accuracy']:6.2%} ({res['correct']:4d}/{res['total']:4d})")

    # ä¿å­˜ç»“æœ
    os.makedirs(output_dir, exist_ok=True)
    output_file = Path(output_dir) / f"category_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

    return summary


def main():
    parser = argparse.ArgumentParser(description="å¹¶å‘è¯„ä¼° Primevul CWE ç±»åˆ«")
    parser.add_argument("--data", default="./data/primevul/primevul/primevul_valid.jsonl", help="JSONL æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--prompt-file", help="Prompt æ–‡ä»¶è·¯å¾„ (åŒ…å« {input} å ä½ç¬¦)")
    parser.add_argument("--workers", type=int, default=8, help="å¹¶å‘çº¿ç¨‹æ•°")
    parser.add_argument("--max-samples", type=int, default=None, help="æ¯ç±»æœ€å¤§æ ·æœ¬æ•° (None=å…¨é‡)")
    parser.add_argument("--output", default="./outputs", help="è¾“å‡ºç›®å½•")

    args = parser.parse_args()

    # åŠ è½½ prompt
    if args.prompt_file and os.path.exists(args.prompt_file):
        with open(args.prompt_file, "r", encoding="utf-8") as f:
            prompt = f.read()
        print(f"ğŸ“ ä»æ–‡ä»¶åŠ è½½ Prompt: {args.prompt_file}")
    else:
        prompt = """Analyze this code for security vulnerabilities. Classify into one of these CWE major categories:
- Buffer Errors, Injection, Memory Management, Pointer Dereference, Integer Errors
- Concurrency Issues, Path Traversal, Cryptography Issues, Information Exposure, Other
- Benign (no vulnerabilities)

Code:
{input}

Category:"""

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.data):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        return 1

    # è¿è¡Œè¯„ä¼°
    run_full_concurrent_evaluation(
        args.data,
        prompt,
        max_workers=args.workers,
        max_samples_per_category=args.max_samples,
        output_dir=args.output
    )

    return 0


if __name__ == "__main__":
    sys.exit(main())
