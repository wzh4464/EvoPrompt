#!/usr/bin/env python3
"""å¹¶å‘å…¨é‡è¯„ä¼°ä¸‰å±‚æ£€æµ‹å™¨

æ”¯æŒ:
- å…¨é‡ Primevul JSONL æ•°æ®
- å¹¶å‘åŠ é€Ÿ + è¿›åº¦æ¡
- ä¸‰å±‚æ£€æµ‹ (Major â†’ Middle â†’ CWE)
- 1:1:1 å¹³è¡¡é‡‡æ · (vul/other_vul/benign)
- ä¸Šçº§åˆ†ç±»ç»Ÿè®¡
"""

import os
import sys
import json
import time
import random
import argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict

sys.path.insert(0, "src")

from tqdm import tqdm
from evoprompt.llm.client import create_llm_client, load_env_vars
from evoprompt.prompts.hierarchical_three_layer import ThreeLayerPromptFactory
from evoprompt.detectors.three_layer_detector import ThreeLayerDetector
from evoprompt.data.cwe_categories import CWE_MAJOR_CATEGORIES, map_cwe_to_major

# ä¸Šçº§åˆ†ç±»æ˜ å°„
CATEGORY_TO_MAJOR = {
    "Buffer Errors": "Memory",
    "Memory Management": "Memory",
    "Pointer Dereference": "Memory",
    "Integer Errors": "Memory",
    "Injection": "Injection",
    "Concurrency Issues": "Logic",
    "Path Traversal": "Input",
    "Cryptography Issues": "Crypto",
    "Information Exposure": "Logic",
    "Other": "Logic",
    "Benign": "Benign",
}


def load_jsonl_data(data_file: str) -> List[Dict]:
    """åŠ è½½ JSONL æ•°æ®"""
    samples = []
    with open(data_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                try:
                    samples.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    return samples


def get_ground_truth_category(item: Dict) -> Tuple[str, bool]:
    """è·å–æ ·æœ¬çš„çœŸå®ç±»åˆ«"""
    target = int(item.get("target", 0))
    if target == 0:
        return "Benign", False

    cwe_codes = item.get("cwe", [])
    if isinstance(cwe_codes, str):
        cwe_codes = [cwe_codes] if cwe_codes else []

    category = map_cwe_to_major(cwe_codes) if cwe_codes else "Other"
    return category, True


def get_sample_id(item: Dict) -> str:
    """è·å–æ ·æœ¬å”¯ä¸€ID"""
    if "idx" in item:
        return str(item["idx"])
    return str(hash(item.get("func", "")[:200]))


def balanced_sample(
    category_samples: Dict[str, List[Dict]],
    target_category: str,
    n_per_type: int
) -> List[Tuple[Dict, str]]:
    """1:1:1 å¹³è¡¡é‡‡æ ·: target_vul / other_vul / benign"""
    result = []

    # 1. Target category samples
    target_samples = category_samples.get(target_category, [])
    sampled = random.sample(target_samples, min(n_per_type, len(target_samples)))
    result.extend([(s, target_category) for s in sampled])

    # 2. Other vulnerable samples
    other_vul = []
    for cat, samples in category_samples.items():
        if cat != target_category and cat != "Benign":
            other_vul.extend([(s, cat) for s in samples])
    if other_vul:
        sampled = random.sample(other_vul, min(n_per_type, len(other_vul)))
        result.extend(sampled)

    # 3. Benign samples
    benign = category_samples.get("Benign", [])
    sampled = random.sample(benign, min(n_per_type, len(benign)))
    result.extend([(s, "Benign") for s in sampled])

    random.shuffle(result)
    return result


def evaluate_single_sample(
    item: Dict,
    prompt_set,
    expected_category: str,
    use_scale: bool = False
) -> Dict:
    """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
    llm_client = create_llm_client()
    detector = ThreeLayerDetector(
        prompt_set=prompt_set,
        llm_client=llm_client,
        use_scale_enhancement=use_scale
    )

    code = item.get("func", "")
    expected_major = CATEGORY_TO_MAJOR.get(expected_category, "Logic")

    try:
        _, details = detector.detect(code, return_intermediate=True)

        layer1_pred = details.get("layer1", "Unknown")
        layer2_pred = details.get("layer2", "Unknown")
        layer3_pred = details.get("layer3", "Unknown")

        layer1_correct = layer1_pred == expected_major

        return {
            "expected_category": expected_category,
            "expected_major": expected_major,
            "layer1_pred": layer1_pred,
            "layer2_pred": layer2_pred,
            "layer3_pred": layer3_pred,
            "layer1_correct": layer1_correct,
            "error": None
        }

    except Exception as e:
        return {
            "expected_category": expected_category,
            "expected_major": expected_major,
            "layer1_pred": None,
            "layer2_pred": None,
            "layer3_pred": None,
            "layer1_correct": False,
            "error": str(e)
        }


def load_checkpoint(checkpoint_file: str) -> Dict[str, Dict]:
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    completed = {}
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        record = json.loads(line)
                        completed[record["sample_id"]] = record
                    except Exception:
                        continue
    return completed


def save_checkpoint(checkpoint_file: str, sample_id: str, result: Dict):
    """è¿½åŠ ä¿å­˜æ£€æŸ¥ç‚¹"""
    record = {"sample_id": sample_id, **result}
    with open(checkpoint_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")


def run_concurrent_evaluation(
    data_file: str,
    max_workers: int = 64,
    max_samples_per_category: Optional[int] = None,
    output_dir: str = "./outputs",
    use_scale: bool = False,
    resume: bool = False,
    balanced: bool = False,
    n_per_type: int = 100
) -> Dict[str, Any]:
    """å¹¶å‘å…¨é‡è¯„ä¼°"""

    load_env_vars()

    print("=" * 70)
    print("ğŸ”¥ ä¸‰å±‚æ£€æµ‹å™¨å¹¶å‘å…¨é‡è¯„ä¼°")
    if use_scale:
        print("   ğŸ“Š SCALE Enhancement: ENABLED")
    if balanced:
        print(f"   âš–ï¸  Balanced Sampling: {n_per_type} per type (1:1:1)")
    print("=" * 70)

    # æ£€æŸ¥ç‚¹
    os.makedirs(output_dir, exist_ok=True)
    checkpoint_file = Path(output_dir) / "eval_checkpoint.jsonl"

    completed_ids = set()
    completed_results = {}
    if resume and checkpoint_file.exists():
        completed_results = load_checkpoint(str(checkpoint_file))
        completed_ids = set(completed_results.keys())
        print(f"   ğŸ”„ Resume: å·²åŠ è½½ {len(completed_ids)} ä¸ªå·²å®Œæˆæ ·æœ¬")
    elif not resume and checkpoint_file.exists():
        checkpoint_file.unlink()
        print("   ğŸ—‘ï¸  æ¸…ç©ºæ—§æ£€æŸ¥ç‚¹")

    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {data_file}")
    samples = load_jsonl_data(data_file)
    print(f"   æ€»æ ·æœ¬æ•°: {len(samples)}")

    # æŒ‰ç±»åˆ«åˆ†ç»„
    category_samples: Dict[str, List[Dict]] = defaultdict(list)
    for item in samples:
        category, _ = get_ground_truth_category(item)
        category_samples[category].append(item)

    print("\nğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for cat, cat_samples in sorted(category_samples.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"   {cat:25s}: {len(cat_samples):5d} æ ·æœ¬")

    # åˆ›å»º prompt set
    prompt_set = ThreeLayerPromptFactory.create_default_prompt_set()

    # å‡†å¤‡è¯„ä¼°ä»»åŠ¡
    eval_tasks = []
    if balanced:
        # å¯¹æ¯ä¸ªæ¼æ´ç±»åˆ«åš 1:1:1 é‡‡æ ·
        for target_cat in category_samples.keys():
            if target_cat != "Benign":
                sampled = balanced_sample(category_samples, target_cat, n_per_type)
                eval_tasks.extend(sampled)
    else:
        # å…¨é‡è¯„ä¼°
        for cat, cat_samples in category_samples.items():
            samples_to_eval = cat_samples[:max_samples_per_category] if max_samples_per_category else cat_samples
            eval_tasks.extend([(s, cat) for s in samples_to_eval])

    # è¿‡æ»¤å·²å®Œæˆ
    pending_tasks = [(item, cat) for item, cat in eval_tasks if get_sample_id(item) not in completed_ids]

    print(f"\nğŸš€ å¯åŠ¨å¹¶å‘è¯„ä¼° (workers={max_workers})")
    print(f"   å¾…è¯„ä¼°: {len(pending_tasks)} / æ€»ä»»åŠ¡: {len(eval_tasks)}")

    # ç»Ÿè®¡ç»“æ„
    major_stats = defaultdict(lambda: {"total": 0, "correct": 0})
    category_stats = defaultdict(lambda: {"total": 0, "correct": 0})

    # åŠ è½½å·²å®Œæˆçš„ç»Ÿè®¡
    for record in completed_results.values():
        cat = record.get("expected_category", "Unknown")
        major = record.get("expected_major", CATEGORY_TO_MAJOR.get(cat, "Logic"))
        correct = record.get("layer1_correct", False)

        category_stats[cat]["total"] += 1
        category_stats[cat]["correct"] += 1 if correct else 0
        major_stats[major]["total"] += 1
        major_stats[major]["correct"] += 1 if correct else 0

    start_time = time.time()

    # å¹¶å‘è¯„ä¼° + è¿›åº¦æ¡
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for item, cat in pending_tasks:
            future = executor.submit(evaluate_single_sample, item, prompt_set, cat, use_scale)
            futures[future] = (item, cat)

        with tqdm(total=len(pending_tasks), desc="è¯„ä¼°è¿›åº¦", unit="æ ·æœ¬") as pbar:
            for future in as_completed(futures):
                item, cat = futures[future]
                sample_id = get_sample_id(item)

                try:
                    result = future.result()

                    # æ›´æ–°ç»Ÿè®¡
                    major = result.get("expected_major", "Logic")
                    correct = result.get("layer1_correct", False)

                    category_stats[cat]["total"] += 1
                    category_stats[cat]["correct"] += 1 if correct else 0
                    major_stats[major]["total"] += 1
                    major_stats[major]["correct"] += 1 if correct else 0

                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    save_checkpoint(str(checkpoint_file), sample_id, result)

                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    total_correct = sum(s["correct"] for s in major_stats.values())
                    total_eval = sum(s["total"] for s in major_stats.values())
                    acc = total_correct / total_eval if total_eval > 0 else 0
                    pbar.set_postfix({"acc": f"{acc:.1%}"})

                except Exception as e:
                    tqdm.write(f"âŒ æ ·æœ¬ {sample_id} å¤±è´¥: {e}")

                pbar.update(1)

    elapsed = time.time() - start_time

    # æ±‡æ€»ç»“æœ
    total_evaluated = sum(s["total"] for s in category_stats.values())
    total_correct = sum(s["correct"] for s in category_stats.values())
    overall_accuracy = total_correct / total_evaluated if total_evaluated > 0 else 0

    # è®¡ç®—å„ç±»å‡†ç¡®ç‡
    category_accuracies = []
    for cat, stats in category_stats.items():
        if stats["total"] > 0:
            category_accuracies.append(stats["correct"] / stats["total"])
    macro_accuracy = sum(category_accuracies) / len(category_accuracies) if category_accuracies else 0

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š ä¸‰å±‚æ£€æµ‹è¯„ä¼°ç»“æœ")
    print("=" * 70)
    print(f"æ€»æ ·æœ¬æ•°: {total_evaluated}")
    print(f"Layer1 æ­£ç¡®æ•°: {total_correct}")
    print(f"Layer1 å‡†ç¡®ç‡ (Micro): {overall_accuracy:.2%}")
    print(f"Layer1 å®å¹³å‡å‡†ç¡®ç‡ (Macro): {macro_accuracy:.2%}")
    print(f"è€—æ—¶: {elapsed:.1f}ç§’")
    if elapsed > 0:
        print(f"ååé‡: {len(pending_tasks) / elapsed:.1f} æ ·æœ¬/ç§’")

    # ä¸Šçº§åˆ†ç±»ç»Ÿè®¡
    print("\nğŸ“ˆ ä¸Šçº§åˆ†ç±» (Major) å‡†ç¡®ç‡:")
    for major in ["Memory", "Injection", "Input", "Crypto", "Logic", "Benign"]:
        stats = major_stats.get(major, {"total": 0, "correct": 0})
        if stats["total"] > 0:
            acc = stats["correct"] / stats["total"]
            print(f"   {major:12s}: {acc:6.2%} ({stats['correct']:4d}/{stats['total']:4d})")

    # ç»†åˆ†ç±»åˆ«ç»Ÿè®¡
    print("\nğŸ“ˆ ç»†åˆ†ç±»åˆ«å‡†ç¡®ç‡:")
    sorted_cats = sorted(category_stats.items(), key=lambda x: x[1]["correct"] / max(x[1]["total"], 1), reverse=True)
    for cat, stats in sorted_cats:
        if stats["total"] > 0:
            acc = stats["correct"] / stats["total"]
            print(f"   {cat:25s}: {acc:6.2%} ({stats['correct']:4d}/{stats['total']:4d})")

    # ä¿å­˜ç»“æœ
    summary = {
        "timestamp": datetime.now().isoformat(),
        "data_file": data_file,
        "use_scale": use_scale,
        "balanced": balanced,
        "elapsed_seconds": elapsed,
        "total_samples": total_evaluated,
        "total_correct": total_correct,
        "overall_accuracy": overall_accuracy,
        "macro_accuracy": macro_accuracy,
        "major_stats": {k: dict(v) for k, v in major_stats.items()},
        "category_stats": {k: dict(v) for k, v in category_stats.items()},
    }

    output_file = Path(output_dir) / f"three_layer_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

    return summary


def main():
    parser = argparse.ArgumentParser(description="ä¸‰å±‚æ£€æµ‹å™¨å¹¶å‘è¯„ä¼°")
    parser.add_argument("--data", default="./data/primevul/primevul/primevul_valid.jsonl", help="JSONL æ•°æ®æ–‡ä»¶")
    parser.add_argument("--workers", type=int, default=64, help="å¹¶å‘çº¿ç¨‹æ•°")
    parser.add_argument("--max-samples", type=int, default=None, help="æ¯ç±»æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--output", default="./outputs", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--use-scale", action="store_true", help="å¯ç”¨ SCALE Enhancement")
    parser.add_argument("--resume", action="store_true", help="ä»æ£€æŸ¥ç‚¹æ¢å¤")
    parser.add_argument("--balanced", action="store_true", help="å¯ç”¨ 1:1:1 å¹³è¡¡é‡‡æ ·")
    parser.add_argument("--n-per-type", type=int, default=100, help="å¹³è¡¡é‡‡æ ·æ—¶æ¯ç±»æ ·æœ¬æ•°")

    args = parser.parse_args()

    if not os.path.exists(args.data):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        return 1

    run_concurrent_evaluation(
        args.data,
        max_workers=args.workers,
        max_samples_per_category=args.max_samples,
        output_dir=args.output,
        use_scale=args.use_scale,
        resume=args.resume,
        balanced=args.balanced,
        n_per_type=args.n_per_type
    )

    return 0


if __name__ == "__main__":
    sys.exit(main())
