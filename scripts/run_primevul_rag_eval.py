#!/usr/bin/env python3
"""RAG å¢å¼ºçš„ Primevul å¹¶å‘è¯„ä¼°

1. ä» Primevul æ•°æ®é›†ä¸ºæ¯ä¸ª CWE å¤§ç±»æ„å»º one-shot çŸ¥è¯†åº“
2. ä½¿ç”¨ top-k RAG æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
3. å°†æ¡ˆä¾‹ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ LLM è¾…åŠ©åˆ¤æ–­
"""

import os
import sys
import json
import time
import random
import argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional

sys.path.insert(0, "src")

from evoprompt.llm.client import create_llm_client, load_env_vars
from evoprompt.data.cwe_categories import CWE_MAJOR_CATEGORIES, map_cwe_to_major, canonicalize_category
from evoprompt.rag.knowledge_base import KnowledgeBase, CodeExample
from evoprompt.rag.retriever import CodeSimilarityRetriever


def build_kb_from_primevul(
    data_file: str,
    samples_per_category: int = 2,
    max_code_length: int = 1000
) -> KnowledgeBase:
    """ä» Primevul JSONL æ„å»ºçŸ¥è¯†åº“ï¼Œæ¯ä¸ª CWE å¤§ç±»è‡³å°‘ä¸€ä¸ªæ¡ˆä¾‹"""

    print(f"ğŸ“š ä» {data_file} æ„å»ºçŸ¥è¯†åº“...")

    # æŒ‰ç±»åˆ«æ”¶é›†æ ·æœ¬
    category_samples: Dict[str, List[Dict]] = {cat: [] for cat in CWE_MAJOR_CATEGORIES}

    with open(data_file, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                item = json.loads(line)
                target = int(item.get("target", 0))

                # åªæ”¶é›†æ¼æ´æ ·æœ¬
                if target != 1:
                    continue

                cwe_codes = item.get("cwe", [])
                if isinstance(cwe_codes, str):
                    cwe_codes = [cwe_codes] if cwe_codes else []

                category = map_cwe_to_major(cwe_codes) if cwe_codes else "Other"

                if category in category_samples:
                    # é™åˆ¶ä»£ç é•¿åº¦
                    code = item.get("func", "")[:max_code_length]
                    if len(code) > 100:  # è¿‡æ»¤å¤ªçŸ­çš„ä»£ç 
                        category_samples[category].append({
                            "code": code,
                            "cwe": cwe_codes,
                            "category": category
                        })

            except json.JSONDecodeError:
                continue

    # æ„å»ºçŸ¥è¯†åº“
    kb = KnowledgeBase()

    for category, samples in category_samples.items():
        if not samples:
            print(f"   âš ï¸ {category}: æ— æ ·æœ¬")
            continue

        # éšæœºé€‰æ‹©æ ·æœ¬
        selected = random.sample(samples, min(samples_per_category, len(samples)))

        for sample in selected:
            kb.major_examples.setdefault(category, []).append(
                CodeExample(
                    code=sample["code"],
                    category=category,
                    description=f"{category} vulnerability example",
                    cwe=sample["cwe"][0] if sample["cwe"] else None
                )
            )

        print(f"   âœ… {category}: {len(selected)} ä¸ªæ¡ˆä¾‹")

    # æ·»åŠ  Benign æ¡ˆä¾‹
    kb.major_examples["Benign"] = [
        CodeExample(
            code="int add(int a, int b) { return a + b; }",
            category="Benign",
            description="Safe arithmetic operation"
        )
    ]

    print(f"\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡: {kb.statistics()}")
    return kb


def evaluate_with_rag(
    code: str,
    category: str,
    retriever: CodeSimilarityRetriever,
    llm_client,
    base_prompt: str,
    top_k: int = 3
) -> Dict:
    """ä½¿ç”¨ RAG å¢å¼ºè¯„ä¼°å•ä¸ªæ ·æœ¬"""

    # æ£€ç´¢ç›¸ä¼¼æ¡ˆä¾‹
    retrieval = retriever.retrieve_for_major_category(code, top_k=top_k)

    # æ„å»ºå¢å¼º prompt
    if retrieval.formatted_text:
        enhanced_prompt = f"{retrieval.formatted_text}\n\n{base_prompt}"
    else:
        enhanced_prompt = base_prompt

    query = enhanced_prompt.replace("{input}", code[:2000])  # é™åˆ¶ä»£ç é•¿åº¦

    try:
        response = llm_client.generate(query, temperature=0.1, max_tokens=50)

        predicted = canonicalize_category(response)
        if predicted is None:
            if any(w in response.lower() for w in ["vulnerable", "vuln"]):
                predicted = "Other"
            else:
                predicted = "Benign"

        is_correct = predicted == category

        return {
            "expected": category,
            "predicted": predicted,
            "correct": is_correct,
            "num_examples_used": len(retrieval.examples),
            "raw_response": response[:100]
        }

    except Exception as e:
        return {
            "expected": category,
            "predicted": None,
            "correct": False,
            "error": str(e)
        }


def evaluate_category_with_rag(
    category: str,
    samples: List[Dict],
    retriever: CodeSimilarityRetriever,
    base_prompt: str,
    max_samples: Optional[int] = None,
    top_k: int = 3
) -> Dict[str, Any]:
    """ä½¿ç”¨ RAG è¯„ä¼°å•ä¸ªç±»åˆ«"""

    if not samples:
        return {"category": category, "total": 0, "correct": 0, "accuracy": 0.0}

    eval_samples = samples[:max_samples] if max_samples else samples
    llm_client = create_llm_client()

    correct = 0
    results = []

    for sample in eval_samples:
        code = sample.get("func", "")
        result = evaluate_with_rag(code, category, retriever, llm_client, base_prompt, top_k)

        if result.get("correct"):
            correct += 1
        results.append(result)

    accuracy = correct / len(eval_samples) if eval_samples else 0

    return {
        "category": category,
        "total": len(eval_samples),
        "correct": correct,
        "accuracy": accuracy,
        "results": results[:5]  # åªä¿ç•™å‰5ä¸ªç¤ºä¾‹
    }


def run_rag_concurrent_evaluation(
    data_file: str,
    kb: KnowledgeBase,
    base_prompt: str,
    max_workers: int = 8,
    max_samples_per_category: Optional[int] = None,
    top_k: int = 3,
    output_dir: str = "./outputs",
    debug_rag: bool = False
) -> Dict[str, Any]:
    """å¹¶å‘ RAG å¢å¼ºè¯„ä¼°"""

    load_env_vars()

    print("=" * 70)
    print("ğŸ”¥ RAG å¢å¼º Primevul å¹¶å‘è¯„ä¼°")
    print("=" * 70)

    # åˆ›å»º retriever (with debug flag)
    retriever = CodeSimilarityRetriever(kb, debug=debug_rag)

    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {data_file}")
    category_samples: Dict[str, List[Dict]] = {cat: [] for cat in CWE_MAJOR_CATEGORIES}
    category_samples["Benign"] = []

    total = 0
    with open(data_file, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                item = json.loads(line)
                total += 1

                target = int(item.get("target", 0))
                cwe_codes = item.get("cwe", [])
                if isinstance(cwe_codes, str):
                    cwe_codes = [cwe_codes] if cwe_codes else []

                if target == 0:
                    category_samples["Benign"].append(item)
                else:
                    category = map_cwe_to_major(cwe_codes) if cwe_codes else "Other"
                    if category in category_samples:
                        category_samples[category].append(item)
                    else:
                        category_samples["Other"].append(item)

            except json.JSONDecodeError:
                continue

    print(f"   æ€»æ ·æœ¬æ•°: {total}")

    print("\nğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for cat, samples in sorted(category_samples.items(), key=lambda x: len(x[1]), reverse=True):
        if samples:
            print(f"   {cat:25s}: {len(samples):5d} æ ·æœ¬")

    # å¹¶å‘è¯„ä¼°
    print(f"\nğŸš€ å¯åŠ¨ RAG å¹¶å‘è¯„ä¼° (workers={max_workers}, top_k={top_k})")

    start_time = time.time()
    results = {}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}

        for category, samples in category_samples.items():
            if samples:
                future = executor.submit(
                    evaluate_category_with_rag,
                    category, samples, retriever, base_prompt,
                    max_samples_per_category, top_k
                )
                futures[future] = category

        for future in as_completed(futures):
            category = futures[future]
            try:
                result = future.result()
                results[category] = result
                print(f"   âœ… {category:25s}: {result['accuracy']:6.2%} ({result['correct']:4d}/{result['total']:4d})")
            except Exception as e:
                print(f"   âŒ {category:25s}: å¤±è´¥ - {e}")
                results[category] = {"category": category, "error": str(e)}

    elapsed = time.time() - start_time

    # æ±‡æ€»
    total_evaluated = sum(r.get("total", 0) for r in results.values())
    total_correct = sum(r.get("correct", 0) for r in results.values())
    overall_accuracy = total_correct / total_evaluated if total_evaluated > 0 else 0

    category_accuracies = [r.get("accuracy", 0) for r in results.values() if r.get("total", 0) > 0]
    macro_accuracy = sum(category_accuracies) / len(category_accuracies) if category_accuracies else 0

    summary = {
        "timestamp": datetime.now().isoformat(),
        "data_file": data_file,
        "rag_top_k": top_k,
        "kb_stats": kb.statistics(),
        "elapsed_seconds": elapsed,
        "total_samples": total_evaluated,
        "total_correct": total_correct,
        "overall_accuracy": overall_accuracy,
        "macro_accuracy": macro_accuracy,
        "category_results": {k: {kk: vv for kk, vv in v.items() if kk != "results"}
                            for k, v in results.items()},
        "prompt_used": base_prompt
    }

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š RAG å¢å¼ºè¯„ä¼°ç»“æœ")
    print("=" * 70)
    print(f"RAG top-k: {top_k}")
    print(f"æ€»æ ·æœ¬æ•°: {total_evaluated}")
    print(f"æ­£ç¡®æ•°: {total_correct}")
    print(f"æ€»ä½“å‡†ç¡®ç‡ (Micro): {overall_accuracy:.2%}")
    print(f"å®å¹³å‡å‡†ç¡®ç‡ (Macro): {macro_accuracy:.2%}")
    print(f"è€—æ—¶: {elapsed:.1f}ç§’")

    print("\nğŸ“ˆ å„ç±»åˆ«å‡†ç¡®ç‡:")
    for cat, res in sorted(results.items(), key=lambda x: x[1].get('accuracy', 0), reverse=True):
        if res.get('total', 0) > 0:
            print(f"   {cat:25s}: {res['accuracy']:6.2%} ({res['correct']:4d}/{res['total']:4d})")

    # ä¿å­˜ç»“æœ
    os.makedirs(output_dir, exist_ok=True)
    output_file = Path(output_dir) / f"rag_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

    return summary


def main():
    parser = argparse.ArgumentParser(description="RAG å¢å¼º Primevul è¯„ä¼°")
    parser.add_argument("--data", default="./data/primevul/primevul/primevul_valid.jsonl", help="è¯„ä¼°æ•°æ®")
    parser.add_argument("--kb-data", default="./data/primevul/primevul/primevul_train.jsonl", help="çŸ¥è¯†åº“æ•°æ®æº")
    parser.add_argument("--kb-file", help="å·²æœ‰çŸ¥è¯†åº“æ–‡ä»¶ (è·³è¿‡æ„å»º)")
    parser.add_argument("--samples-per-cat", type=int, default=2, help="æ¯ç±»çŸ¥è¯†åº“æ ·æœ¬æ•°")
    parser.add_argument("--top-k", type=int, default=3, help="RAG æ£€ç´¢ top-k")
    parser.add_argument("--workers", type=int, default=8, help="å¹¶å‘çº¿ç¨‹æ•°")
    parser.add_argument("--max-samples", type=int, default=None, help="æ¯ç±»æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°")
    parser.add_argument("--output", default="./outputs", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--save-kb", help="ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶")
    parser.add_argument("--debug-rag", action="store_true", help="æ‰“å° RAG æ£€ç´¢è°ƒè¯•ä¿¡æ¯")

    args = parser.parse_args()

    random.seed(42)

    # æ„å»ºæˆ–åŠ è½½çŸ¥è¯†åº“
    if args.kb_file and os.path.exists(args.kb_file):
        print(f"ğŸ“– åŠ è½½å·²æœ‰çŸ¥è¯†åº“: {args.kb_file}")
        kb = KnowledgeBase.load(args.kb_file)
    else:
        kb = build_kb_from_primevul(args.kb_data, args.samples_per_cat)

        if args.save_kb:
            kb.save(args.save_kb)
            print(f"ğŸ’¾ çŸ¥è¯†åº“å·²ä¿å­˜: {args.save_kb}")

    # åŸºç¡€ prompt
    base_prompt = """Based on the examples above, classify this code into one of these CWE major categories:
- Buffer Errors, Injection, Memory Management, Pointer Dereference, Integer Errors
- Concurrency Issues, Path Traversal, Cryptography Issues, Information Exposure, Other
- Benign (no vulnerabilities)

Code to analyze:
{input}

Category:"""

    # è¿è¡Œè¯„ä¼°
    run_rag_concurrent_evaluation(
        args.data,
        kb,
        base_prompt,
        max_workers=args.workers,
        max_samples_per_category=args.max_samples,
        top_k=args.top_k,
        output_dir=args.output,
        debug_rag=args.debug_rag
    )

    return 0


if __name__ == "__main__":
    sys.exit(main())
