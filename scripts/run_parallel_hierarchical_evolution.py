#!/usr/bin/env python3
"""
å¹¶è¡Œåˆ†å±‚æ¼æ´æ£€æµ‹è¿›åŒ–å®éªŒ

åŠŸèƒ½:
- Seed prompts ä½œä¸ºåˆå§‹ç§ç¾¤ï¼Œè¾“å‡º CONFIDENCE score
- Task-aware evolution prompts æŒ‡å¯¼è¿›åŒ–
- è¿›åº¦æ¡æ˜¾ç¤º
- å®æ—¶ä¿å­˜ç»“æœ
- Checkpoint resume æ”¯æŒ
"""

import sys
import json
import re
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field, asdict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from evoprompt.llm.client import create_default_client
from evoprompt.algorithms.genetic import GeneticAlgorithm
from evoprompt.algorithms.base import Individual
from evoprompt.prompts import (
    load_seeds_for_ga,
    get_task_context,
    LAYER1_SEED_PROMPTS,
)
from evoprompt.data.dataset import PrimevulDataset
from evoprompt.data.sampler import sample_primevul_1percent
from evoprompt.utils.text import safe_format


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class SampleResult:
    """å•ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ"""
    sample_id: str
    code_hash: str
    ground_truth: int  # 0=benign, 1=vulnerable
    predicted_score: float  # 0.0-1.0
    prompt_idx: int
    category: str
    response_raw: str = ""


@dataclass
class PromptEvaluation:
    """å•ä¸ª prompt çš„è¯„ä¼°ç»“æœ"""
    prompt_idx: int
    prompt_text: str
    category: str
    samples_evaluated: int = 0
    total_samples: int = 0
    true_positives: int = 0
    true_negatives: int = 0
    false_positives: int = 0
    false_negatives: int = 0
    sample_results: List[SampleResult] = field(default_factory=list)

    @property
    def accuracy(self) -> float:
        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives
        if total == 0:
            return 0.0
        return (self.true_positives + self.true_negatives) / total

    @property
    def precision(self) -> float:
        if self.true_positives + self.false_positives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_positives)

    @property
    def recall(self) -> float:
        if self.true_positives + self.false_negatives == 0:
            return 0.0
        return self.true_positives / (self.true_positives + self.false_negatives)

    @property
    def f1(self) -> float:
        if self.precision + self.recall == 0:
            return 0.0
        return 2 * self.precision * self.recall / (self.precision + self.recall)


@dataclass
class GenerationResult:
    """å•ä»£è¿›åŒ–ç»“æœ"""
    generation: int
    category: str
    prompt_evaluations: List[PromptEvaluation] = field(default_factory=list)
    best_prompt_idx: int = 0
    best_accuracy: float = 0.0
    best_f1: float = 0.0
    timestamp: str = ""

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()


@dataclass
class ExperimentCheckpoint:
    """å®éªŒ checkpoint"""
    config: Dict[str, Any]
    category: str
    current_generation: int
    current_prompt_idx: int
    current_sample_idx: int
    generation_results: List[GenerationResult] = field(default_factory=list)
    prompts: List[str] = field(default_factory=list)
    timestamp: str = ""
    completed: bool = False

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

    def save(self, path: Path):
        """ä¿å­˜ checkpoint"""
        data = asdict(self)
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    @classmethod
    def load(cls, path: Path) -> "ExperimentCheckpoint":
        """åŠ è½½ checkpoint"""
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # é‡å»ºåµŒå¥—å¯¹è±¡
        gen_results = []
        for gr in data.get("generation_results", []):
            prompt_evals = []
            for pe in gr.get("prompt_evaluations", []):
                sample_results = [SampleResult(**sr) for sr in pe.get("sample_results", [])]
                pe_obj = PromptEvaluation(
                    prompt_idx=pe["prompt_idx"],
                    prompt_text=pe["prompt_text"],
                    category=pe["category"],
                    samples_evaluated=pe.get("samples_evaluated", 0),
                    total_samples=pe.get("total_samples", 0),
                    true_positives=pe.get("true_positives", 0),
                    true_negatives=pe.get("true_negatives", 0),
                    false_positives=pe.get("false_positives", 0),
                    false_negatives=pe.get("false_negatives", 0),
                    sample_results=sample_results,
                )
                prompt_evals.append(pe_obj)
            gr_obj = GenerationResult(
                generation=gr["generation"],
                category=gr["category"],
                prompt_evaluations=prompt_evals,
                best_prompt_idx=gr.get("best_prompt_idx", 0),
                best_accuracy=gr.get("best_accuracy", 0.0),
                best_f1=gr.get("best_f1", 0.0),
                timestamp=gr.get("timestamp", ""),
            )
            gen_results.append(gr_obj)

        return cls(
            config=data["config"],
            category=data["category"],
            current_generation=data["current_generation"],
            current_prompt_idx=data["current_prompt_idx"],
            current_sample_idx=data["current_sample_idx"],
            generation_results=gen_results,
            prompts=data.get("prompts", []),
            timestamp=data.get("timestamp", ""),
            completed=data.get("completed", False),
        )


# =============================================================================
# Score Parsing
# =============================================================================

def parse_confidence_score(response: str) -> float:
    """ä» LLM å“åº”ä¸­è§£æ CONFIDENCE score

    æ”¯æŒæ ¼å¼:
    - CONFIDENCE: 0.8
    - CONFIDENCE: 0.85
    - confidence: 0.7
    - Score: 0.9
    - 0.75 (çº¯æ•°å­—)
    """
    response = response.strip()

    # å°è¯•åŒ¹é… CONFIDENCE: <score> æ ¼å¼
    patterns = [
        r'CONFIDENCE:\s*([\d.]+)',
        r'confidence:\s*([\d.]+)',
        r'Score:\s*([\d.]+)',
        r'score:\s*([\d.]+)',
        r'^([\d.]+)$',  # çº¯æ•°å­—
        r'(0\.\d+|1\.0|1|0)',  # ä»»ä½• 0-1 æ•°å­—
    ]

    for pattern in patterns:
        match = re.search(pattern, response, re.IGNORECASE)
        if match:
            try:
                score = float(match.group(1))
                return max(0.0, min(1.0, score))  # é™åˆ¶åœ¨ 0-1 èŒƒå›´
            except ValueError:
                continue

    # å¦‚æœåŒ…å« vulnerable/unsafe ç­‰å…³é”®è¯ï¼Œè¿”å›é«˜åˆ†
    vulnerable_keywords = ['vulnerable', 'unsafe', 'insecure', 'dangerous', 'risk', 'flaw']
    safe_keywords = ['safe', 'secure', 'benign', 'clean', 'no vulnerability']

    response_lower = response.lower()
    for kw in vulnerable_keywords:
        if kw in response_lower:
            return 0.8
    for kw in safe_keywords:
        if kw in response_lower:
            return 0.2

    # é»˜è®¤è¿”å› 0.5 (ä¸ç¡®å®š)
    return 0.5


# =============================================================================
# Pipeline
# =============================================================================

class ParallelHierarchicalEvolutionPipeline:
    """å¹¶è¡Œåˆ†å±‚æ¼æ´æ£€æµ‹è¿›åŒ–å®éªŒ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.output_dir = Path(config.get("output_dir", "outputs/parallel_evolution"))
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Checkpoint ç›®å½•
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # ç»“æœç›®å½•
        self.results_dir = self.output_dir / "results"
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # Score é˜ˆå€¼ (ç”¨äºäºŒåˆ†ç±»)
        self.score_threshold = config.get("score_threshold", 0.5)

        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        print("åˆå§‹åŒ– LLM å®¢æˆ·ç«¯...")
        self.llm_client = self._create_llm_client()

        # è¿›åŒ–é…ç½®
        self.ga_config = {
            "population_size": config.get("population_size", 5),
            "max_generations": config.get("max_generations", 3),
            "mutation_rate": config.get("mutation_rate", 0.3),
            "crossover_rate": config.get("crossover_rate", 0.8),
        }

        print(f"âœ… Pipeline åˆå§‹åŒ–å®Œæˆ")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   ç§ç¾¤å¤§å°: {self.ga_config['population_size']}")
        print(f"   è¿›åŒ–ä»£æ•°: {self.ga_config['max_generations']}")
        print(f"   Score é˜ˆå€¼: {self.score_threshold}")

    def _create_llm_client(self):
        """åˆ›å»º LLM å®¢æˆ·ç«¯"""
        return create_default_client()

    def load_dataset(self) -> tuple:
        """åŠ è½½æ•°æ®é›†"""
        print("\nğŸ“ åŠ è½½æ•°æ®é›†...")

        primevul_dir = Path(self.config.get("primevul_dir", "./data/primevul/primevul"))
        sample_dir = Path(self.config.get("sample_dir", "./data/primevul_1percent_sample"))

        if not sample_dir.exists():
            print(f"   ç”Ÿæˆ 1% é‡‡æ ·æ•°æ®åˆ° {sample_dir}")
            sample_primevul_1percent(str(primevul_dir), str(sample_dir), seed=42)

        train_file = sample_dir / "train.txt"
        dev_file = sample_dir / "dev.txt"

        train_dataset = PrimevulDataset(str(train_file), "train")
        dev_dataset = PrimevulDataset(str(dev_file), "dev")

        print(f"   âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"   âœ… å¼€å‘é›†: {len(dev_dataset)} æ ·æœ¬")

        return train_dataset, dev_dataset

    def get_checkpoint_path(self, category: str) -> Path:
        """è·å– checkpoint æ–‡ä»¶è·¯å¾„"""
        return self.checkpoint_dir / f"checkpoint_{category}.json"

    def get_results_path(self, category: str) -> Path:
        """è·å–ç»“æœæ–‡ä»¶è·¯å¾„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return self.results_dir / f"results_{category}_{timestamp}.json"

    def save_checkpoint(self, checkpoint: ExperimentCheckpoint):
        """ä¿å­˜ checkpoint"""
        path = self.get_checkpoint_path(checkpoint.category)
        checkpoint.save(path)
        print(f"\nğŸ’¾ Checkpoint å·²ä¿å­˜: {path}")

    def load_checkpoint(self, category: str) -> Optional[ExperimentCheckpoint]:
        """åŠ è½½ checkpoint"""
        path = self.get_checkpoint_path(category)
        if path.exists():
            return ExperimentCheckpoint.load(path)
        return None

    def save_generation_result(self, gen_result: GenerationResult):
        """å®æ—¶ä¿å­˜å•ä»£ç»“æœ"""
        path = self.results_dir / f"gen_{gen_result.category}_{gen_result.generation}.json"

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        data = {
            "generation": gen_result.generation,
            "category": gen_result.category,
            "best_prompt_idx": gen_result.best_prompt_idx,
            "best_accuracy": gen_result.best_accuracy,
            "best_f1": gen_result.best_f1,
            "timestamp": gen_result.timestamp,
            "prompt_evaluations": [
                {
                    "prompt_idx": pe.prompt_idx,
                    "accuracy": pe.accuracy,
                    "precision": pe.precision,
                    "recall": pe.recall,
                    "f1": pe.f1,
                    "samples_evaluated": pe.samples_evaluated,
                    "true_positives": pe.true_positives,
                    "true_negatives": pe.true_negatives,
                    "false_positives": pe.false_positives,
                    "false_negatives": pe.false_negatives,
                }
                for pe in gen_result.prompt_evaluations
            ]
        }

        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def evaluate_prompt_with_progress(
        self,
        prompt: str,
        prompt_idx: int,
        category: str,
        dataset,
        max_samples: int = None,
        start_sample_idx: int = 0,
        existing_eval: Optional[PromptEvaluation] = None,
    ) -> PromptEvaluation:
        """è¯„ä¼°å•ä¸ª promptï¼Œå¸¦è¿›åº¦æ¡å’Œå®æ—¶ä¿å­˜"""
        samples = dataset.get_samples()
        if max_samples:
            samples = samples[:max_samples]

        # ä½¿ç”¨ç°æœ‰è¯„ä¼°æˆ–åˆ›å»ºæ–°çš„
        if existing_eval:
            eval_result = existing_eval
        else:
            eval_result = PromptEvaluation(
                prompt_idx=prompt_idx,
                prompt_text=prompt[:200] + "..." if len(prompt) > 200 else prompt,
                category=category,
                total_samples=len(samples),
            )

        # ä» start_sample_idx ç»§ç»­
        samples_to_process = samples[start_sample_idx:]

        pbar = tqdm(
            enumerate(samples_to_process, start=start_sample_idx),
            total=len(samples),
            initial=start_sample_idx,
            desc=f"  Prompt {prompt_idx+1}",
            leave=False,
        )

        for sample_idx, sample in pbar:
            code = sample.input_text
            query = safe_format(prompt, input=code, CODE=code)

            # Ground truth: 0=benign, 1=vulnerable
            gt_binary = int(sample.target)

            # é¢„æµ‹
            try:
                response = self.llm_client.generate(query, temperature=0.1, max_tokens=100)
                score = parse_confidence_score(response)
            except Exception as e:
                print(f"\n      âš ï¸ æ ·æœ¬ {sample_idx} é¢„æµ‹å¤±è´¥: {e}")
                score = 0.5
                response = f"ERROR: {e}"

            # æ ¹æ®é˜ˆå€¼è½¬æ¢ä¸ºäºŒåˆ†ç±»
            predicted_binary = 1 if score >= self.score_threshold else 0

            # è®°å½•ç»“æœ
            sample_result = SampleResult(
                sample_id=str(sample_idx),
                code_hash=str(hash(code))[:16],
                ground_truth=gt_binary,
                predicted_score=score,
                prompt_idx=prompt_idx,
                category=category,
                response_raw=response[:200] if response else "",
            )
            eval_result.sample_results.append(sample_result)

            # æ›´æ–°ç»Ÿè®¡
            if predicted_binary == 1 and gt_binary == 1:
                eval_result.true_positives += 1
            elif predicted_binary == 0 and gt_binary == 0:
                eval_result.true_negatives += 1
            elif predicted_binary == 1 and gt_binary == 0:
                eval_result.false_positives += 1
            else:
                eval_result.false_negatives += 1

            eval_result.samples_evaluated += 1

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                "acc": f"{eval_result.accuracy:.1%}",
                "f1": f"{eval_result.f1:.2f}",
            })

        return eval_result

    def run_category_evolution(
        self,
        category: str,
        train_dataset,
        dev_dataset,
        resume_checkpoint: Optional[ExperimentCheckpoint] = None,
    ) -> Dict[str, Any]:
        """å¯¹å•ä¸ªç±»åˆ«è¿è¡Œè¿›åŒ–"""
        print(f"\n{'='*60}")
        print(f"ğŸ§¬ è¿›åŒ–ç±»åˆ«: {category}")
        print(f"{'='*60}")

        # è·å– task context
        task_context = get_task_context(category)
        if task_context:
            print(f"   ä»»åŠ¡æè¿°: {task_context.description[:60]}...")
            print(f"   å…³é”®æŒ‡æ ‡: {len(task_context.indicators)} ä¸ª")

        # åˆ›å»ºæˆ–æ¢å¤ checkpoint
        if resume_checkpoint and not resume_checkpoint.completed:
            checkpoint = resume_checkpoint
            prompts = checkpoint.prompts
            start_gen = checkpoint.current_generation
            start_prompt_idx = checkpoint.current_prompt_idx
            start_sample_idx = checkpoint.current_sample_idx
            generation_results = checkpoint.generation_results
            print(f"   ğŸ“‚ ä» checkpoint æ¢å¤: ç¬¬ {start_gen} ä»£, Prompt {start_prompt_idx}, æ ·æœ¬ {start_sample_idx}")
        else:
            # åˆ›å»º GA å¹¶è·å–ç§å­ prompts
            ga = GeneticAlgorithm.with_seed_prompts(
                self.ga_config,
                layer=1,
                category=category
            )
            prompts = ga._seed_prompts
            start_gen = 0
            start_prompt_idx = 0
            start_sample_idx = 0
            generation_results = []

            checkpoint = ExperimentCheckpoint(
                config=self.config,
                category=category,
                current_generation=0,
                current_prompt_idx=0,
                current_sample_idx=0,
                prompts=prompts,
            )

        print(f"   ç§å­ prompts: {len(prompts)} ä¸ª")

        eval_samples = self.config.get("eval_samples", 50)

        # è¿›åŒ–å¾ªç¯
        for gen in range(start_gen, self.ga_config["max_generations"] + 1):
            print(f"\nğŸ“ˆ ç¬¬ {gen} ä»£ (å…± {self.ga_config['max_generations']} ä»£)")

            gen_result = GenerationResult(generation=gen, category=category)

            # ç¡®å®šèµ·å§‹ prompt ç´¢å¼•
            prompt_start = start_prompt_idx if gen == start_gen else 0

            for prompt_idx in range(prompt_start, len(prompts)):
                prompt = prompts[prompt_idx]

                # ç¡®å®šèµ·å§‹æ ·æœ¬ç´¢å¼•
                sample_start = start_sample_idx if (gen == start_gen and prompt_idx == start_prompt_idx) else 0

                # æŸ¥æ‰¾å·²æœ‰è¯„ä¼°
                existing_eval = None
                if sample_start > 0:
                    # ä» checkpoint æ¢å¤ç°æœ‰è¯„ä¼°
                    for gr in generation_results:
                        if gr.generation == gen:
                            for pe in gr.prompt_evaluations:
                                if pe.prompt_idx == prompt_idx:
                                    existing_eval = pe
                                    break

                # è¯„ä¼° prompt
                eval_result = self.evaluate_prompt_with_progress(
                    prompt=prompt,
                    prompt_idx=prompt_idx,
                    category=category,
                    dataset=dev_dataset,
                    max_samples=eval_samples,
                    start_sample_idx=sample_start,
                    existing_eval=existing_eval,
                )

                gen_result.prompt_evaluations.append(eval_result)
                print(f"      Prompt {prompt_idx+1}: acc={eval_result.accuracy:.1%}, f1={eval_result.f1:.2f}, "
                      f"TP={eval_result.true_positives}, TN={eval_result.true_negatives}, "
                      f"FP={eval_result.false_positives}, FN={eval_result.false_negatives}")

                # æ›´æ–° checkpoint
                checkpoint.current_generation = gen
                checkpoint.current_prompt_idx = prompt_idx + 1
                checkpoint.current_sample_idx = 0
                self.save_checkpoint(checkpoint)

            # æ‰¾æœ€ä½³ prompt
            if gen_result.prompt_evaluations:
                best_eval = max(gen_result.prompt_evaluations, key=lambda x: x.f1)
                gen_result.best_prompt_idx = best_eval.prompt_idx
                gen_result.best_accuracy = best_eval.accuracy
                gen_result.best_f1 = best_eval.f1

            generation_results.append(gen_result)
            checkpoint.generation_results = generation_results

            # å®æ—¶ä¿å­˜æœ¬ä»£ç»“æœ
            self.save_generation_result(gen_result)

            print(f"\n   âœ… ç¬¬ {gen} ä»£å®Œæˆ: æœ€ä½³ F1={gen_result.best_f1:.2f}, Acc={gen_result.best_accuracy:.1%}")

            # é‡ç½®èµ·å§‹ç´¢å¼•
            start_prompt_idx = 0
            start_sample_idx = 0

        # æ ‡è®°å®Œæˆ
        checkpoint.completed = True
        self.save_checkpoint(checkpoint)

        # è¿”å›ç»“æœ
        best_gen = max(generation_results, key=lambda x: x.best_f1)
        best_prompt = prompts[best_gen.best_prompt_idx]

        return {
            "category": category,
            "best_prompt": best_prompt,
            "best_f1": best_gen.best_f1,
            "best_accuracy": best_gen.best_accuracy,
            "fitness_history": [gr.best_f1 for gr in generation_results],
            "generation_results": generation_results,
        }

    def run(self, resume: bool = True) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("\n" + "="*80)
        print("ğŸš€ å¹¶è¡Œåˆ†å±‚æ¼æ´æ£€æµ‹è¿›åŒ–å®éªŒ")
        print("="*80)

        # åŠ è½½æ•°æ®
        train_dataset, dev_dataset = self.load_dataset()

        # é€‰æ‹©è¦è¿›åŒ–çš„ç±»åˆ«
        categories = self.config.get("categories", ["Memory"])
        print(f"\nç›®æ ‡ç±»åˆ«: {categories}")

        # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œè¿›åŒ–
        results = {}
        for category in categories:
            if category not in LAYER1_SEED_PROMPTS:
                print(f"âš ï¸ è·³è¿‡ç±»åˆ« {category}: æ—  seed prompts")
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰å¯æ¢å¤çš„ checkpoint
            checkpoint = None
            if resume:
                checkpoint = self.load_checkpoint(category)
                if checkpoint and checkpoint.completed:
                    print(f"\nâ­ï¸ ç±»åˆ« {category} å·²å®Œæˆï¼Œè·³è¿‡")
                    continue

            result = self.run_category_evolution(
                category, train_dataset, dev_dataset, checkpoint
            )
            results[category] = result

        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results(results)

        return results

    def save_final_results(self, results: Dict[str, Any]):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜æ±‡æ€»
        summary_file = self.output_dir / f"evolution_summary_{timestamp}.json"

        summary = {
            "timestamp": timestamp,
            "config": self.config,
            "results": {
                cat: {
                    "best_f1": r["best_f1"],
                    "best_accuracy": r["best_accuracy"],
                    "fitness_history": r["fitness_history"],
                }
                for cat, r in results.items()
            }
        }

        with open(summary_file, "w", encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {summary_file}")

        # æ‰“å°æœ€ç»ˆç»“æœ
        print("\n" + "="*80)
        print("ğŸ“Š æœ€ç»ˆç»“æœ")
        print("="*80)

        for cat, result in results.items():
            print(f"\n{cat}:")
            print(f"  æœ€ä½³ F1: {result['best_f1']:.2f}")
            print(f"  æœ€ä½³ Accuracy: {result['best_accuracy']:.1%}")
            print(f"  F1 å†å²: {' â†’ '.join(f'{f:.2f}' for f in result['fitness_history'])}")


def main():
    parser = argparse.ArgumentParser(description="å¹¶è¡Œåˆ†å±‚æ¼æ´æ£€æµ‹è¿›åŒ–å®éªŒ")
    parser.add_argument("--population-size", type=int, default=5, help="ç§ç¾¤å¤§å°")
    parser.add_argument("--max-generations", type=int, default=3, help="æœ€å¤§è¿›åŒ–ä»£æ•°")
    parser.add_argument("--eval-samples", type=int, default=50, help="æ¯æ¬¡è¯„ä¼°æ ·æœ¬æ•°")
    parser.add_argument("--score-threshold", type=float, default=0.5, help="Score äºŒåˆ†ç±»é˜ˆå€¼")
    parser.add_argument("--categories", nargs="+", default=["Memory"],
                        help="è¦è¿›åŒ–çš„ç±»åˆ«")
    parser.add_argument("--output-dir", type=str, default="outputs/parallel_evolution",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--primevul-dir", type=str, default="./data/primevul/primevul")
    parser.add_argument("--sample-dir", type=str, default="./data/primevul_1percent_sample")
    parser.add_argument("--no-resume", action="store_true", help="ä¸ä» checkpoint æ¢å¤")

    args = parser.parse_args()

    config = {
        "population_size": args.population_size,
        "max_generations": args.max_generations,
        "eval_samples": args.eval_samples,
        "score_threshold": args.score_threshold,
        "categories": args.categories,
        "output_dir": args.output_dir,
        "primevul_dir": args.primevul_dir,
        "sample_dir": args.sample_dir,
    }

    pipeline = ParallelHierarchicalEvolutionPipeline(config)
    results = pipeline.run(resume=not args.no_resume)

    print("\nâœ… å®éªŒå®Œæˆ!")
    return 0


if __name__ == "__main__":
    sys.exit(main())
