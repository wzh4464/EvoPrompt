#!/usr/bin/env python3
"""å®Œæ•´çš„ä¸‰å±‚æ£€æµ‹è®­ç»ƒè„šæœ¬

æ”¯æŒ:
- RAGå¢å¼º (å¯é€‰)
- Scaleå¢å¼º (å¯é€‰)
- Multi-agentååŒè¿›åŒ–
- å±‚çº§è®­ç»ƒç­–ç•¥
"""

import os
import sys
import json
import argparse
import time
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from evoprompt.prompts.hierarchical_three_layer import (
    ThreeLayerPromptFactory,
    ThreeLayerPromptSet,
)
from evoprompt.detectors.three_layer_detector import ThreeLayerDetector, ThreeLayerEvaluator
from evoprompt.detectors.rag_three_layer_detector import RAGThreeLayerDetector
from evoprompt.rag.knowledge_base import KnowledgeBase, KnowledgeBaseBuilder
from evoprompt.data.dataset import PrimevulDataset
from evoprompt.llm.client import load_env_vars, create_llm_client
from evoprompt.multiagent.agents import create_detection_agent, create_meta_agent
from evoprompt.multiagent.coordinator import MultiAgentCoordinator, CoordinatorConfig
from evoprompt.algorithms.coevolution import CoevolutionaryAlgorithm
from evoprompt.utils.trace import TraceManager, TraceConfig, trace_enabled_from_env


def setup_environment():
    """é…ç½®ç¯å¢ƒ"""
    load_env_vars()

    api_key = os.getenv("API_KEY")
    if not api_key:
        print("âŒ API_KEY not found in .env")
        return False

    print("âœ… Environment configured:")
    print(f"   Detection Model: {os.getenv('MODEL_NAME', 'gpt-4')}")
    print(f"   Meta Model: {os.getenv('META_MODEL_NAME', 'claude-4.5')}")

    return True


def load_or_build_knowledge_base(args):
    """åŠ è½½æˆ–æ„å»ºçŸ¥è¯†åº“

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        KnowledgeBase or None
    """
    if not args.use_rag:
        print("\nâ­ï¸  RAG disabled, skipping knowledge base")
        return None

    print("\nğŸ“š Knowledge Base Setup")
    print("=" * 70)

    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„çŸ¥è¯†åº“
    if args.kb_path and Path(args.kb_path).exists():
        print(f"   ğŸ“– Loading existing KB: {args.kb_path}")
        kb = KnowledgeBase.load(args.kb_path)
        stats = kb.statistics()
        print(f"   âœ… Loaded {stats['total_examples']} examples")
        return kb

    # æ„å»ºæ–°çŸ¥è¯†åº“
    print("   ğŸ”¨ Building new knowledge base...")

    if args.kb_from_dataset:
        # ä»æ•°æ®é›†æ„å»º
        print(f"   ğŸ“‚ Source: Dataset ({args.kb_samples_per_category} samples/category)")
        dataset = PrimevulDataset(args.train_file, "train")

        from evoprompt.rag.knowledge_base import create_knowledge_base_from_dataset
        import tempfile

        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            temp_path = f.name

        try:
            kb = create_knowledge_base_from_dataset(
                dataset,
                temp_path,
                samples_per_category=args.kb_samples_per_category
            )
            kb = KnowledgeBase.load(temp_path)
        finally:
            if Path(temp_path).exists():
                Path(temp_path).unlink()
    else:
        # ä½¿ç”¨é»˜è®¤ç¤ºä¾‹
        print("   ğŸ“¦ Source: Default examples")
        kb = KnowledgeBaseBuilder.create_default_kb()

    stats = kb.statistics()
    print(f"   âœ… Built KB with {stats['total_examples']} examples")

    # ä¿å­˜çŸ¥è¯†åº“
    if args.kb_path:
        Path(args.kb_path).parent.mkdir(parents=True, exist_ok=True)
        kb.save(args.kb_path)
        print(f"   ğŸ’¾ Saved to: {args.kb_path}")

    return kb


def create_detector(prompt_set, llm_client, kb, args):
    """åˆ›å»ºæ£€æµ‹å™¨

    Args:
        prompt_set: Prompté›†åˆ
        llm_client: LLMå®¢æˆ·ç«¯
        kb: çŸ¥è¯†åº“ (å¯ä¸ºNone)
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        æ£€æµ‹å™¨å®ä¾‹
    """
    print("\nğŸ”§ Creating Detector")
    print("=" * 70)

    if args.use_rag and kb is not None:
        print(f"   ğŸ¯ Type: RAG-Enhanced Three-Layer")
        print(f"   ğŸ“Š RAG top-k: {args.rag_top_k}")
        print(f"   ğŸ” Retriever: {args.rag_retriever_type}")
        print(f"   âš¡ Scale enhancement: {args.use_scale}")

        detector = RAGThreeLayerDetector(
            prompt_set=prompt_set,
            llm_client=llm_client,
            knowledge_base=kb,
            use_scale_enhancement=args.use_scale,
            retriever_type=args.rag_retriever_type,
            top_k=args.rag_top_k
        )
    else:
        print(f"   ğŸ¯ Type: Basic Three-Layer")
        print(f"   âš¡ Scale enhancement: {args.use_scale}")

        detector = ThreeLayerDetector(
            prompt_set=prompt_set,
            llm_client=llm_client,
            use_scale_enhancement=args.use_scale
        )

    return detector


def run_evaluation(detector, dataset, args, trace_manager: TraceManager = None):
    """è¿è¡Œè¯„ä¼°

    Args:
        detector: æ£€æµ‹å™¨
        dataset: æ•°æ®é›†
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    print("\nğŸ“Š Running Evaluation")
    print("=" * 70)

    evaluator = ThreeLayerEvaluator(detector, dataset)

    eval_count = args.eval_samples if args.eval_samples is not None else "all"
    print(f"   ğŸ” Evaluating on {eval_count} samples...")
    start = time.time()

    # ä½¿ç”¨verbose=Trueæ‰“å°è¯¦ç»†çš„Macro/Weighted/Micro F1
    metrics = evaluator.evaluate(sample_size=args.eval_samples, verbose=True)

    elapsed = time.time() - start

    print(f"\n   âœ… Evaluation completed in {elapsed:.1f}s")

    if trace_manager and trace_manager.enabled:
        trace_manager.log_event(
            "evaluation",
            {
                "mode": "baseline" if not args.train else "evaluation",
                "metrics": metrics,
                "eval_samples": args.eval_samples,
            },
        )

    return metrics


def run_training(initial_prompt_set, detector, dataset, kb, args, trace_manager: TraceManager = None):
    """è¿è¡Œè®­ç»ƒ

    Args:
        initial_prompt_set: åˆå§‹prompté›†åˆ
        detector: æ£€æµ‹å™¨
        dataset: æ•°æ®é›†
        kb: çŸ¥è¯†åº“
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        ä¼˜åŒ–åçš„prompté›†åˆ
    """
    print("\nğŸš€ Starting Training")
    print("=" * 70)

    # åˆ›å»ºagents
    print("   ğŸ¤– Creating agents...")
    detection_agent = create_detection_agent(
        model_name=os.getenv("MODEL_NAME", "gpt-4")
    )
    meta_agent = create_meta_agent(
        model_name=os.getenv("META_MODEL_NAME", "claude-4.5")
    )

    # åˆ›å»ºåè°ƒå™¨
    print("   ğŸ¯ Creating coordinator...")
    coordinator_config = CoordinatorConfig(
        batch_size=args.batch_size,
        enable_batch_feedback=True,
        statistics_window=5
    )
    coordinator = MultiAgentCoordinator(
        detection_agent=detection_agent,
        meta_agent=meta_agent,
        config=coordinator_config,
        trace_manager=trace_manager,
    )

    # åˆ›å»ºè¿›åŒ–ç®—æ³•é…ç½®
    print("   ğŸ§¬ Creating evolution algorithm...")
    config = {
        "population_size": args.population_size,
        "max_generations": args.max_generations,
        "elite_size": args.elite_size,
        "mutation_rate": args.mutation_rate,
        "meta_improve_interval": args.meta_improve_interval,
        "meta_improve_count": args.meta_improve_count,
        "top_k": args.elite_size,
        "enable_elitism": True,
        "meta_improvement_rate": 0.3
    }

    algorithm = CoevolutionaryAlgorithm(
        config=config,
        coordinator=coordinator,
        dataset=dataset
    )

    print()
    print(f"   ğŸ“‹ Configuration:")
    print(f"      Population: {args.population_size}")
    print(f"      Generations: {args.max_generations}")
    print(f"      Elite size: {args.elite_size}")
    print(f"      Mutation rate: {args.mutation_rate}")
    print(f"      Batch size: {args.batch_size}")
    print(f"      Meta improve interval: {args.meta_improve_interval}")
    print(f"      Meta improve count: {args.meta_improve_count}")

    # è¿è¡Œè¿›åŒ–
    print()
    print("   ğŸ¬ Starting evolution...")
    print("=" * 70)

    # æå–åˆå§‹prompts - ä½¿ç”¨layer1 promptä½œä¸ºåˆå§‹ç§ç¾¤
    # TODO: æœªæ¥åº”è¯¥æ”¯æŒå®Œæ•´çš„ä¸‰å±‚prompté›†åˆä¼˜åŒ–
    initial_prompts = [initial_prompt_set.layer1_prompt]

    if trace_manager and trace_manager.enabled:
        trace_manager.save_prompt_snapshot(
            "initial_layer1_prompt",
            initial_prompt_set.layer1_prompt,
            metadata={"stage": "initialization"},
        )

    best_individual = algorithm.evolve(initial_prompts=initial_prompts)

    print()
    print("   âœ… Training completed!")
    print(f"      Best fitness: {best_individual.fitness:.4f}")

    # TODO: å°†best_individual.promptè½¬æ¢å›ThreeLayerPromptSet
    # ç›®å‰è¿”å›åˆå§‹prompté›†åˆ
    if trace_manager and trace_manager.enabled:
        trace_manager.log_event(
            "training_complete",
            {
                "best_fitness": getattr(best_individual, "fitness", None),
            },
        )

    return initial_prompt_set


def save_results(output_dir, metrics, prompt_set, args):
    """ä¿å­˜ç»“æœ

    Args:
        output_dir: è¾“å‡ºç›®å½•
        metrics: è¯„ä¼°æŒ‡æ ‡
        prompt_set: Prompté›†åˆ
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print(f"\nğŸ’¾ Saving Results to: {output_dir}")
    print("=" * 70)

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜é…ç½®
    config = {
        "use_rag": args.use_rag,
        "use_scale": args.use_scale,
        "rag_top_k": args.rag_top_k if args.use_rag else None,
        "rag_retriever_type": args.rag_retriever_type if args.use_rag else None,
        "train": args.train,
        "population_size": args.population_size if args.train else None,
        "max_generations": args.max_generations if args.train else None,
        "timestamp": datetime.now().isoformat(),
    }

    with open(output_dir / "config.json", "w") as f:
        json.dump(config, f, indent=2)
    print("   âœ… config.json")

    # ä¿å­˜è¯„ä¼°ç»“æœ
    with open(output_dir / "metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)
    print("   âœ… metrics.json")

    # ä¿å­˜prompté›†åˆ
    with open(output_dir / "prompts.json", "w") as f:
        json.dump(prompt_set.to_dict(), f, indent=2, ensure_ascii=False)
    print("   âœ… prompts.json")

    # ä¿å­˜å¯è¯»çš„promptæ–‡æœ¬
    with open(output_dir / "prompts.txt", "w", encoding="utf-8") as f:
        f.write("="*70 + "\n")
        f.write("Three-Layer Prompts\n")
        f.write("="*70 + "\n\n")

        f.write("LAYER 1 PROMPT\n")
        f.write("-"*70 + "\n")
        f.write(prompt_set.layer1_prompt + "\n\n")

        f.write("LAYER 2 PROMPTS\n")
        f.write("-"*70 + "\n")
        for cat, prompt in prompt_set.layer2_prompts.items():
            f.write(f"\n[{cat.value}]\n")
            f.write(prompt + "\n")

        f.write("\nLAYER 3 PROMPTS\n")
        f.write("-"*70 + "\n")
        for cat, prompt in prompt_set.layer3_prompts.items():
            f.write(f"\n[{cat.value}]\n")
            f.write(prompt + "\n")
    print("   âœ… prompts.txt")

    print(f"\nğŸ“ All results saved to: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸‰å±‚æ£€æµ‹è®­ç»ƒè„šæœ¬ - æ”¯æŒRAGå’ŒScaleå¢å¼º"
    )

    # æ•°æ®é›†å‚æ•°
    parser.add_argument(
        "--train-file",
        default="./data/primevul/primevul/dev.jsonl",
        help="è®­ç»ƒæ•°æ®æ–‡ä»¶"
    )
    parser.add_argument(
        "--eval-file",
        default="./data/primevul/primevul/primevul_test.jsonl",
        help="è¯„ä¼°æ•°æ®æ–‡ä»¶"
    )
    parser.add_argument(
        "--eval-samples",
        type=int,
        default=None,
        help="è¯„ä¼°æ ·æœ¬æ•°é‡ (é»˜è®¤å…¨é‡)"
    )

    # RAGå‚æ•°
    parser.add_argument(
        "--use-rag",
        action="store_true",
        help="å¯ç”¨RAGå¢å¼º"
    )
    parser.add_argument(
        "--kb-path",
        default="./outputs/knowledge_base.json",
        help="çŸ¥è¯†åº“è·¯å¾„"
    )
    parser.add_argument(
        "--kb-from-dataset",
        action="store_true",
        help="ä»æ•°æ®é›†æ„å»ºçŸ¥è¯†åº“"
    )
    parser.add_argument(
        "--kb-samples-per-category",
        type=int,
        default=3,
        help="æ¯ä¸ªç±»åˆ«é‡‡æ ·æ•°é‡"
    )
    parser.add_argument(
        "--rag-top-k",
        type=int,
        default=2,
        help="RAGæ£€ç´¢top-k"
    )
    parser.add_argument(
        "--rag-retriever-type",
        choices=["lexical", "embedding"],
        default="lexical",
        help="RAGæ£€ç´¢å™¨ç±»å‹"
    )

    # Scaleå¢å¼ºå‚æ•°
    parser.add_argument(
        "--use-scale",
        action="store_true",
        help="å¯ç”¨Scaleå¢å¼º"
    )

    # è®­ç»ƒå‚æ•°
    parser.add_argument(
        "--train",
        action="store_true",
        help="è¿è¡Œè®­ç»ƒ (å¦åˆ™ä»…è¯„ä¼°)"
    )
    parser.add_argument(
        "--population-size",
        type=int,
        default=5,
        help="ç§ç¾¤å¤§å°"
    )
    parser.add_argument(
        "--max-generations",
        type=int,
        default=10,
        help="æœ€å¤§ä»£æ•°"
    )
    parser.add_argument(
        "--elite-size",
        type=int,
        default=1,
        help="ç²¾è‹±ä¸ªä½“æ•°é‡"
    )
    parser.add_argument(
        "--mutation-rate",
        type=float,
        default=0.3,
        help="å˜å¼‚ç‡"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="æ‰¹å¤„ç†å¤§å°"
    )
    parser.add_argument(
        "--meta-improve-interval",
        type=int,
        default=3,
        help="Metaä¼˜åŒ–é—´éš”"
    )
    parser.add_argument(
        "--meta-improve-count",
        type=int,
        default=2,
        help="æ¯æ¬¡Metaä¼˜åŒ–ä¸ªä½“æ•°"
    )
    parser.add_argument(
        "--release",
        action="store_true",
        help="å…³é—­è¯¦ç»†è¿½è¸ªè¾“å‡º (é»˜è®¤å¼€å¯)",
    )

    # è¾“å‡ºå‚æ•°
    parser.add_argument(
        "--output-dir",
        help="è¾“å‡ºç›®å½• (é»˜è®¤è‡ªåŠ¨ç”Ÿæˆ)"
    )

    args = parser.parse_args()

    if args.release:
        os.environ["EVOPROMPT_RELEASE"] = "1"

    # è®¾ç½®è¾“å‡ºç›®å½•
    if not args.output_dir:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mode = "train" if args.train else "eval"
        rag_suffix = "_rag" if args.use_rag else ""
        scale_suffix = "_scale" if args.use_scale else ""
        args.output_dir = f"./outputs/three_layer_{mode}{rag_suffix}{scale_suffix}_{timestamp}"

    trace_enabled = not args.release if args.release else trace_enabled_from_env()
    trace_manager = TraceManager(
        TraceConfig(
            enabled=trace_enabled,
            base_dir=Path(args.output_dir),
            experiment_id=Path(args.output_dir).name,
        )
    )

    # å¼€å§‹
    print("ğŸ—ï¸  Three-Layer Detection Training System")
    print("=" * 70)
    print()
    print("ğŸ“‹ Configuration:")
    print(f"   Mode: {'Training' if args.train else 'Evaluation Only'}")
    print(f"   RAG: {'âœ… Enabled' if args.use_rag else 'âŒ Disabled'}")
    print(f"   Scale: {'âœ… Enabled' if args.use_scale else 'âŒ Disabled'}")
    print(f"   Output: {args.output_dir}")

    # ç¯å¢ƒè®¾ç½®
    if not setup_environment():
        return 1

    # åŠ è½½çŸ¥è¯†åº“
    kb = load_or_build_knowledge_base(args)

    # åŠ è½½æ•°æ®é›†
    print("\nğŸ“‚ Loading Dataset")
    print("=" * 70)
    print(f"   Training: {args.train_file}")
    print(f"   Evaluation: {args.eval_file}")

    train_dataset = PrimevulDataset(args.train_file, "train")
    eval_dataset = PrimevulDataset(args.eval_file, "dev")

    print(f"   âœ… Train: {len(train_dataset)} samples")
    print(f"   âœ… Eval: {len(eval_dataset)} samples")

    # åˆ›å»ºåˆå§‹prompté›†åˆ
    print("\nğŸ“ Creating Initial Prompts")
    print("=" * 70)
    prompt_set = ThreeLayerPromptFactory.create_default_prompt_set()
    counts = prompt_set.count_prompts()
    print(f"   âœ… Created {counts['total']} prompts")
    print(f"      Layer 1: {counts['layer1']}")
    print(f"      Layer 2: {counts['layer2']}")
    print(f"      Layer 3: {counts['layer3']}")

    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = create_llm_client(llm_type=os.getenv("MODEL_NAME", "gpt-4"))

    # åˆ›å»ºæ£€æµ‹å™¨
    detector = create_detector(prompt_set, llm_client, kb, args)

    # è¯„ä¼°åŸºçº¿
    print("\nğŸ“Š Baseline Evaluation")
    print("=" * 70)
    baseline_metrics = run_evaluation(detector, eval_dataset, args, trace_manager=trace_manager)

    # è®­ç»ƒ
    if args.train:
        prompt_set = run_training(prompt_set, detector, train_dataset, kb, args, trace_manager=trace_manager)

        # é‡æ–°åˆ›å»ºæ£€æµ‹å™¨å¹¶è¯„ä¼°
        print("\nğŸ“Š Final Evaluation")
        print("=" * 70)
        detector = create_detector(prompt_set, llm_client, kb, args)
        final_metrics = run_evaluation(detector, eval_dataset, args, trace_manager=trace_manager)

        # ä¿å­˜æœ€ç»ˆç»“æœ
        save_results(args.output_dir, final_metrics, prompt_set, args)
    else:
        # ä»…è¯„ä¼°ï¼Œä¿å­˜åŸºçº¿ç»“æœ
        save_results(args.output_dir, baseline_metrics, prompt_set, args)

    print("\n" + "=" * 70)
    print("âœ¨ Completed!")
    print()
    print("ğŸ“ Results:")
    print(f"   {args.output_dir}/")
    print(f"   â”œâ”€â”€ config.json      # é…ç½®")
    print(f"   â”œâ”€â”€ metrics.json     # è¯„ä¼°æŒ‡æ ‡")
    print(f"   â”œâ”€â”€ prompts.json     # Prompté›†åˆ")
    print(f"   â””â”€â”€ prompts.txt      # å¯è¯»Prompt")

    if kb and args.kb_path:
        print()
        print(f"ğŸ“š Knowledge Base: {args.kb_path}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
