#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å¹¶å‘åŠ é€Ÿçš„Primevul 1% Promptè¿›åŒ–å®éªŒ
æ”¯æŒè®­ç»ƒé›†æ‰“ä¹±å’Œæ¯ä¸ªæ ·æœ¬ç»“æœåé¦ˆæ›´æ–°prompt
"""

import os
import sys
import json
import time
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, 'src')

from evoprompt.data.sampler import sample_primevul_1percent
from evoprompt.workflows import VulnerabilityDetectionWorkflow
from evoprompt.core.prompt_tracker import PromptTracker
from evoprompt.algorithms.differential import DifferentialEvolution
from evoprompt.algorithms.base import Population
from evoprompt.data.dataset import PrimevulDataset


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    import logging
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('primevul_concurrent_evolution.log', encoding='utf-8')
        ]
    )
    
    return logging.getLogger(__name__)


def check_api_configuration():
    """æ£€æŸ¥APIé…ç½®ï¼ˆChatAnywhereï¼‰"""
    # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    from src.evoprompt.llm.client import load_env_vars
    load_env_vars()
    
    api_key = os.getenv("API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® API_KEY ç¯å¢ƒå˜é‡")
        print("   åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: API_KEY='your-api-key-here'")
        return None
    
    # æ£€æŸ¥ChatAnywhereé…ç½®
    api_base = os.getenv("API_BASE_URL", "https://api.chatanywhere.tech/v1")
    model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")
    
    print(f"âœ… ChatAnywhere APIé…ç½®æ£€æŸ¥é€šè¿‡:")
    print(f"   API_BASE_URL: {api_base}")
    print(f"   MODEL_NAME: {model_name}")
    print(f"   API_KEY: {api_key[:10]}...")
    
    return api_key


def create_optimized_config():
    """åˆ›å»ºä¼˜åŒ–çš„å®éªŒé…ç½®ï¼Œå……åˆ†åˆ©ç”¨å¹¶å‘ä¼˜åŠ¿"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    config = {
        # å®éªŒæ ‡è¯†
        "experiment_id": f"primevul_concurrent_1pct_{timestamp}",
        "experiment_name": "Primevul 1% Concurrent Optimized Evolution with Sample-wise Feedback",
        "timestamp": timestamp,
        
        # æ•°æ®é…ç½®
        "dataset": "primevul",
        "sample_ratio": 0.01,
        "balanced_sampling": True,
        "shuffle_training_data": True,  # å¯ç”¨è®­ç»ƒé›†æ‰“ä¹±
        
        # ç®—æ³•é…ç½® - ä¼˜åŒ–å‚æ•°ä»¥åˆ©ç”¨å¹¶å‘
        "algorithm": "de",
        "population_size": 8,    # è°ƒæ•´ç§ç¾¤å¤§å°é€‚åº”æ ·æœ¬çº§åé¦ˆ
        "max_generations": 5,    # è°ƒæ•´ä»£æ•°
        "mutation_rate": 0.15,
        "crossover_probability": 0.8,
        
        # æ ·æœ¬çº§åé¦ˆé…ç½®
        "sample_wise_feedback": True,     # å¯ç”¨æ ·æœ¬çº§åé¦ˆ
        "feedback_batch_size": 10,        # æ¯æ‰¹åé¦ˆçš„æ ·æœ¬æ•°
        "feedback_update_threshold": 0.1, # åé¦ˆæ›´æ–°é˜ˆå€¼
        "record_all_samples": True,       # è®°å½•æ‰€æœ‰æ ·æœ¬ç»“æœ
        
        # å¹¶å‘ä¼˜åŒ–é…ç½®
        "max_concurrency": 16,        # åŸºäºæµ‹è¯•çš„æœ€ä½³å¹¶å‘åº¦
        "force_async": True,          # å¼ºåˆ¶ä½¿ç”¨å¼‚æ­¥å¤„ç†
        "batch_evaluation": True,     # æ‰¹é‡è¯„ä¼°ä¼˜åŒ–
        
        # LLMé…ç½®
        "llm_type": "gpt-3.5-turbo",
        "max_tokens": 150,
        "temperature": 0.7,
        
        # è¯„ä¼°é…ç½®
        "sample_size": None,
        "test_sample_size": None,
        
        # è¾“å‡ºé…ç½®
        "output_dir": "./outputs/primevul_concurrent_feedback",
        "save_population": True,
        "detailed_logging": True,
        
        # é«˜æ€§èƒ½è¿½è¸ªé…ç½®
        "track_every_evaluation": True,
        "save_intermediate_results": True,
        "export_top_k": 15,
        "save_sample_results": True,  # ä¿å­˜æ ·æœ¬çº§ç»“æœ
    }
    
    return config


def create_diverse_initial_prompts():
    """åˆ›å»ºå¤šæ ·åŒ–çš„åˆå§‹prompté›†åˆï¼Œå……åˆ†åˆ©ç”¨ç§ç¾¤è§„æ¨¡"""
    initial_prompts = [
        # ç®€æ´åˆ†æç±»
        "Analyze this code for security vulnerabilities. Look for buffer overflows, injection attacks, memory corruption, and unsafe function usage. Respond 'vulnerable' if you find security issues, 'benign' if the code appears safe.\n\nCode to analyze:\n{input}\n\nSecurity assessment:",
        
        "Check this code for security vulnerabilities. Focus on real exploitable issues. Answer 'vulnerable' or 'benign':\n\n{input}\n\nResult:",
        
        # ä¸“å®¶è§’è‰²ç±»
        "You are a cybersecurity expert conducting a code security review. Examine this code snippet for potential vulnerabilities including but not limited to: SQL injection, buffer overflow, use-after-free, null pointer dereference, and input validation issues. Classify as 'vulnerable' or 'benign'.\n\n{input}\n\nYour expert assessment:",
        
        "As a security-focused code reviewer, examine this code with a defensive mindset. Consider: Are there any unsafe operations? Is input properly validated? Could this code be exploited by an attacker? Respond with 'vulnerable' for unsafe code or 'benign' for secure code.\n\nCode under review:\n{input}\n\nDefensive analysis result:",
        
        # ç»“æ„åŒ–åˆ†æç±»
        "Perform a systematic security analysis of this code:\n1. Check for unsafe function calls\n2. Analyze input validation\n3. Look for memory management issues\n4. Identify potential attack vectors\n\nCode: {input}\n\nBased on your analysis, is this code 'vulnerable' or 'benign'?",
        
        "Evaluate this code's security on multiple levels:\n- Syntax level: unsafe functions, operations\n- Logic level: control flow vulnerabilities\n- Data level: input/output handling issues\n\nCode: {input}\n\nOverall security verdict ('vulnerable' or 'benign'):",
        
        # CWEå¯¼å‘ç±»
        "Review this code for Common Weakness Enumeration (CWE) patterns such as CWE-120 (buffer overflow), CWE-79 (XSS), CWE-89 (SQL injection), CWE-476 (null pointer dereference), and other security weaknesses. Answer 'vulnerable' if any CWE patterns are found, 'benign' otherwise.\n\n{input}\n\nCWE-based assessment:",
        
        "Examine this code for specific vulnerability patterns: command injection, path traversal, integer overflow, race conditions, and cryptographic weaknesses. Provide verdict 'vulnerable' or 'benign'.\n\n{input}\n\nPattern analysis:",
        
        # æ”»å‡»è€…è§†è§’ç±»
        "Think like an attacker: could you exploit this code? Look for entry points, unsafe operations, and potential attack surfaces. If you can find a way to exploit it, answer 'vulnerable'. If not, answer 'benign'.\n\n{input}\n\nAttacker's assessment:",
        
        "From a penetration tester's perspective, assess this code for exploitable vulnerabilities. Focus on real-world attack scenarios. Answer 'vulnerable' if exploitable, 'benign' if secure.\n\n{input}\n\nPentest verdict:",
        
        # ç®€åŒ–é«˜æ•ˆç±»
        "Security check: Is this code vulnerable? Answer 'vulnerable' or 'benign'.\n\n{input}\n\nQuick assessment:",
        
        "Code security analysis. Look for vulnerabilities and unsafe patterns. Reply 'vulnerable' or 'benign'.\n\n{input}\n\nAnalysis:"
    ]
    
    return initial_prompts


class SampleWiseTracker:
    """æ ·æœ¬çº§ç»“æœè¿½è¸ªå™¨"""
    
    def __init__(self, exp_dir: Path):
        self.exp_dir = exp_dir
        self.sample_results = []
        self.sample_feedback_log = exp_dir / "sample_feedback.jsonl"
        self.sample_stats = exp_dir / "sample_statistics.json"
        
    def log_sample_result(self, prompt_id: str, sample_idx: int, sample_data: dict, 
                         prediction: str, ground_truth: int, correct: bool, 
                         generation: int, feedback_applied: bool = False):
        """è®°å½•å•ä¸ªæ ·æœ¬çš„ç»“æœ"""
        cwe_codes = sample_data.get('cwe', [])
        result = {
            "timestamp": datetime.now().isoformat(),
            "prompt_id": prompt_id,
            "generation": generation,
            "sample_idx": sample_idx,
            "sample_func": sample_data.get('func', '')[:100],  # æˆªå–å‰100å­—ç¬¦
            "sample_target": ground_truth,
            "prediction": prediction,
            "correct": correct,
            "feedback_applied": feedback_applied,
            "cwe_codes": cwe_codes,  # æ˜ç¡®è¾“å‡ºCWEä»£ç 
            "cve_id": sample_data.get('cve', 'None'),
            "metadata": {
                "project": sample_data.get('project', ''),
                "cwe": cwe_codes,
                "cve": sample_data.get('cve', 'None'),
                "cve_desc": sample_data.get('cve_desc', 'None'),
                "func_hash": sample_data.get('func_hash', ''),
                "file_name": sample_data.get('file_name', '')
            }
        }
        
        self.sample_results.append(result)
        
        # å®æ—¶å†™å…¥æ—¥å¿—
        with open(self.sample_feedback_log, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    def get_recent_performance(self, prompt_id: str, last_n: int = 10):
        """è·å–æœ€è¿‘Nä¸ªæ ·æœ¬çš„æ€§èƒ½"""
        recent_results = [r for r in self.sample_results 
                         if r['prompt_id'] == prompt_id][-last_n:]
        
        if not recent_results:
            return {"accuracy": 0.0, "count": 0}
            
        correct_count = sum(1 for r in recent_results if r['correct'])
        return {
            "accuracy": correct_count / len(recent_results),
            "count": len(recent_results),
            "correct": correct_count,
            "total": len(recent_results)
        }
    
    def save_statistics(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.sample_results:
            return
            
        # æŒ‰promptç»Ÿè®¡
        prompt_stats = {}
        cwe_analysis = {}
        
        for result in self.sample_results:
            prompt_id = result['prompt_id']
            if prompt_id not in prompt_stats:
                prompt_stats[prompt_id] = {
                    "total_samples": 0,
                    "correct_samples": 0,
                    "accuracy": 0.0,
                    "generations": set(),
                    "feedback_applied_count": 0
                }
            
            stats = prompt_stats[prompt_id]
            stats["total_samples"] += 1
            if result['correct']:
                stats["correct_samples"] += 1
            stats["generations"].add(result['generation'])
            if result['feedback_applied']:
                stats["feedback_applied_count"] += 1
            
            # CWEç»Ÿè®¡åˆ†æ
            cwe_codes = result.get('cwe_codes', [])
            for cwe in cwe_codes:
                if cwe not in cwe_analysis:
                    cwe_analysis[cwe] = {
                        "total_samples": 0,
                        "correct_predictions": 0,
                        "false_positives": 0,
                        "false_negatives": 0,
                        "accuracy": 0.0
                    }
                
                cwe_stats = cwe_analysis[cwe]
                cwe_stats["total_samples"] += 1
                
                if result['correct']:
                    cwe_stats["correct_predictions"] += 1
                else:
                    # åˆ†æé”™è¯¯ç±»å‹
                    if result['sample_target'] == 1:  # å®é™…æ˜¯æ¼æ´ï¼Œä½†é¢„æµ‹é”™äº†
                        cwe_stats["false_negatives"] += 1
                    else:  # å®é™…ä¸æ˜¯æ¼æ´ï¼Œä½†é¢„æµ‹æ˜¯æ¼æ´
                        cwe_stats["false_positives"] += 1
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        for prompt_id in prompt_stats:
            stats = prompt_stats[prompt_id]
            stats["accuracy"] = stats["correct_samples"] / stats["total_samples"] if stats["total_samples"] > 0 else 0
            stats["generations"] = list(stats["generations"])
        
        # è®¡ç®—CWEå‡†ç¡®ç‡
        for cwe in cwe_analysis:
            cwe_stats = cwe_analysis[cwe]
            cwe_stats["accuracy"] = cwe_stats["correct_predictions"] / cwe_stats["total_samples"] if cwe_stats["total_samples"] > 0 else 0
            cwe_stats["precision"] = cwe_stats["correct_predictions"] / (cwe_stats["correct_predictions"] + cwe_stats["false_positives"]) if (cwe_stats["correct_predictions"] + cwe_stats["false_positives"]) > 0 else 0
            cwe_stats["recall"] = cwe_stats["correct_predictions"] / (cwe_stats["correct_predictions"] + cwe_stats["false_negatives"]) if (cwe_stats["correct_predictions"] + cwe_stats["false_negatives"]) > 0 else 0
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        final_stats = {
            "total_samples_evaluated": len(self.sample_results),
            "total_prompts": len(prompt_stats),
            "overall_accuracy": sum(1 for r in self.sample_results if r['correct']) / len(self.sample_results),
            "prompt_statistics": prompt_stats,
            "generation_summary": self._get_generation_summary(),
            "cwe_analysis": cwe_analysis,
            "cwe_summary": {
                "total_cwe_types": len(cwe_analysis),
                "most_common_cwes": sorted(cwe_analysis.items(), key=lambda x: x[1]["total_samples"], reverse=True)[:10],
                "best_performing_cwes": sorted(cwe_analysis.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:10],
                "worst_performing_cwes": sorted(cwe_analysis.items(), key=lambda x: x[1]["accuracy"])[:10]
            }
        }
        
        with open(self.sample_stats, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, indent=2, ensure_ascii=False)
    
    def _get_generation_summary(self):
        """è·å–ä»£é™…æ€»ç»“"""
        gen_stats = {}
        for result in self.sample_results:
            gen = result['generation']
            if gen not in gen_stats:
                gen_stats[gen] = {"total": 0, "correct": 0}
            gen_stats[gen]["total"] += 1
            if result['correct']:
                gen_stats[gen]["correct"] += 1
        
        for gen in gen_stats:
            gen_stats[gen]["accuracy"] = gen_stats[gen]["correct"] / gen_stats[gen]["total"]
        
        return gen_stats


def run_concurrent_evolution_with_feedback(config: dict, sample_data_dir: str):
    """è¿è¡Œæ”¯æŒæ ·æœ¬çº§åé¦ˆçš„é«˜å¹¶å‘è¿›åŒ–å®éªŒ"""
    print(f"âš¡ å¼€å§‹æ ·æœ¬çº§åé¦ˆçš„é«˜å¹¶å‘Promptè¿›åŒ–å®éªŒ: {config['experiment_id']}")
    print(f"ğŸ”¥ ä½¿ç”¨ {config['max_concurrency']} ä¸ªå¹¶å‘è¿æ¥")
    print(f"ğŸ“Š æ ·æœ¬çº§åé¦ˆ: æ¯æ‰¹ {config['feedback_batch_size']} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºå®éªŒç›®å½•
    exp_dir = Path(config['output_dir']) / config['experiment_id']
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ ·æœ¬è¿½è¸ªå™¨
    sample_tracker = SampleWiseTracker(exp_dir)
    
    # åˆ›å»ºpromptè¿½è¸ªå™¨
    prompt_tracker = PromptTracker(str(config['output_dir']), config['experiment_id'])
    prompt_tracker.set_config(config)
    
    # è®¾ç½®æ•°æ®è·¯å¾„
    dev_file = Path(sample_data_dir) / "dev.txt"
    train_file = Path(sample_data_dir) / "train.txt"
    
    print(f"ğŸ“ æ•°æ®é…ç½®:")
    print(f"   å¼€å‘é›†: {dev_file}")
    print(f"   è®­ç»ƒé›†: {train_file}")
    
    # åŠ è½½å’Œæ‰“ä¹±è®­ç»ƒæ•°æ®
    print(f"ğŸ”„ åŠ è½½å¹¶æ‰“ä¹±è®­ç»ƒæ•°æ®...")
    train_dataset = PrimevulDataset(str(train_file), "train")
    train_samples = train_dataset.get_samples()  # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è·å–æ ·æœ¬
    
    if config.get('shuffle_training_data', True):
        random.seed(42)  # ç¡®ä¿å¯é‡ç°
        random.shuffle(train_samples)
        print(f"   âœ… è®­ç»ƒæ•°æ®å·²æ‰“ä¹±: {len(train_samples)} ä¸ªæ ·æœ¬")
    
    # åŠ è½½å¼€å‘é›†
    dev_dataset = PrimevulDataset(str(dev_file), "dev") 
    print(f"   å¼€å‘é›†: {len(dev_dataset)} ä¸ªæ ·æœ¬")
    
    # ä¿å­˜é…ç½®
    config_file = exp_dir / "experiment_config.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    from evoprompt.llm.client import create_default_client
    llm_client = create_default_client()
    if hasattr(llm_client, 'max_concurrency'):
        llm_client.max_concurrency = config['max_concurrency']
    
    # åˆ›å»ºåˆå§‹prompts
    initial_prompts = create_diverse_initial_prompts()
    
    # ä¿å­˜åˆå§‹prompts
    initial_prompts_file = exp_dir / "initial_prompts.txt"
    with open(initial_prompts_file, 'w', encoding='utf-8') as f:
        f.write("Initial Prompts for Sample-wise Feedback Evolution\n")
        f.write("=" * 50 + "\n\n")
        for i, prompt in enumerate(initial_prompts, 1):
            f.write(f"Prompt {i}:\n{'-' * 20}\n{prompt}\n\n")
    
    print(f"ğŸ’¾ å®éªŒé…ç½®:")
    print(f"   åˆå§‹prompts: {len(initial_prompts)}")
    print(f"   ç§ç¾¤å¤§å°: {config['population_size']}")
    print(f"   æœ€å¤§ä»£æ•°: {config['max_generations']}")
    print(f"   åé¦ˆæ‰¹å¤§å°: {config['feedback_batch_size']}")
    
    # åˆ›å»ºè¿›åŒ–ç®—æ³•
    algorithm = DifferentialEvolution({
        "population_size": config['population_size'],
        "max_generations": config['max_generations'],
        "mutation_factor": config.get('mutation_rate', 0.15),
        "crossover_probability": config.get('crossover_probability', 0.8)
    })
    
    # è®°å½•åˆå§‹prompts
    for i, prompt in enumerate(initial_prompts):
        prompt_tracker.log_prompt(
            prompt=prompt,
            generation=0,
            individual_id=f"initial_{i}",
            operation="initialization",
            metadata={"sample_wise_feedback": True, "prompt_index": i}
        )
    
    print(f"\nğŸš€ å¯åŠ¨æ ·æœ¬çº§åé¦ˆè¿›åŒ–...")
    start_time = time.time()
    
    try:
        # åˆå§‹åŒ–ç§ç¾¤
        from evoprompt.algorithms.base import Individual
        population_individuals = [Individual(prompt) for prompt in initial_prompts]
        population = Population(population_individuals)
        
        # åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°åˆå§‹ç§ç¾¤
        print(f"ğŸ“Š è¯„ä¼°åˆå§‹ç§ç¾¤...")
        for i, individual in enumerate(population.individuals):
            individual.fitness = evaluate_on_dataset(
                individual.prompt, dev_dataset, llm_client, f"initial_{i}", 
                sample_tracker, generation=0
            )
        
        print(f"   åˆå§‹é€‚åº”åº¦: {[f'{ind.fitness:.3f}' for ind in population.individuals]}")
        
        # è®°å½•åˆå§‹è¯„ä¼°
        prompt_tracker.log_population(population.individuals, generation=0, operation="initial_evaluation")
        
        best_fitness_history = []
        
        # æ ·æœ¬çº§åé¦ˆè¿›åŒ–å¾ªç¯
        for generation in range(1, config['max_generations'] + 1):
            print(f"\nâš¡ ç¬¬ {generation} ä»£æ ·æœ¬çº§åé¦ˆè¿›åŒ–...")
            gen_start_time = time.time()
            
            current_best = population.best()
            best_fitness_history.append(current_best.fitness)
            print(f"   å½“å‰æœ€ä½³é€‚åº”åº¦: {current_best.fitness:.4f}")
            
            # æ ·æœ¬çº§è®­ç»ƒå’Œåé¦ˆ
            new_individuals = []
            batch_count = 0
            
            for i, target_individual in enumerate(population.individuals):
                print(f"   å¤„ç†ä¸ªä½“ {i+1}/{len(population.individuals)}")
                
                # DEè¿›åŒ–æ“ä½œ
                candidates = [ind for j, ind in enumerate(population.individuals) if j != i]
                if len(candidates) >= 3:
                    # é€‰æ‹©ä¸‰ä¸ªä¸åŒçš„ä¸ªä½“
                    parents = random.sample(candidates, 3)
                    
                    # åˆ›å»ºå˜å¼‚ä¸ªä½“
                    mutant_prompt = create_mutant_prompt(
                        target_individual.prompt, 
                        [p.prompt for p in parents], 
                        llm_client, 
                        config
                    )
                    
                    if mutant_prompt and mutant_prompt != target_individual.prompt:
                        # åœ¨è®­ç»ƒæ ·æœ¬ä¸Šè¿›è¡Œæ ·æœ¬çº§åé¦ˆ
                        improved_prompt = sample_wise_feedback_training(
                            mutant_prompt,
                            train_samples,
                            llm_client,
                            sample_tracker,
                            config,
                            generation,
                            f"gen{generation}_individual_{i}",
                            batch_count
                        )
                        
                        # åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°æ”¹è¿›åçš„prompt
                        trial_individual = Individual(improved_prompt)
                        trial_individual.fitness = evaluate_on_dataset(
                            improved_prompt, dev_dataset, llm_client, 
                            f"gen{generation}_trial_{i}", sample_tracker, generation
                        )
                        
                        # è®°å½•è¯•éªŒä¸ªä½“
                        prompt_tracker.log_prompt(
                            prompt=improved_prompt,
                            fitness=trial_individual.fitness,
                            generation=generation,
                            individual_id=f"gen{generation}_trial_{i}",
                            operation="sample_feedback_evolution",
                            metadata={
                                "target_fitness": target_individual.fitness,
                                "improvement": trial_individual.fitness - target_individual.fitness,
                                "feedback_applied": True
                            }
                        )
                        
                        # é€‰æ‹©æ›´å¥½çš„ä¸ªä½“
                        if trial_individual.fitness > target_individual.fitness:
                            new_individuals.append(trial_individual)
                            print(f"     âœ… æ¥å—æ”¹è¿›ä¸ªä½“: {trial_individual.fitness:.4f} > {target_individual.fitness:.4f}")
                        else:
                            new_individuals.append(target_individual)
                            print(f"     âŒ ä¿ç•™åŸä¸ªä½“: {trial_individual.fitness:.4f} <= {target_individual.fitness:.4f}")
                        
                        batch_count += 1
                    else:
                        new_individuals.append(target_individual)
                        print(f"     âš ï¸ å˜å¼‚å¤±è´¥ï¼Œä¿ç•™åŸä¸ªä½“")
                else:
                    new_individuals.append(target_individual)
                    print(f"     âš ï¸ å€™é€‰ä¸è¶³ï¼Œä¿ç•™åŸä¸ªä½“")
            
            # æ›´æ–°ç§ç¾¤
            population = Population(new_individuals)
            
            # è®°å½•æœ¬ä»£ç»“æœ
            prompt_tracker.log_population(
                population.individuals,
                generation=generation,
                operation="sample_feedback_generation_complete"
            )
            
            gen_time = time.time() - gen_start_time
            print(f"   ç¬¬{generation}ä»£å®Œæˆ: {gen_time:.1f}ç§’")
            print(f"   æœ¬ä»£æœ€ä½³: {population.best().fitness:.4f}")
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            intermediate_file = exp_dir / f"generation_{generation}_results.json"
            intermediate_data = {
                "generation": generation,
                "duration": gen_time,
                "best_fitness": population.best().fitness,
                "best_prompt": population.best().prompt,
                "fitness_history": best_fitness_history,
                "sample_batches_processed": batch_count
            }
            with open(intermediate_file, 'w', encoding='utf-8') as f:
                json.dump(intermediate_data, f, indent=2, ensure_ascii=False)
        
        # æœ€ç»ˆç»“æœ
        final_best = population.best()
        total_time = time.time() - start_time
        
        print(f"\nğŸ‰ æ ·æœ¬çº§åé¦ˆè¿›åŒ–å®Œæˆ!")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"   æœ€ç»ˆæœ€ä½³é€‚åº”åº¦: {final_best.fitness:.4f}")
        
        if best_fitness_history:
            improvement = final_best.fitness - best_fitness_history[0]
            print(f"   æ€»ä½“æå‡: {improvement:+.4f}")
        
        # ä¿å­˜æ ·æœ¬ç»Ÿè®¡
        sample_tracker.save_statistics()
        
        final_results = {
            "experiment_id": config['experiment_id'],
            "best_prompt": final_best.prompt,
            "best_fitness": final_best.fitness,
            "fitness_history": best_fitness_history,
            "total_time": total_time,
            "algorithm": config['algorithm'],
            "population_size": config['population_size'],
            "sample_wise_feedback": True,
            "training_samples": len(train_samples)
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        prompt_tracker.save_summary(final_results)
        
        return final_results, exp_dir
        
    except Exception as e:
        print(f"âŒ æ ·æœ¬çº§åé¦ˆè¿›åŒ–å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise


def create_mutant_prompt(target_prompt: str, parent_prompts: List[str], 
                        llm_client, config: dict) -> str:
    """ä½¿ç”¨LLMåˆ›å»ºå˜å¼‚prompt"""
    mutation_instruction = f"""
Create an improved version of the target prompt by incorporating ideas from the parent prompts.

Target Prompt:
{target_prompt}

Parent Prompts:
1. {parent_prompts[0]}
2. {parent_prompts[1]}  
3. {parent_prompts[2]}

Task: Create a new prompt that combines the best aspects of these prompts for vulnerability detection. The new prompt should:
- Maintain the same input format {{input}}
- Respond with 'vulnerable' or 'benign' 
- Be more effective at detecting security issues
- Incorporate different analysis approaches from the parents

Generate only the improved prompt, nothing else:
"""
    
    try:
        response = llm_client.generate(mutation_instruction, temperature=0.8, max_tokens=200)
        # ç¡®ä¿åŒ…å«{input}å ä½ç¬¦
        if '{input}' not in response:
            response = response + "\n\nCode: {input}\n\nSecurity assessment:"
        return response.strip()
    except Exception as e:
        print(f"     âš ï¸ å˜å¼‚æ“ä½œå¤±è´¥: {e}")
        return target_prompt


def sample_wise_feedback_training(initial_prompt: str, train_samples: List[dict], 
                                llm_client, sample_tracker: SampleWiseTracker,
                                config: dict, generation: int, prompt_id: str,
                                batch_idx: int) -> str:
    """ä½¿ç”¨è®­ç»ƒæ ·æœ¬è¿›è¡Œæ ·æœ¬çº§åé¦ˆè®­ç»ƒ"""
    current_prompt = initial_prompt
    batch_size = config.get('feedback_batch_size', 10)
    
    # éšæœºé€‰æ‹©ä¸€æ‰¹è®­ç»ƒæ ·æœ¬
    selected_samples = random.sample(train_samples, min(batch_size, len(train_samples)))
    
    print(f"     ğŸ“ æ ·æœ¬çº§åé¦ˆè®­ç»ƒ: {len(selected_samples)} ä¸ªæ ·æœ¬")
    
    improvements_count = 0
    
    for sample_idx, sample in enumerate(selected_samples):
        try:
            # ä½¿ç”¨å½“å‰prompté¢„æµ‹
            code = sample.input_text
            ground_truth = int(sample.target)
            
            query = current_prompt.format(input=code)
            prediction_text = llm_client.generate(query, temperature=0.1, max_tokens=10)
            
            # è§£æé¢„æµ‹ç»“æœ
            prediction_binary = 1 if 'vulnerable' in prediction_text.lower() else 0
            correct = (prediction_binary == ground_truth)
            
            # è½¬æ¢Sampleå¯¹è±¡ä¸ºå­—å…¸æ ¼å¼
            sample_data = {
                'func': sample.input_text,
                'target': int(sample.target),
                'project': sample.metadata.get('project', ''),
                'cwe': sample.metadata.get('cwe', []),
                'cve': sample.metadata.get('cve', 'None'),
                'cve_desc': sample.metadata.get('cve_desc', 'None'),
                'func_hash': sample.metadata.get('func_hash', ''),
                'file_name': sample.metadata.get('file_name', '')
            }
            
            # è®°å½•æ ·æœ¬ç»“æœ
            sample_tracker.log_sample_result(
                prompt_id=f"{prompt_id}_feedback_{batch_idx}",
                sample_idx=sample_idx,
                sample_data=sample_data,
                prediction=prediction_text,
                ground_truth=ground_truth,
                correct=correct,
                generation=generation,
                feedback_applied=True
            )
            
            # å¦‚æœé¢„æµ‹é”™è¯¯ï¼Œå°è¯•æ”¹è¿›prompt
            if not correct:
                # æ„å»ºCWEç›¸å…³çš„åé¦ˆä¿¡æ¯
                cwe_info = ""
                if sample_data.get('cwe') and sample_data['cwe']:
                    cwe_list = ", ".join(sample_data['cwe'])
                    cwe_info = f"\nCWE Categories: {cwe_list}"
                
                cve_info = ""
                if sample_data.get('cve') and sample_data['cve'] != 'None':
                    cve_info = f"\nCVE ID: {sample_data['cve']}"
                
                project_info = ""
                if sample_data.get('project'):
                    project_info = f"\nProject: {sample_data['project']}"
                
                feedback_instruction = f"""
The current prompt made an incorrect prediction. Please improve it based on the specific vulnerability information.

Current Prompt:
{current_prompt}

Code Sample:
{code[:500]}...

Ground Truth: {"vulnerable" if ground_truth == 1 else "benign"}
Predicted: {"vulnerable" if prediction_binary == 1 else "benign"}{project_info}{cwe_info}{cve_info}

Create an improved prompt that would correctly classify this sample. Focus on:
1. The specific CWE categories mentioned above (if any)
2. The vulnerability patterns or security aspects this sample demonstrates  
3. Common security issues in {sample_data.get('project', 'this type of')} code

Improved prompt:
"""
                
                try:
                    improved_prompt = llm_client.generate(feedback_instruction, temperature=0.7, max_tokens=250)
                    if '{input}' in improved_prompt and len(improved_prompt.strip()) > 50:
                        current_prompt = improved_prompt.strip()
                        improvements_count += 1
                        print(f"       âš¡ æ ·æœ¬ {sample_idx+1}: promptå·²æ”¹è¿›")
                except Exception as e:
                    print(f"       âš ï¸ æ ·æœ¬ {sample_idx+1}: æ”¹è¿›å¤±è´¥ - {e}")
            else:
                print(f"       âœ… æ ·æœ¬ {sample_idx+1}: é¢„æµ‹æ­£ç¡®")
                
        except Exception as e:
            print(f"       âŒ æ ·æœ¬ {sample_idx+1}: å¤„ç†å¤±è´¥ - {e}")
    
    print(f"     ğŸ“ˆ åé¦ˆè®­ç»ƒå®Œæˆ: {improvements_count}/{len(selected_samples)} ä¸ªæ ·æœ¬è§¦å‘æ”¹è¿›")
    return current_prompt


def evaluate_on_dataset(prompt: str, dataset, llm_client, prompt_id: str,
                       sample_tracker: SampleWiseTracker, generation: int) -> float:
    """åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°promptæ€§èƒ½"""
    correct = 0
    samples = dataset.get_samples()
    total = len(samples)
    
    for idx, sample in enumerate(samples):
        try:
            code = sample.input_text
            ground_truth = int(sample.target)
            
            query = prompt.format(input=code)
            prediction_text = llm_client.generate(query, temperature=0.1, max_tokens=10)
            
            prediction_binary = 1 if 'vulnerable' in prediction_text.lower() else 0
            is_correct = (prediction_binary == ground_truth)
            
            if is_correct:
                correct += 1
            
            # è®°å½•è¯„ä¼°ç»“æœ - å°†Sampleå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            sample_data = {
                'func': sample.input_text,
                'target': int(sample.target),
                'project': sample.metadata.get('project', ''),
                'cwe': sample.metadata.get('cwe', []),
                'cve': sample.metadata.get('cve', 'None'),
                'cve_desc': sample.metadata.get('cve_desc', 'None'),
                'func_hash': sample.metadata.get('func_hash', ''),
                'file_name': sample.metadata.get('file_name', '')
            }
            
            sample_tracker.log_sample_result(
                prompt_id=prompt_id,
                sample_idx=idx,
                sample_data=sample_data,
                prediction=prediction_text,
                ground_truth=ground_truth,
                correct=is_correct,
                generation=generation,
                feedback_applied=False
            )
            
        except Exception as e:
            print(f"     âš ï¸ æ ·æœ¬ {idx} è¯„ä¼°å¤±è´¥: {e}")
    
    accuracy = correct / total if total > 0 else 0
    print(f"     ğŸ“Š è¯„ä¼°å®Œæˆ: {correct}/{total} = {accuracy:.4f}")
    return accuracy


def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ Primevul 1% æ ·æœ¬çº§åé¦ˆé«˜å¹¶å‘è¿›åŒ–å®éªŒ")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    
    # æ£€æŸ¥APIé…ç½®
    api_key = check_api_configuration()
    if not api_key:
        return 1
    
    # è·¯å¾„é…ç½®
    primevul_dir = "./data/primevul/primevul"
    sample_output_dir = "./data/primevul_1percent_sample"
    
    try:
        # 1. å‡†å¤‡æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not os.path.exists(sample_output_dir):
            if not os.path.exists(primevul_dir):
                print(f"âŒ Primevulæ•°æ®ç›®å½•ä¸å­˜åœ¨: {primevul_dir}")
                print("è¯·ç¡®ä¿Primevulæ•°æ®å·²ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®")
                return 1
            
            print("ğŸ“Š å‡†å¤‡1%é‡‡æ ·æ•°æ®...")
            sample_result = sample_primevul_1percent(primevul_dir, sample_output_dir, seed=42)
            print(f"âœ… é‡‡æ ·å®Œæˆ: {sample_result['total_samples']} æ ·æœ¬")
        else:
            print(f"âœ… ä½¿ç”¨å·²å­˜åœ¨çš„é‡‡æ ·æ•°æ®: {sample_output_dir}")
        
        # 2. åˆ›å»ºæ ·æœ¬çº§åé¦ˆé…ç½®
        config = create_optimized_config()
        config["api_key"] = api_key
        
        print(f"\nâš™ï¸ æ ·æœ¬çº§åé¦ˆå®éªŒé…ç½®:")
        print(f"   å®éªŒID: {config['experiment_id']}")
        print(f"   ç®—æ³•: {config['algorithm'].upper()}")
        print(f"   ç§ç¾¤å¤§å°: {config['population_size']}")
        print(f"   è¿›åŒ–ä»£æ•°: {config['max_generations']}")
        print(f"   æœ€å¤§å¹¶å‘: {config['max_concurrency']}")
        print(f"   è®­ç»ƒé›†æ‰“ä¹±: {config['shuffle_training_data']}")
        print(f"   æ ·æœ¬çº§åé¦ˆ: {config['sample_wise_feedback']}")
        print(f"   åé¦ˆæ‰¹å¤§å°: {config['feedback_batch_size']}")
        print(f"   è®°å½•æ‰€æœ‰æ ·æœ¬: {config['record_all_samples']}")
        
        # 3. è¿è¡Œæ ·æœ¬çº§åé¦ˆè¿›åŒ–
        results, exp_dir = run_concurrent_evolution_with_feedback(config, sample_output_dir)
        
        print(f"\nâœ… æ ·æœ¬çº§åé¦ˆå®éªŒå®Œæˆ!")
        print(f"ğŸ“‚ ç»“æœç›®å½•: {exp_dir}")
        print(f"ğŸ¯ æœ€ä½³é€‚åº”åº¦: {results['best_fitness']:.4f}")
        print(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
        print(f"   è®­ç»ƒæ ·æœ¬: {results['training_samples']}")
        print(f"   æ ·æœ¬çº§åé¦ˆ: {results['sample_wise_feedback']}")
        
        if results.get('fitness_history'):
            initial_fitness = results['fitness_history'][0]
            final_fitness = results['best_fitness']
            improvement = final_fitness - initial_fitness
            print(f"   é€‚åº”åº¦æå‡: {initial_fitness:.4f} â†’ {final_fitness:.4f} (+{improvement:.4f})")
        
        # 4. æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        print(f"\nğŸ“ ç”Ÿæˆçš„åˆ†ææ–‡ä»¶:")
        analysis_files = [
            "sample_feedback.jsonl",      # æ ·æœ¬çº§åé¦ˆè®°å½•
            "sample_statistics.json",     # æ ·æœ¬ç»Ÿè®¡
            "prompt_evolution.jsonl",     # promptè¿›åŒ–è®°å½•
            "experiment_config.json",     # å®éªŒé…ç½®
            "initial_prompts.txt",        # åˆå§‹prompts
            "experiment_summary.json"     # å®éªŒæ€»ç»“
        ]
        
        for filename in analysis_files:
            filepath = exp_dir / filename
            if filepath.exists():
                size = filepath.stat().st_size
                print(f"   âœ… {filename} ({size:,} bytes)")
            else:
                print(f"   âŒ {filename} (missing)")
        
        return 0
        
    except Exception as e:
        logger.error(f"æ ·æœ¬çº§åé¦ˆå®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())