#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å¹¶å‘åŠ é€Ÿçš„Primevul 1% Promptè¿›åŒ–å®éªŒ
æ”¯æŒè®­ç»ƒé›†æ‰“ä¹±å’Œæ¯ä¸ªæ ·æœ¬ç»“æœåé¦ˆæ›´æ–°prompt
"""

import os
import sys
import json
import time
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, 'src')

from evoprompt.data.sampler import sample_primevul_1percent
from evoprompt.workflows import VulnerabilityDetectionWorkflow
from evoprompt.core.prompt_tracker import PromptTracker
from evoprompt.algorithms.differential import DifferentialEvolution
from evoprompt.algorithms.base import Population
from evoprompt.data.dataset import PrimevulDataset
from evoprompt.data.cwe_categories import (
    CWE_MAJOR_CATEGORIES, map_cwe_to_major, canonicalize_category
)


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    import logging
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('primevul_concurrent_evolution.log', encoding='utf-8')
        ]
    )
    
    return logging.getLogger(__name__)


def check_api_configuration():
    """æ£€æŸ¥APIé…ç½®ï¼ˆChatAnywhereï¼‰"""
    # åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
    from src.evoprompt.llm.client import load_env_vars
    load_env_vars()
    
    api_key = os.getenv("API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® API_KEY ç¯å¢ƒå˜é‡")
        print("   åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: API_KEY='your-api-key-here'")
        return None
    
    # æ£€æŸ¥ChatAnywhereé…ç½®
    api_base = os.getenv("API_BASE_URL", "https://api.chatanywhere.tech/v1")
    model_name = os.getenv("MODEL_NAME", "gpt-3.5-turbo")
    
    print(f"âœ… ChatAnywhere APIé…ç½®æ£€æŸ¥é€šè¿‡:")
    print(f"   API_BASE_URL: {api_base}")
    print(f"   MODEL_NAME: {model_name}")
    print(f"   API_KEY: {api_key[:10]}...")
    
    return api_key


def create_optimized_config():
    """åˆ›å»ºä¼˜åŒ–çš„å®éªŒé…ç½®ï¼Œå……åˆ†åˆ©ç”¨å¹¶å‘ä¼˜åŠ¿"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    config = {
        # å®éªŒæ ‡è¯†
        "experiment_id": f"primevul_concurrent_1pct_{timestamp}",
        "experiment_name": "Primevul 1% Concurrent Optimized Evolution with Sample-wise Feedback",
        "timestamp": timestamp,
        
        # æ•°æ®é…ç½®
        "dataset": "primevul",
        "sample_ratio": 0.01,
        "balanced_sampling": True,
        "shuffle_training_data": True,  # å¯ç”¨è®­ç»ƒé›†æ‰“ä¹±
        
        # ç®—æ³•é…ç½® - ä¼˜åŒ–å‚æ•°ä»¥åˆ©ç”¨å¹¶å‘
        "algorithm": "de",
        "population_size": 8,    # è°ƒæ•´ç§ç¾¤å¤§å°é€‚åº”æ ·æœ¬çº§åé¦ˆ
        "max_generations": 5,    # è°ƒæ•´ä»£æ•°
        "mutation_rate": 0.15,
        "crossover_probability": 0.8,
        
        # æ ·æœ¬çº§åé¦ˆé…ç½®
        "sample_wise_feedback": True,     # å¯ç”¨æ ·æœ¬çº§åé¦ˆ
        "feedback_batch_size": 10,        # æ¯æ‰¹åé¦ˆçš„æ ·æœ¬æ•°
        "feedback_update_threshold": 0.1, # åé¦ˆæ›´æ–°é˜ˆå€¼
        "record_all_samples": True,       # è®°å½•æ‰€æœ‰æ ·æœ¬ç»“æœ
        
        # å¹¶å‘ä¼˜åŒ–é…ç½®
        "max_concurrency": 16,        # åŸºäºæµ‹è¯•çš„æœ€ä½³å¹¶å‘åº¦
        "force_async": True,          # å¼ºåˆ¶ä½¿ç”¨å¼‚æ­¥å¤„ç†
        "batch_evaluation": True,     # æ‰¹é‡è¯„ä¼°ä¼˜åŒ–
        
        # LLMæ‰¹å¤„ç†é…ç½®
        "llm_batch_size": 8,          # LLM APIè°ƒç”¨çš„æ‰¹å¤„ç†å¤§å°
        "enable_batch_processing": True, # å¯ç”¨æ‰¹å¤„ç†ä¼˜åŒ–
        "enable_concurrent": True,    # å¯ç”¨æ‰¹æ¬¡å†…å¹¶å‘å¤„ç†
        
        # LLMé…ç½®
        "llm_type": "gpt-3.5-turbo",
        "temperature": 0.7,
        
        # è¯„ä¼°é…ç½®
        "sample_size": None,
        "test_sample_size": None,
        
        # è¾“å‡ºé…ç½®
        "output_dir": "./outputs/primevul_concurrent_feedback",
        "save_population": True,
        "detailed_logging": True,
        
        # é«˜æ€§èƒ½è¿½è¸ªé…ç½®
        "track_every_evaluation": True,
        "save_intermediate_results": True,
        "export_top_k": 15,
        "save_sample_results": True,  # ä¿å­˜æ ·æœ¬çº§ç»“æœ

        # å¯ç”¨CWEå¤§ç±»æ¨¡å¼
        "use_cwe_major": True,
    }
    
    return config


def create_diverse_initial_prompts():
    """åˆ›å»ºå¤šæ ·åŒ–çš„åˆå§‹prompté›†åˆï¼Œæ”¯æŒCWEå¤§ç±»åˆ†ç±»"""
    # è·å–CWEå¤§ç±»åˆ—è¡¨ç”¨äºæç¤º
    categories_text = ", ".join([f"'{cat}'" for cat in CWE_MAJOR_CATEGORIES])
    
    initial_prompts = [
        # CWEå¤§ç±»å¯¼å‘åˆ†æ
        f"Analyze this code for security vulnerabilities and classify it into one of these CWE major categories: {categories_text}. If no vulnerability is found, respond with 'Benign'.\n\nCode to analyze:\n{{input}}\n\nCWE Major Category:",
        
        f"You are a security expert. Examine this code and identify the primary CWE major category from: {categories_text}. For secure code, use 'Benign'.\n\nCode: {{input}}\n\nCWE Classification:",
        
        # å…·ä½“åˆ†æå¯¼å‘ç±»
        f"Perform detailed security analysis and classify into CWE major categories:\n- Buffer Errors: buffer overflows, bounds checking issues\n- Injection: SQL, command, XSS injection attacks\n- Memory Management: use-after-free, double-free, memory leaks\n- Pointer Dereference: null pointer, invalid pointer usage\n- Integer Errors: overflow, underflow, wraparound\n- Concurrency Issues: race conditions, synchronization problems\n- Path Traversal: directory traversal attacks\n- Cryptography Issues: weak crypto, broken algorithms\n- Information Exposure: data leaks, privacy issues\n- Other: other security issues\n- Benign: no vulnerabilities\n\nCode: {{input}}\n\nCategory:",
        
        f"Identify the primary vulnerability type. Choose from: {categories_text}. Focus on the most significant security issue present.\n\n{{input}}\n\nPrimary vulnerability category:",
        
        # ä¸“å®¶è§’è‰²ç±»
        f"As a cybersecurity expert, classify this code's primary security issue using CWE major categories: {categories_text}. Use 'Benign' for secure code.\n\nCode under review:\n{{input}}\n\nExpert classification:",
        
        f"Security code review: Examine for buffer errors, injection flaws, memory issues, pointer problems, integer errors, concurrency bugs, path traversal, crypto weaknesses, or information exposure. Classify accordingly or mark as 'Benign'.\n\nCode: {{input}}\n\nSecurity classification:",
        
        # ç»“æ„åŒ–åˆ†æç±»
        f"Systematic vulnerability analysis:\n1. Check for buffer/bounds issues â†’ Buffer Errors\n2. Look for injection vectors â†’ Injection\n3. Analyze memory management â†’ Memory Management\n4. Check pointer usage â†’ Pointer Dereference\n5. Review integer operations â†’ Integer Errors\n6. Examine concurrency â†’ Concurrency Issues\n7. Check path handling â†’ Path Traversal\n8. Review cryptography â†’ Cryptography Issues\n9. Look for data leaks â†’ Information Exposure\n10. Other security issues â†’ Other\n11. No issues â†’ Benign\n\nCode: {{input}}\n\nResult:",
        
        # CWEæ¨¡å¼è¯†åˆ«ç±»
        f"Identify CWE patterns and map to major categories. Examples:\n- CWE-120,119,787: Buffer Errors\n- CWE-78,79,89: Injection\n- CWE-416,415,401: Memory Management\n- CWE-476: Pointer Dereference\n- CWE-190,191: Integer Errors\n- CWE-362: Concurrency Issues\n- CWE-22: Path Traversal\n- CWE-327,326: Cryptography Issues\n- CWE-200: Information Exposure\n\nClassify: {{input}}\n\nCWE Major Category:",
        
        # æ”»å‡»åœºæ™¯ç±»
        f"From an attacker's perspective, what's the primary exploitable weakness? Categorize as: {categories_text}.\n\n{{input}}\n\nExploitable weakness category:",
        
        # ç®€æ´é«˜æ•ˆç±»
        f"Security category classification. Options: {categories_text}.\n\nCode: {{input}}\n\nCategory:",
        
        f"Vulnerability type identification. Choose the most appropriate: Buffer Errors, Injection, Memory Management, Pointer Dereference, Integer Errors, Concurrency Issues, Path Traversal, Cryptography Issues, Information Exposure, Other, or Benign.\n\n{{input}}\n\nType:",
        
        # é˜²å¾¡è§’åº¦ç±»
        f"Defense-focused analysis: What type of security control would prevent exploitation? Map to vulnerability categories: {categories_text}.\n\nCode to protect:\n{{input}}\n\nVulnerability category:"
    ]
    
    return initial_prompts


class SampleWiseTracker:
    """æ ·æœ¬çº§ç»“æœè¿½è¸ªå™¨"""
    
    def __init__(self, exp_dir: Path):
        self.exp_dir = exp_dir
        self.sample_results = []
        self.sample_feedback_log = exp_dir / "sample_feedback.jsonl"
        self.sample_stats = exp_dir / "sample_statistics.json"
        
    def log_sample_result(self, prompt_id: str, sample_idx: int, sample_data: dict, 
                         prediction: str, ground_truth: int, correct: bool, 
                         generation: int, feedback_applied: bool = False):
        """è®°å½•å•ä¸ªæ ·æœ¬çš„ç»“æœ"""
        cwe_codes = sample_data.get('cwe', [])
        result = {
            "timestamp": datetime.now().isoformat(),
            "prompt_id": prompt_id,
            "generation": generation,
            "sample_idx": sample_idx,
            "sample_func": sample_data.get('func', '')[:100],  # æˆªå–å‰100å­—ç¬¦
            "sample_target": ground_truth,
            "prediction": prediction,
            "correct": correct,
            "feedback_applied": feedback_applied,
            "cwe_codes": cwe_codes,  # æ˜ç¡®è¾“å‡ºCWEä»£ç 
            "cve_id": sample_data.get('cve', 'None'),
            "metadata": {
                "project": sample_data.get('project', ''),
                "cwe": cwe_codes,
                "cve": sample_data.get('cve', 'None'),
                "cve_desc": sample_data.get('cve_desc', 'None'),
                "func_hash": sample_data.get('func_hash', ''),
                "file_name": sample_data.get('file_name', ''),
                "ground_truth_category": sample_data.get('ground_truth_category', 'Unknown'),
                "predicted_category": sample_data.get('predicted_category', 'Unknown')
            }
        }
        
        self.sample_results.append(result)
        
        # å®æ—¶å†™å…¥æ—¥å¿—
        with open(self.sample_feedback_log, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    def get_recent_performance(self, prompt_id: str, last_n: int = 10):
        """è·å–æœ€è¿‘Nä¸ªæ ·æœ¬çš„æ€§èƒ½"""
        recent_results = [r for r in self.sample_results 
                         if r['prompt_id'] == prompt_id][-last_n:]
        
        if not recent_results:
            return {"accuracy": 0.0, "count": 0}
            
        correct_count = sum(1 for r in recent_results if r['correct'])
        return {
            "accuracy": correct_count / len(recent_results),
            "count": len(recent_results),
            "correct": correct_count,
            "total": len(recent_results)
        }
    
    def save_statistics(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ”¯æŒCWEå¤§ç±»å¤šåˆ†ç±»ï¼‰"""
        if not self.sample_results:
            return
            
        # æŒ‰promptç»Ÿè®¡
        prompt_stats = {}
        cwe_analysis = {}
        category_analysis = {}  # æ–°å¢ï¼šCWEå¤§ç±»åˆ†æ
        confusion_matrix = {}   # æ–°å¢ï¼šæ··æ·†çŸ©é˜µ
        
        for result in self.sample_results:
            prompt_id = result['prompt_id']
            if prompt_id not in prompt_stats:
                prompt_stats[prompt_id] = {
                    "total_samples": 0,
                    "correct_samples": 0,
                    "accuracy": 0.0,
                    "generations": set(),
                    "feedback_applied_count": 0
                }
            
            stats = prompt_stats[prompt_id]
            stats["total_samples"] += 1
            if result['correct']:
                stats["correct_samples"] += 1
            stats["generations"].add(result['generation'])
            if result['feedback_applied']:
                stats["feedback_applied_count"] += 1
            
            # CWEå¤§ç±»åˆ†æï¼ˆæ–°å¢ï¼‰
            # ä»æ ·æœ¬ç»“æœçš„metadataä¸­è·å–ç±»åˆ«ä¿¡æ¯
            metadata = result.get('metadata', {})
            ground_truth_cat = metadata.get('ground_truth_category', 'Unknown')
            predicted_cat = metadata.get('predicted_category', 'Unknown')
            
            # æ›´æ–°æ··æ·†çŸ©é˜µ
            if ground_truth_cat not in confusion_matrix:
                confusion_matrix[ground_truth_cat] = {}
            if predicted_cat not in confusion_matrix[ground_truth_cat]:
                confusion_matrix[ground_truth_cat][predicted_cat] = 0
            confusion_matrix[ground_truth_cat][predicted_cat] += 1
            
            # æŒ‰CWEå¤§ç±»ç»Ÿè®¡
            if ground_truth_cat not in category_analysis:
                category_analysis[ground_truth_cat] = {
                    "total_samples": 0,
                    "correct_predictions": 0,
                    "accuracy": 0.0,
                    "predicted_as": {}  # è¢«é¢„æµ‹ä¸ºå„ä¸ªç±»åˆ«çš„æ¬¡æ•°
                }
            
            cat_stats = category_analysis[ground_truth_cat]
            cat_stats["total_samples"] += 1
            if result['correct']:
                cat_stats["correct_predictions"] += 1
            
            # è®°å½•é¢„æµ‹åˆ†å¸ƒ
            if predicted_cat not in cat_stats["predicted_as"]:
                cat_stats["predicted_as"][predicted_cat] = 0
            cat_stats["predicted_as"][predicted_cat] += 1
            
            # åŸæœ‰CWEç»Ÿè®¡åˆ†æï¼ˆä¿ç•™å…¼å®¹ï¼‰
            cwe_codes = result.get('cwe_codes', [])
            for cwe in cwe_codes:
                if cwe not in cwe_analysis:
                    cwe_analysis[cwe] = {
                        "total_samples": 0,
                        "correct_predictions": 0,
                        "false_positives": 0,
                        "false_negatives": 0,
                        "accuracy": 0.0
                    }
                
                cwe_stats = cwe_analysis[cwe]
                cwe_stats["total_samples"] += 1
                
                if result['correct']:
                    cwe_stats["correct_predictions"] += 1
                else:
                    # åˆ†æé”™è¯¯ç±»å‹
                    if result['sample_target'] == 1:  # å®é™…æ˜¯æ¼æ´ï¼Œä½†é¢„æµ‹é”™äº†
                        cwe_stats["false_negatives"] += 1
                    else:  # å®é™…ä¸æ˜¯æ¼æ´ï¼Œä½†é¢„æµ‹æ˜¯æ¼æ´
                        cwe_stats["false_positives"] += 1
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        for prompt_id in prompt_stats:
            stats = prompt_stats[prompt_id]
            stats["accuracy"] = stats["correct_samples"] / stats["total_samples"] if stats["total_samples"] > 0 else 0
            stats["generations"] = list(stats["generations"])
        
        # è®¡ç®—CWEå¤§ç±»å‡†ç¡®ç‡
        for cat in category_analysis:
            cat_stats = category_analysis[cat]
            cat_stats["accuracy"] = cat_stats["correct_predictions"] / cat_stats["total_samples"] if cat_stats["total_samples"] > 0 else 0
        
        # è®¡ç®—åŸæœ‰CWEå‡†ç¡®ç‡
        for cwe in cwe_analysis:
            cwe_stats = cwe_analysis[cwe]
            cwe_stats["accuracy"] = cwe_stats["correct_predictions"] / cwe_stats["total_samples"] if cwe_stats["total_samples"] > 0 else 0
            cwe_stats["precision"] = cwe_stats["correct_predictions"] / (cwe_stats["correct_predictions"] + cwe_stats["false_positives"]) if (cwe_stats["correct_predictions"] + cwe_stats["false_positives"]) > 0 else 0
            cwe_stats["recall"] = cwe_stats["correct_predictions"] / (cwe_stats["correct_predictions"] + cwe_stats["false_negatives"]) if (cwe_stats["correct_predictions"] + cwe_stats["false_negatives"]) > 0 else 0
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        final_stats = {
            "total_samples_evaluated": len(self.sample_results),
            "total_prompts": len(prompt_stats),
            "overall_accuracy": sum(1 for r in self.sample_results if r['correct']) / len(self.sample_results),
            "prompt_statistics": prompt_stats,
            "generation_summary": self._get_generation_summary(),
            "cwe_major_category_analysis": category_analysis,  # æ–°å¢ï¼šCWEå¤§ç±»åˆ†æ
            "confusion_matrix": confusion_matrix,              # æ–°å¢ï¼šæ··æ·†çŸ©é˜µ
            "category_summary": {                              # æ–°å¢ï¼šç±»åˆ«æ€»ç»“
                "total_categories": len(category_analysis),
                "best_performing_categories": sorted(category_analysis.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:5],
                "worst_performing_categories": sorted(category_analysis.items(), key=lambda x: x[1]["accuracy"])[:5],
                "most_common_categories": sorted(category_analysis.items(), key=lambda x: x[1]["total_samples"], reverse=True)[:5]
            },
            "cwe_analysis": cwe_analysis,                      # ä¿ç•™åŸæœ‰ç»Ÿè®¡
            "cwe_summary": {
                "total_cwe_types": len(cwe_analysis),
                "most_common_cwes": sorted(cwe_analysis.items(), key=lambda x: x[1]["total_samples"], reverse=True)[:10],
                "best_performing_cwes": sorted(cwe_analysis.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:10],
                "worst_performing_cwes": sorted(cwe_analysis.items(), key=lambda x: x[1]["accuracy"])[:10]
            }
        }
        
        with open(self.sample_stats, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, indent=2, ensure_ascii=False)
    
    def _get_generation_summary(self):
        """è·å–ä»£é™…æ€»ç»“"""
        gen_stats = {}
        for result in self.sample_results:
            gen = result['generation']
            if gen not in gen_stats:
                gen_stats[gen] = {"total": 0, "correct": 0}
            gen_stats[gen]["total"] += 1
            if result['correct']:
                gen_stats[gen]["correct"] += 1
        
        for gen in gen_stats:
            gen_stats[gen]["accuracy"] = gen_stats[gen]["correct"] / gen_stats[gen]["total"]
        
        return gen_stats


def run_concurrent_evolution_with_feedback(config: dict, sample_data_dir: str):
    """è¿è¡Œæ”¯æŒæ ·æœ¬çº§åé¦ˆçš„é«˜å¹¶å‘è¿›åŒ–å®éªŒ"""
    print(f"âš¡ å¼€å§‹æ ·æœ¬çº§åé¦ˆçš„é«˜å¹¶å‘Promptè¿›åŒ–å®éªŒ: {config['experiment_id']}")
    print(f"ğŸ”¥ ä½¿ç”¨ {config['max_concurrency']} ä¸ªå¹¶å‘è¿æ¥")
    print(f"ğŸ“Š æ ·æœ¬çº§åé¦ˆ: æ¯æ‰¹ {config['feedback_batch_size']} ä¸ªæ ·æœ¬")
    if config.get('use_cwe_major'):
        print("ğŸ” å·²å¯ç”¨ CWE å¤§ç±»æ¨¡å¼ï¼šå›ºå®šè¦æ±‚æ¨¡å‹åªè¾“å‡ºå¤§ç±»ï¼ˆæˆ–Benignï¼‰ä½œä¸ºè¯„ä¼°ä¾æ®")
    
    # åˆ›å»ºå®éªŒç›®å½•
    exp_dir = Path(config['output_dir']) / config['experiment_id']
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ ·æœ¬è¿½è¸ªå™¨
    sample_tracker = SampleWiseTracker(exp_dir)
    
    # åˆ›å»ºpromptè¿½è¸ªå™¨
    prompt_tracker = PromptTracker(str(config['output_dir']), config['experiment_id'])
    prompt_tracker.set_config(config)
    
    # è®¾ç½®æ•°æ®è·¯å¾„
    dev_file = Path(sample_data_dir) / "dev.txt"
    train_file = Path(sample_data_dir) / "train.txt"
    
    print(f"ğŸ“ æ•°æ®é…ç½®:")
    print(f"   å¼€å‘é›†: {dev_file}")
    print(f"   è®­ç»ƒé›†: {train_file}")
    
    # åŠ è½½å’Œæ‰“ä¹±è®­ç»ƒæ•°æ®
    print(f"ğŸ”„ åŠ è½½å¹¶æ‰“ä¹±è®­ç»ƒæ•°æ®...")
    train_dataset = PrimevulDataset(str(train_file), "train")
    train_samples = train_dataset.get_samples()  # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è·å–æ ·æœ¬
    
    if config.get('shuffle_training_data', True):
        random.seed(42)  # ç¡®ä¿å¯é‡ç°
        random.shuffle(train_samples)
        print(f"   âœ… è®­ç»ƒæ•°æ®å·²æ‰“ä¹±: {len(train_samples)} ä¸ªæ ·æœ¬")
    
    # åŠ è½½å¼€å‘é›†
    dev_dataset = PrimevulDataset(str(dev_file), "dev") 
    print(f"   å¼€å‘é›†: {len(dev_dataset)} ä¸ªæ ·æœ¬")
    
    # ä¿å­˜é…ç½®
    config_file = exp_dir / "experiment_config.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯ - æ”¯æŒå¹¶å‘æ‰¹å¤„ç†
    from evoprompt.llm.client import create_default_client
    llm_client = create_default_client()
    if hasattr(llm_client, 'max_concurrency'):
        llm_client.max_concurrency = config['max_concurrency']
    print(f"ğŸ¤– ä½¿ç”¨LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¹¶å‘æ‰¹å¤„ç† (concurrent={config['enable_concurrent']})")
    
    # åˆ›å»ºåˆå§‹prompts
    initial_prompts = create_diverse_initial_prompts()
    
    # ä¿å­˜åˆå§‹prompts
    initial_prompts_file = exp_dir / "initial_prompts.txt"
    with open(initial_prompts_file, 'w', encoding='utf-8') as f:
        f.write("Initial Prompts for Sample-wise Feedback Evolution\n")
        f.write("=" * 50 + "\n\n")
        for i, prompt in enumerate(initial_prompts, 1):
            f.write(f"Prompt {i}:\n{'-' * 20}\n{prompt}\n\n")
    
    print(f"ğŸ’¾ å®éªŒé…ç½®:")
    print(f"   åˆå§‹prompts: {len(initial_prompts)}")
    print(f"   ç§ç¾¤å¤§å°: {config['population_size']}")
    print(f"   æœ€å¤§ä»£æ•°: {config['max_generations']}")
    print(f"   åé¦ˆæ‰¹å¤§å°: {config['feedback_batch_size']}")
    print(f"   CWEå¤§ç±»æ¨¡å¼: {config.get('use_cwe_major', False)}")
    
    # åˆ›å»ºè¿›åŒ–ç®—æ³•
    algorithm = DifferentialEvolution({
        "population_size": config['population_size'],
        "max_generations": config['max_generations'],
        "mutation_factor": config.get('mutation_rate', 0.15),
        "crossover_probability": config.get('crossover_probability', 0.8)
    })
    
    # è®°å½•åˆå§‹prompts
    for i, prompt in enumerate(initial_prompts):
        prompt_tracker.log_prompt(
            prompt=prompt,
            generation=0,
            individual_id=f"initial_{i}",
            operation="initialization",
            metadata={"sample_wise_feedback": True, "prompt_index": i}
        )
    
    print(f"\nğŸš€ å¯åŠ¨æ ·æœ¬çº§åé¦ˆè¿›åŒ–...")
    start_time = time.time()
    
    try:
        # åˆå§‹åŒ–ç§ç¾¤
        from evoprompt.algorithms.base import Individual
        population_individuals = [Individual(prompt) for prompt in initial_prompts]
        population = Population(population_individuals)
        
        # åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°åˆå§‹ç§ç¾¤
        print(f"ğŸ“Š è¯„ä¼°åˆå§‹ç§ç¾¤...")
        for i, individual in enumerate(population.individuals):
            individual.fitness = evaluate_on_dataset(
                individual.prompt, dev_dataset, llm_client, f"initial_{i}", 
                sample_tracker, generation=0, config=config
            )
        
        print(f"   åˆå§‹é€‚åº”åº¦: {[f'{ind.fitness:.3f}' for ind in population.individuals]}")
        
        # è®°å½•åˆå§‹è¯„ä¼°
        prompt_tracker.log_population(population.individuals, generation=0, operation="initial_evaluation")
        
        best_fitness_history = []
        
        # æ ·æœ¬çº§åé¦ˆè¿›åŒ–å¾ªç¯
        for generation in range(1, config['max_generations'] + 1):
            print(f"\nâš¡ ç¬¬ {generation} ä»£æ ·æœ¬çº§åé¦ˆè¿›åŒ–...")
            gen_start_time = time.time()
            
            current_best = population.best()
            best_fitness_history.append(current_best.fitness)
            print(f"   å½“å‰æœ€ä½³é€‚åº”åº¦: {current_best.fitness:.4f}")
            
            # æ ·æœ¬çº§è®­ç»ƒå’Œåé¦ˆ
            new_individuals = []
            batch_count = 0
            
            for i, target_individual in enumerate(population.individuals):
                print(f"   å¤„ç†ä¸ªä½“ {i+1}/{len(population.individuals)}")
                
                # DEè¿›åŒ–æ“ä½œ
                candidates = [ind for j, ind in enumerate(population.individuals) if j != i]
                if len(candidates) >= 3:
                    # é€‰æ‹©ä¸‰ä¸ªä¸åŒçš„ä¸ªä½“
                    parents = random.sample(candidates, 3)
                    
                    # åˆ›å»ºå˜å¼‚ä¸ªä½“
                    mutant_prompt = create_mutant_prompt(
                        target_individual.prompt, 
                        [p.prompt for p in parents], 
                        llm_client, 
                        config
                    )
                    
                    if mutant_prompt and mutant_prompt != target_individual.prompt:
                        # åœ¨è®­ç»ƒæ ·æœ¬ä¸Šè¿›è¡Œæ ·æœ¬çº§åé¦ˆ
                        improved_prompt = sample_wise_feedback_training(
                            mutant_prompt,
                            train_samples,
                            llm_client,
                            sample_tracker,
                            config,
                            generation,
                            f"gen{generation}_individual_{i}",
                            batch_count
                        )
                        
                        # åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°æ”¹è¿›åçš„prompt
                        trial_individual = Individual(improved_prompt)
                        trial_individual.fitness = evaluate_on_dataset(
                            improved_prompt, dev_dataset, llm_client, 
                            f"gen{generation}_trial_{i}", sample_tracker, generation, config
                        )
                        
                        # è®°å½•è¯•éªŒä¸ªä½“
                        prompt_tracker.log_prompt(
                            prompt=improved_prompt,
                            fitness=trial_individual.fitness,
                            generation=generation,
                            individual_id=f"gen{generation}_trial_{i}",
                            operation="sample_feedback_evolution",
                            metadata={
                                "target_fitness": target_individual.fitness,
                                "improvement": trial_individual.fitness - target_individual.fitness,
                                "feedback_applied": True
                            }
                        )
                        
                        # é€‰æ‹©æ›´å¥½çš„ä¸ªä½“
                        if trial_individual.fitness > target_individual.fitness:
                            new_individuals.append(trial_individual)
                            print(f"     âœ… æ¥å—æ”¹è¿›ä¸ªä½“: {trial_individual.fitness:.4f} > {target_individual.fitness:.4f}")
                        else:
                            new_individuals.append(target_individual)
                            print(f"     âŒ ä¿ç•™åŸä¸ªä½“: {trial_individual.fitness:.4f} <= {target_individual.fitness:.4f}")
                        
                        batch_count += 1
                    else:
                        new_individuals.append(target_individual)
                        print(f"     âš ï¸ å˜å¼‚å¤±è´¥ï¼Œä¿ç•™åŸä¸ªä½“")
                else:
                    new_individuals.append(target_individual)
                    print(f"     âš ï¸ å€™é€‰ä¸è¶³ï¼Œä¿ç•™åŸä¸ªä½“")
            
            # æ›´æ–°ç§ç¾¤
            population = Population(new_individuals)
            
            # è®°å½•æœ¬ä»£ç»“æœ
            prompt_tracker.log_population(
                population.individuals,
                generation=generation,
                operation="sample_feedback_generation_complete"
            )
            
            gen_time = time.time() - gen_start_time
            print(f"   ç¬¬{generation}ä»£å®Œæˆ: {gen_time:.1f}ç§’")
            print(f"   æœ¬ä»£æœ€ä½³: {population.best().fitness:.4f}")
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            intermediate_file = exp_dir / f"generation_{generation}_results.json"
            intermediate_data = {
                "generation": generation,
                "duration": gen_time,
                "best_fitness": population.best().fitness,
                "best_prompt": population.best().prompt,
                "fitness_history": best_fitness_history,
                "sample_batches_processed": batch_count
            }
            with open(intermediate_file, 'w', encoding='utf-8') as f:
                json.dump(intermediate_data, f, indent=2, ensure_ascii=False)
        
        # æœ€ç»ˆç»“æœ
        final_best = population.best()
        total_time = time.time() - start_time
        
        print(f"\nğŸ‰ æ ·æœ¬çº§åé¦ˆè¿›åŒ–å®Œæˆ!")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"   æœ€ç»ˆæœ€ä½³é€‚åº”åº¦: {final_best.fitness:.4f}")
        
        if best_fitness_history:
            improvement = final_best.fitness - best_fitness_history[0]
            print(f"   æ€»ä½“æå‡: {improvement:+.4f}")
        
        # ä¿å­˜æ ·æœ¬ç»Ÿè®¡
        sample_tracker.save_statistics()
        
        final_results = {
            "experiment_id": config['experiment_id'],
            "best_prompt": final_best.prompt,
            "best_fitness": final_best.fitness,
            "fitness_history": best_fitness_history,
            "total_time": total_time,
            "algorithm": config['algorithm'],
            "population_size": config['population_size'],
            "sample_wise_feedback": True,
            "training_samples": len(train_samples)
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        prompt_tracker.save_summary(final_results)
        
        return final_results, exp_dir
        
    except Exception as e:
        print(f"âŒ æ ·æœ¬çº§åé¦ˆè¿›åŒ–å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise


def create_mutant_prompt(target_prompt: str, parent_prompts: List[str], 
                        llm_client, config: dict) -> str:
    """ä½¿ç”¨LLMåˆ›å»ºå˜å¼‚prompt"""
    mutation_instruction = f"""
Create an improved version of the target prompt by incorporating ideas from the parent prompts.

Target Prompt:
{target_prompt}

Parent Prompts:
1. {parent_prompts[0]}
2. {parent_prompts[1]}  
3. {parent_prompts[2]}

Task: Create a new prompt that combines the best aspects of these prompts for vulnerability detection. The new prompt should:
- Maintain the same input format {{input}}
- Respond with 'vulnerable' or 'benign' 
- Be more effective at detecting security issues
- Incorporate different analysis approaches from the parents

Generate only the improved prompt, nothing else:
"""
    
    try:
        response = llm_client.generate(mutation_instruction, temperature=0.8)
        # ç¡®ä¿åŒ…å«{input}å ä½ç¬¦
        if '{input}' not in response:
            response = response + "\n\nCode: {input}\n\nSecurity assessment:"
        return response.strip()
    except Exception as e:
        print(f"     âš ï¸ å˜å¼‚æ“ä½œå¤±è´¥: {e}")
        return target_prompt


def sample_wise_feedback_training(initial_prompt: str, train_samples, 
                                llm_client, sample_tracker: SampleWiseTracker,
                                config: dict, generation: int, prompt_id: str,
                                batch_idx: int) -> str:
    """ä½¿ç”¨è®­ç»ƒæ ·æœ¬è¿›è¡Œæ ·æœ¬çº§åé¦ˆè®­ç»ƒï¼ˆæ”¯æŒæ‰¹å¤„ç†ï¼‰"""
    current_prompt = initial_prompt
    batch_size = config.get('feedback_batch_size', 10)
    enable_batch = config.get('enable_batch_processing', False)
    llm_batch_size = config.get('llm_batch_size', 8)
    enable_concurrent = config.get('enable_concurrent', True)
    
    # éšæœºé€‰æ‹©ä¸€æ‰¹è®­ç»ƒæ ·æœ¬
    selected_samples = random.sample(train_samples, min(batch_size, len(train_samples)))
    
    print(f"     ğŸ“ æ ·æœ¬çº§åé¦ˆè®­ç»ƒ: {len(selected_samples)} ä¸ªæ ·æœ¬")
    if enable_batch:
        concurrent_text = "å¹¶å‘" if enable_concurrent else "é¡ºåº"
        print(f"     ğŸš€ ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼ï¼ŒLLM batch_size={llm_batch_size} ({concurrent_text})")
    
    improvements_count = 0
    
    if enable_batch and len(selected_samples) > 1:
        # æ‰¹å¤„ç†æ¨¡å¼ï¼šå…ˆæ‰¹é‡é¢„æµ‹æ‰€æœ‰æ ·æœ¬
        try:
            # å‡†å¤‡æ‰¹é‡é¢„æµ‹æ•°æ®
            batch_queries = []
            sample_metadata = []
            
            for sample_idx, sample in enumerate(selected_samples):
                code = sample.input_text
                ground_truth_binary = int(sample.target)
                
                # ä»æ ·æœ¬çš„CWEä»£ç è·å–çœŸå®çš„CWEå¤§ç±»
                cwe_codes = sample.metadata.get('cwe', [])
                if ground_truth_binary == 1 and cwe_codes:
                    ground_truth_category = map_cwe_to_major(cwe_codes)
                else:
                    ground_truth_category = "Benign"
                
                query = current_prompt.format(input=code)
                batch_queries.append(query)
                sample_metadata.append({
                    'sample_idx': sample_idx,
                    'sample': sample,
                    'code': code,
                    'ground_truth_binary': ground_truth_binary,
                    'ground_truth_category': ground_truth_category,
                    'cwe_codes': cwe_codes
                })
            
            # æ‰¹é‡è°ƒç”¨LLMè¿›è¡Œé¢„æµ‹
            prediction_texts = llm_client.batch_generate(
                batch_queries, 
                temperature=0.1, 
                max_tokens=20,
                batch_size=llm_batch_size,
                concurrent=enable_concurrent
            )
            
            # å¤„ç†æ‰¹é‡é¢„æµ‹ç»“æœ
            incorrect_samples = []  # æ”¶é›†éœ€è¦æ”¹è¿›çš„æ ·æœ¬
            
            for metadata, prediction_text in zip(sample_metadata, prediction_texts):
                if prediction_text == "error":
                    print(f"       âš ï¸ æ ·æœ¬ {metadata['sample_idx']+1}: é¢„æµ‹å¤±è´¥")
                    continue
                
                # è§„èŒƒåŒ–æ¨¡å‹è¾“å‡ºåˆ°CWEå¤§ç±»
                predicted_category = canonicalize_category(prediction_text)
                if predicted_category is None:
                    if any(word in prediction_text.lower() for word in ['vulnerable', 'vulnerability', 'vuln', 'exploit']):
                        predicted_category = "Other"
                    else:
                        predicted_category = "Benign"
                
                correct = (predicted_category == metadata['ground_truth_category'])
                
                # è®°å½•æ ·æœ¬ç»“æœ
                sample_data = {
                    'func': metadata['sample'].input_text,
                    'target': metadata['ground_truth_binary'],
                    'project': metadata['sample'].metadata.get('project', ''),
                    'cwe': metadata['cwe_codes'],
                    'cve': metadata['sample'].metadata.get('cve', 'None'),
                    'cve_desc': metadata['sample'].metadata.get('cve_desc', 'None'),
                    'func_hash': metadata['sample'].metadata.get('func_hash', ''),
                    'file_name': metadata['sample'].metadata.get('file_name', ''),
                    'ground_truth_category': metadata['ground_truth_category'],
                    'predicted_category': predicted_category
                }
                
                sample_tracker.log_sample_result(
                    prompt_id=f"{prompt_id}_feedback_{batch_idx}",
                    sample_idx=metadata['sample_idx'],
                    sample_data=sample_data,
                    prediction=prediction_text,
                    ground_truth=metadata['ground_truth_binary'],
                    correct=correct,
                    generation=generation,
                    feedback_applied=True
                )
                
                # æ”¶é›†é”™è¯¯çš„æ ·æœ¬ç”¨äºæ”¹è¿›
                if not correct:
                    incorrect_samples.append({
                        'metadata': metadata,
                        'prediction_text': prediction_text,
                        'predicted_category': predicted_category,
                        'sample_data': sample_data
                    })
                else:
                    print(f"       âœ… æ ·æœ¬ {metadata['sample_idx']+1}: é¢„æµ‹æ­£ç¡®")
            
            # æ‰¹é‡ç”Ÿæˆæ”¹è¿›æŒ‡ä»¤ï¼ˆå¯¹äºé”™è¯¯çš„æ ·æœ¬ï¼‰
            if incorrect_samples:
                improvement_instructions = []
                
                for item in incorrect_samples:
                    metadata = item['metadata']
                    sample_data = item['sample_data']
                    
                    # æ„å»ºåé¦ˆä¿¡æ¯
                    cwe_info = ""
                    if sample_data.get('cwe') and sample_data['cwe']:
                        cwe_list = ", ".join(sample_data['cwe'])
                        cwe_info = f"\nCWE Categories: {cwe_list}"
                    
                    cve_info = ""
                    if sample_data.get('cve') and sample_data['cve'] != 'None':
                        cve_info = f"\nCVE ID: {sample_data['cve']}"
                    
                    project_info = ""
                    if sample_data.get('project'):
                        project_info = f"\nProject: {sample_data['project']}"
                    
                    feedback_instruction = f"""
The current prompt made an incorrect CWE major category classification. Please improve it based on the specific vulnerability information.

Current Prompt:
{current_prompt}

Code Sample:
{metadata['code'][:500]}...

Ground Truth Category: {metadata['ground_truth_category']}
Predicted Category: {item['predicted_category']}{project_info}{cwe_info}{cve_info}

Create an improved prompt that would correctly classify this sample into the correct CWE major category. Focus on:
1. The specific CWE categories: {metadata['ground_truth_category']} characteristics
2. The vulnerability patterns that distinguish {metadata['ground_truth_category']} from other categories
3. Common {metadata['ground_truth_category']} issues in {sample_data.get('project', 'this type of')} code
4. Ensure the prompt can distinguish between all CWE major categories: {", ".join(CWE_MAJOR_CATEGORIES)}

Improved prompt:
"""
                    improvement_instructions.append(feedback_instruction)
                
                # æ‰¹é‡ç”Ÿæˆæ”¹è¿›çš„prompt
                try:
                    improved_prompts = llm_client.batch_generate(
                        improvement_instructions,
                        temperature=0.7,
                        batch_size=llm_batch_size,
                        concurrent=enable_concurrent
                    )
                    
                    # é€‰æ‹©æœ€å¥½çš„æ”¹è¿›ï¼ˆè¿™é‡Œç®€å•é€‰æ‹©ç¬¬ä¸€ä¸ªæˆåŠŸçš„ï¼‰
                    for i, improved_prompt in enumerate(improved_prompts):
                        if improved_prompt != "error" and '{input}' in improved_prompt and len(improved_prompt.strip()) > 50:
                            current_prompt = improved_prompt.strip()
                            improvements_count += 1
                            print(f"       âš¡ æ‰¹é‡æ”¹è¿›æˆåŠŸï¼Œåº”ç”¨æ”¹è¿› {i+1}")
                            break  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„æ”¹è¿›
                    
                except Exception as e:
                    print(f"       âš ï¸ æ‰¹é‡æ”¹è¿›å¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"     âŒ æ‰¹å¤„ç†æ¨¡å¼å¤±è´¥: {e}ï¼Œå›é€€åˆ°å•ä¸ªå¤„ç†æ¨¡å¼")
            enable_batch = False
    
    if not enable_batch:
        # å•ä¸ªå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        for sample_idx, sample in enumerate(selected_samples):
            try:
                # ä½¿ç”¨å½“å‰prompté¢„æµ‹
                code = sample.input_text
                ground_truth_binary = int(sample.target)
                
                # ä»æ ·æœ¬çš„CWEä»£ç è·å–çœŸå®çš„CWEå¤§ç±»
                cwe_codes = sample.metadata.get('cwe', [])
                if ground_truth_binary == 1 and cwe_codes:
                    ground_truth_category = map_cwe_to_major(cwe_codes)
                else:
                    ground_truth_category = "Benign"
                
                query = current_prompt.format(input=code)
                prediction_text = llm_client.generate(query, temperature=0.1, max_tokens=20)
                
                # è§„èŒƒåŒ–æ¨¡å‹è¾“å‡ºåˆ°CWEå¤§ç±»
                predicted_category = canonicalize_category(prediction_text)
                if predicted_category is None:
                    if any(word in prediction_text.lower() for word in ['vulnerable', 'vulnerability', 'vuln', 'exploit']):
                        predicted_category = "Other"
                    else:
                        predicted_category = "Benign"
                
                correct = (predicted_category == ground_truth_category)
                
                # è½¬æ¢Sampleå¯¹è±¡ä¸ºå­—å…¸æ ¼å¼
                sample_data = {
                    'func': sample.input_text,
                    'target': int(sample.target),
                    'project': sample.metadata.get('project', ''),
                    'cwe': sample.metadata.get('cwe', []),
                    'cve': sample.metadata.get('cve', 'None'),
                    'cve_desc': sample.metadata.get('cve_desc', 'None'),
                    'func_hash': sample.metadata.get('func_hash', ''),
                    'file_name': sample.metadata.get('file_name', ''),
                    'ground_truth_category': ground_truth_category,
                    'predicted_category': predicted_category
                }
                
                # è®°å½•æ ·æœ¬ç»“æœ
                sample_tracker.log_sample_result(
                    prompt_id=f"{prompt_id}_feedback_{batch_idx}",
                    sample_idx=sample_idx,
                    sample_data=sample_data,
                    prediction=prediction_text,
                    ground_truth=ground_truth_binary,
                    correct=correct,
                    generation=generation,
                    feedback_applied=True
                )
                
                # å¦‚æœé¢„æµ‹é”™è¯¯ï¼Œå°è¯•æ”¹è¿›prompt
                if not correct:
                    # æ„å»ºCWEç›¸å…³çš„åé¦ˆä¿¡æ¯
                    cwe_info = ""
                    if sample_data.get('cwe') and sample_data['cwe']:
                        cwe_list = ", ".join(sample_data['cwe'])
                        cwe_info = f"\nCWE Categories: {cwe_list}"
                    
                    cve_info = ""
                    if sample_data.get('cve') and sample_data['cve'] != 'None':
                        cve_info = f"\nCVE ID: {sample_data['cve']}"
                    
                    project_info = ""
                    if sample_data.get('project'):
                        project_info = f"\nProject: {sample_data['project']}"
                    
                    feedback_instruction = f"""
The current prompt made an incorrect CWE major category classification. Please improve it based on the specific vulnerability information.

Current Prompt:
{current_prompt}

Code Sample:
{code[:500]}...

Ground Truth Category: {ground_truth_category}
Predicted Category: {predicted_category}{project_info}{cwe_info}{cve_info}

Create an improved prompt that would correctly classify this sample into the correct CWE major category. Focus on:
1. The specific CWE categories: {ground_truth_category} characteristics
2. The vulnerability patterns that distinguish {ground_truth_category} from other categories
3. Common {ground_truth_category} issues in {sample_data.get('project', 'this type of')} code
4. Ensure the prompt can distinguish between all CWE major categories: {", ".join(CWE_MAJOR_CATEGORIES)}

Improved prompt:
"""
                    
                    try:
                        improved_prompt = llm_client.generate(feedback_instruction, temperature=0.7)
                        if '{input}' in improved_prompt and len(improved_prompt.strip()) > 50:
                            current_prompt = improved_prompt.strip()
                            improvements_count += 1
                            print(f"       âš¡ æ ·æœ¬ {sample_idx+1}: promptå·²æ”¹è¿›")
                    except Exception as e:
                        print(f"       âš ï¸ æ ·æœ¬ {sample_idx+1}: æ”¹è¿›å¤±è´¥ - {e}")
                else:
                    print(f"       âœ… æ ·æœ¬ {sample_idx+1}: é¢„æµ‹æ­£ç¡®")
                    
            except Exception as e:
                print(f"       âŒ æ ·æœ¬ {sample_idx+1}: å¤„ç†å¤±è´¥ - {e}")
    
    print(f"     ğŸ“ˆ åé¦ˆè®­ç»ƒå®Œæˆ: {improvements_count}/{len(selected_samples)} ä¸ªæ ·æœ¬è§¦å‘æ”¹è¿›")
    return current_prompt


def evaluate_on_dataset(prompt: str, dataset, llm_client, prompt_id: str,
                       sample_tracker: SampleWiseTracker, generation: int, 
                       config: dict = None) -> float:
    """åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°promptæ€§èƒ½ï¼ˆæ”¯æŒCWEå¤§ç±»å¤šåˆ†ç±»å’Œæ‰¹å¤„ç†ï¼‰"""
    correct = 0
    samples = dataset.get_samples()
    total = len(samples)
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ‰¹å¤„ç†
    enable_batch = config.get('enable_batch_processing', False) if config else False
    batch_size = config.get('llm_batch_size', 8) if config else 8
    enable_concurrent = config.get('enable_concurrent', True) if config else True
    
    if enable_batch and total > 1:
        # æ‰¹å¤„ç†æ¨¡å¼
        print(f"     ğŸš€ å¯ç”¨æ‰¹å¤„ç†è¯„ä¼°: {total} ä¸ªæ ·æœ¬, batch_size={batch_size}")
        
        # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
        batch_queries = []
        batch_samples = []
        batch_metadata = []
        
        for idx, sample in enumerate(samples):
            code = sample.input_text
            ground_truth_binary = int(sample.target)
            
            # ä»æ ·æœ¬çš„CWEä»£ç è·å–çœŸå®çš„CWEå¤§ç±»
            cwe_codes = sample.metadata.get('cwe', [])
            if ground_truth_binary == 1 and cwe_codes:
                ground_truth_category = map_cwe_to_major(cwe_codes)
            else:
                ground_truth_category = "Benign"
            
            query = prompt.format(input=code)
            batch_queries.append(query)
            batch_samples.append(sample)
            batch_metadata.append({
                'idx': idx,
                'ground_truth_binary': ground_truth_binary,
                'ground_truth_category': ground_truth_category
            })
        
        # æ‰¹é‡è°ƒç”¨LLM
        try:
            prediction_texts = llm_client.batch_generate(
                batch_queries, 
                temperature=0.1, 
                max_tokens=20,
                batch_size=batch_size,
                concurrent=enable_concurrent
            )
            
            # å¤„ç†æ‰¹å¤„ç†ç»“æœ
            for idx, (sample, metadata, prediction_text) in enumerate(zip(batch_samples, batch_metadata, prediction_texts)):
                if prediction_text == "error":
                    print(f"     âš ï¸ æ ·æœ¬ {metadata['idx']} é¢„æµ‹å¤±è´¥")
                    continue
                    
                # è§„èŒƒåŒ–æ¨¡å‹è¾“å‡ºåˆ°CWEå¤§ç±»
                predicted_category = canonicalize_category(prediction_text)
                if predicted_category is None:
                    if any(word in prediction_text.lower() for word in ['vulnerable', 'vulnerability', 'vuln', 'exploit']):
                        predicted_category = "Other"
                    else:
                        predicted_category = "Benign"
                
                is_correct = (predicted_category == metadata['ground_truth_category'])
                
                if is_correct:
                    correct += 1
                
                # è®°å½•è¯„ä¼°ç»“æœ
                sample_data = {
                    'func': sample.input_text,
                    'target': metadata['ground_truth_binary'],
                    'project': sample.metadata.get('project', ''),
                    'cwe': sample.metadata.get('cwe', []),
                    'cve': sample.metadata.get('cve', 'None'),
                    'cve_desc': sample.metadata.get('cve_desc', 'None'),
                    'func_hash': sample.metadata.get('func_hash', ''),
                    'file_name': sample.metadata.get('file_name', ''),
                    'ground_truth_category': metadata['ground_truth_category'],
                    'predicted_category': predicted_category
                }
                
                sample_tracker.log_sample_result(
                    prompt_id=prompt_id,
                    sample_idx=metadata['idx'],
                    sample_data=sample_data,
                    prediction=prediction_text,
                    ground_truth=metadata['ground_truth_binary'],
                    correct=is_correct,
                    generation=generation,
                    feedback_applied=False
                )
                
        except Exception as e:
            print(f"     âŒ æ‰¹å¤„ç†è¯„ä¼°å¤±è´¥: {e}")
            # å›é€€åˆ°å•ä¸ªå¤„ç†æ¨¡å¼
            enable_batch = False
    
    if not enable_batch:
        # å•ä¸ªå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        print(f"     ğŸ”„ ä½¿ç”¨å•ä¸ªå¤„ç†è¯„ä¼°: {total} ä¸ªæ ·æœ¬")
        
        for idx, sample in enumerate(samples):
            try:
                code = sample.input_text
                ground_truth_binary = int(sample.target)
                
                # ä»æ ·æœ¬çš„CWEä»£ç è·å–çœŸå®çš„CWEå¤§ç±»
                cwe_codes = sample.metadata.get('cwe', [])
                if ground_truth_binary == 1 and cwe_codes:
                    ground_truth_category = map_cwe_to_major(cwe_codes)
                else:
                    ground_truth_category = "Benign"
                
                query = prompt.format(input=code)
                prediction_text = llm_client.generate(query, temperature=0.1, max_tokens=20)
                
                # è§„èŒƒåŒ–æ¨¡å‹è¾“å‡ºåˆ°CWEå¤§ç±»
                predicted_category = canonicalize_category(prediction_text)
                if predicted_category is None:
                    if any(word in prediction_text.lower() for word in ['vulnerable', 'vulnerability', 'vuln', 'exploit']):
                        predicted_category = "Other"
                    else:
                        predicted_category = "Benign"
                
                is_correct = (predicted_category == ground_truth_category)
                
                if is_correct:
                    correct += 1
                
                # è®°å½•è¯„ä¼°ç»“æœ
                sample_data = {
                    'func': sample.input_text,
                    'target': int(sample.target),
                    'project': sample.metadata.get('project', ''),
                    'cwe': sample.metadata.get('cwe', []),
                    'cve': sample.metadata.get('cve', 'None'),
                    'cve_desc': sample.metadata.get('cve_desc', 'None'),
                    'func_hash': sample.metadata.get('func_hash', ''),
                    'file_name': sample.metadata.get('file_name', ''),
                    'ground_truth_category': ground_truth_category,
                    'predicted_category': predicted_category
                }
                
                sample_tracker.log_sample_result(
                    prompt_id=prompt_id,
                    sample_idx=idx,
                    sample_data=sample_data,
                    prediction=prediction_text,
                    ground_truth=ground_truth_binary,
                    correct=is_correct,
                    generation=generation,
                    feedback_applied=False
                )
                
            except Exception as e:
                print(f"     âš ï¸ æ ·æœ¬ {idx} è¯„ä¼°å¤±è´¥: {e}")
    
    accuracy = correct / total if total > 0 else 0
    print(f"     ğŸ“Š CWEå¤§ç±»è¯„ä¼°å®Œæˆ: {correct}/{total} = {accuracy:.4f}")
    return accuracy


def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ Primevul 1% æ ·æœ¬çº§åé¦ˆé«˜å¹¶å‘è¿›åŒ–å®éªŒ")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    
    # æ£€æŸ¥APIé…ç½®
    api_key = check_api_configuration()
    if not api_key:
        return 1
    
    # è·¯å¾„é…ç½®
    primevul_dir = "./data/primevul/primevul"
    sample_output_dir = "./data/primevul_1percent_sample"
    
    try:
        # 1. å‡†å¤‡æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not os.path.exists(sample_output_dir):
            if not os.path.exists(primevul_dir):
                print(f"âŒ Primevulæ•°æ®ç›®å½•ä¸å­˜åœ¨: {primevul_dir}")
                print("è¯·ç¡®ä¿Primevulæ•°æ®å·²ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®")
                return 1
            
            print("ğŸ“Š å‡†å¤‡1%é‡‡æ ·æ•°æ®...")
            sample_result = sample_primevul_1percent(primevul_dir, sample_output_dir, seed=42)
            print(f"âœ… é‡‡æ ·å®Œæˆ: {sample_result['total_samples']} æ ·æœ¬")
        else:
            print(f"âœ… ä½¿ç”¨å·²å­˜åœ¨çš„é‡‡æ ·æ•°æ®: {sample_output_dir}")
        
        # 2. åˆ›å»ºæ ·æœ¬çº§åé¦ˆé…ç½®
        config = create_optimized_config()
        config["api_key"] = api_key
        
        print(f"\nâš™ï¸ æ ·æœ¬çº§åé¦ˆå®éªŒé…ç½®:")
        print(f"   å®éªŒID: {config['experiment_id']}")
        print(f"   ç®—æ³•: {config['algorithm'].upper()}")
        print(f"   ç§ç¾¤å¤§å°: {config['population_size']}")
        print(f"   è¿›åŒ–ä»£æ•°: {config['max_generations']}")
        print(f"   æœ€å¤§å¹¶å‘: {config['max_concurrency']}")
        print(f"   è®­ç»ƒé›†æ‰“ä¹±: {config['shuffle_training_data']}")
        print(f"   æ ·æœ¬çº§åé¦ˆ: {config['sample_wise_feedback']}")
        print(f"   åé¦ˆæ‰¹å¤§å°: {config['feedback_batch_size']}")
        print(f"   è®°å½•æ‰€æœ‰æ ·æœ¬: {config['record_all_samples']}")
        print(f"   LLMæ‰¹å¤„ç†: {config['enable_batch_processing']}")
        print(f"   LLMæ‰¹å¤§å°: {config['llm_batch_size']}")
        print(f"   æ‰¹æ¬¡å†…å¹¶å‘: {config['enable_concurrent']}")
        
        # 3. è¿è¡Œæ ·æœ¬çº§åé¦ˆè¿›åŒ–
        results, exp_dir = run_concurrent_evolution_with_feedback(config, sample_output_dir)
        
        print(f"\nâœ… æ ·æœ¬çº§åé¦ˆå®éªŒå®Œæˆ!")
        print(f"ğŸ“‚ ç»“æœç›®å½•: {exp_dir}")
        print(f"ğŸ¯ æœ€ä½³é€‚åº”åº¦: {results['best_fitness']:.4f}")
        print(f"ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æ€»è€—æ—¶: {results['total_time']:.2f}ç§’")
        print(f"   è®­ç»ƒæ ·æœ¬: {results['training_samples']}")
        print(f"   æ ·æœ¬çº§åé¦ˆ: {results['sample_wise_feedback']}")
        
        if results.get('fitness_history'):
            initial_fitness = results['fitness_history'][0]
            final_fitness = results['best_fitness']
            improvement = final_fitness - initial_fitness
            print(f"   é€‚åº”åº¦æå‡: {initial_fitness:.4f} â†’ {final_fitness:.4f} (+{improvement:.4f})")
        
        # 4. æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
        print(f"\nğŸ“ ç”Ÿæˆçš„åˆ†ææ–‡ä»¶:")
        analysis_files = [
            "sample_feedback.jsonl",      # æ ·æœ¬çº§åé¦ˆè®°å½•
            "sample_statistics.json",     # æ ·æœ¬ç»Ÿè®¡
            "prompt_evolution.jsonl",     # promptè¿›åŒ–è®°å½•
            "experiment_config.json",     # å®éªŒé…ç½®
            "initial_prompts.txt",        # åˆå§‹prompts
            "experiment_summary.json"     # å®éªŒæ€»ç»“
        ]
        
        for filename in analysis_files:
            filepath = exp_dir / filename
            if filepath.exists():
                size = filepath.stat().st_size
                print(f"   âœ… {filename} ({size:,} bytes)")
            else:
                print(f"   âŒ {filename} (missing)")
        
        return 0
        
    except Exception as e:
        logger.error(f"æ ·æœ¬çº§åé¦ˆå®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())