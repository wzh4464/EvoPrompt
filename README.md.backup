# EvoPrompt: Evolutionary Prompt Optimization for Vulnerability Detection

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

EvoPrompt is a modern prompt evolution framework specifically designed for vulnerability detection tasks. It uses evolutionary algorithms to automatically optimize prompts for better performance on security code analysis.

## ğŸ“ƒ Overview

This is the **modernized version** of the original EvoPrompt paper implementation, featuring:

- ğŸ—ï¸ **Modern Architecture**: Clean, modular design with proper abstractions
- ğŸ§ª **Comprehensive Testing**: Full test suite with pytest and coverage reporting
- ğŸ“¦ **Package Management**: Modern Python packaging with `pyproject.toml`
- ğŸ”§ **Development Tools**: Integrated linting, formatting, and type checking
- ğŸ”„ **Legacy Compatibility**: Backward compatibility with original implementation
- ğŸ“š **Better Documentation**: Comprehensive docs and examples

Based on the paper: [*Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers*](https://openreview.net/pdf?id=ZG3RaNIsO8) (ICLR 2024)

## ğŸš€ Quick Start

### Installation

```bash
# Install the package
pip install -e .

# Install with development dependencies  
pip install -e ".[dev]"

# Install with vulnerability detection dependencies
pip install -e ".[vulnerability]"
```

### Basic Usage

```python
from evoprompt import EvolutionEngine, GeneticAlgorithm, LLMClient

# Create components
llm_client = LLMClient("gpt-3.5-turbo")
algorithm = GeneticAlgorithm({
    "population_size": 20,
    "max_generations": 10,
    "mutation_rate": 0.1
})

# Create evolution engine
engine = EvolutionEngine(
    algorithm=algorithm,
    evaluator=evaluator,  # Your custom evaluator
    llm_client=llm_client
)

# Run evolution
results = engine.evolve(initial_prompts=[
    "Solve this problem: {input}",
    "Answer the question: {input}",
])

print(f"Best prompt: {results['best_prompt']}")
```

### Command Line Interface

```bash
# Modern interface
evoprompt --dataset my_dataset --algorithm de --generations 10

# Legacy compatibility mode
evoprompt --legacy --dataset sven --task vul_detection
```

## ğŸ—ï¸ Architecture

### Modern Structure

```
src/evoprompt/
â”œâ”€â”€ __init__.py              # Package exports
â”œâ”€â”€ cli.py                   # Command-line interface
â”œâ”€â”€ core/                    # Core components
â”‚   â”œâ”€â”€ evolution.py         # Evolution engine
â”‚   â”œâ”€â”€ evaluator.py         # Prompt evaluation
â”‚   â””â”€â”€ dataset.py           # Dataset handling
â”œâ”€â”€ algorithms/              # Evolution algorithms
â”‚   â”œâ”€â”€ base.py             # Abstract base classes
â”‚   â”œâ”€â”€ genetic.py          # Genetic Algorithm
â”‚   â””â”€â”€ differential.py     # Differential Evolution
â”œâ”€â”€ llm/                    # LLM clients
â”‚   â””â”€â”€ client.py           # LLM client implementations
â”œâ”€â”€ metrics/                # Evaluation metrics
â”‚   â””â”€â”€ base.py             # Metric implementations
â”œâ”€â”€ data/                   # Data utilities
â”‚   â””â”€â”€ dataset.py          # Dataset classes
â””â”€â”€ legacy/                 # Legacy compatibility
    â””â”€â”€ __init__.py         # Legacy imports
```

### Key Components

- **EvolutionEngine**: Main orchestrator for the evolution process
- **EvolutionAlgorithm**: Abstract base for GA, DE, and other algorithms
- **Evaluator**: Evaluates prompt performance on datasets
- **LLMClient**: Handles communication with various LLM providers
- **Dataset**: Manages training and evaluation data

## ğŸ§ª Testing

The framework includes comprehensive testing:

```bash
# Run all tests
make test

# Run with coverage
make test-cov

# Run only fast tests (exclude slow integration tests)
make test-fast

# Run legacy compatibility tests  
make legacy-test
```

### Test Categories

- **Unit Tests**: Test individual components in isolation
- **Integration Tests**: Test component interactions
- **Legacy Tests**: Ensure backward compatibility

## ğŸ”§ Development

### Setting Up Development Environment

```bash
# Clone repository
git clone https://github.com/evoprompt/evoprompt.git
cd evoprompt

# Set up development environment
make dev-setup

# Run development checks
make check
```

### Code Quality Tools

```bash
# Format code
make format

# Run linting
make lint

# Type checking
make type-check

# Full validation
make validate
```

### Configuration

The framework uses modern Python tooling configured in `pyproject.toml`:

- **Black**: Code formatting
- **isort**: Import sorting  
- **flake8**: Linting
- **mypy**: Type checking
- **pytest**: Testing framework

## ğŸ“Š Algorithms

### Genetic Algorithm (GA)

```python
from evoprompt.algorithms import GeneticAlgorithm

ga = GeneticAlgorithm({
    "population_size": 20,
    "max_generations": 10, 
    "mutation_rate": 0.1,
    "crossover_rate": 0.8,
    "selection_method": "tournament"
})
```

### Differential Evolution (DE)

```python
from evoprompt.algorithms import DifferentialEvolution

de = DifferentialEvolution({
    "population_size": 20,
    "max_generations": 10,
    "mutation_factor": 0.5,
    "crossover_probability": 0.7
})
```

## ğŸ”„ Legacy Compatibility

The modern version maintains full backward compatibility:

```bash
# Use legacy scripts directly
./run_sven.sh
cd BBH && bash scripts/run_de_cot.sh

# Or use CLI legacy mode
evoprompt --legacy --dataset sven --algorithm de
```

Legacy files are preserved in the `legacy/` directory and can be imported through the compatibility layer.

## ğŸ“š Examples

### Custom Evaluator

```python
from evoprompt.core.evaluator import Evaluator
from evoprompt.metrics.base import Metric

class AccuracyMetric(Metric):
    def compute(self, predictions, targets):
        correct = sum(p == t for p, t in zip(predictions, targets))
        return correct / len(targets)

evaluator = Evaluator(
    dataset=my_dataset,
    metric=AccuracyMetric(),
    llm_client=llm_client
)
```

### Custom Algorithm

```python
from evoprompt.algorithms.base import EvolutionAlgorithm

class MyAlgorithm(EvolutionAlgorithm):
    def select_parents(self, population):
        # Custom parent selection
        pass
    
    def crossover(self, parents, llm_client):
        # Custom crossover operation
        pass
    
    def mutate(self, individual, llm_client):
        # Custom mutation operation  
        pass
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Run the test suite and linting
6. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“– Citation

If you use EvoPrompt in your research, please cite:

```bibtex
@inproceedings{guo2024connecting,
    title={Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers},
    author={Qingyan Guo and Rui Wang and Junliang Guo and Bei Li and Kaitao Song and Xu Tan and Guoqing Liu and Jiang Bian and Yujiu Yang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=ZG3RaNIsO8}
}
```

## ğŸ™ Acknowledgments

- Original EvoPrompt implementation and research team
- [CoT-hub](https://github.com/FranxYao/chain-of-thought-hub) for BBH evaluation framework
- [APE](https://github.com/keirp/automatic_prompt_engineer) for automatic prompt engineering techniques
- [LM-BFF](https://github.com/princeton-nlp/LM-BFF) for few-shot learning datasets

---

**Note**: This is a modernized version of the original EvoPrompt implementation. For the original code structure, please refer to the `legacy/` directory or use the `--legacy` flag.