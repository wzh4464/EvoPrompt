#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæ¼æ´æ£€æµ‹è¯„ä¼°å™¨
é›†æˆè¿›åŒ–è·Ÿè¸ªã€ç»Ÿè®¡åˆ†æå’Œæ™ºèƒ½promptä¼˜åŒ–åŠŸèƒ½
"""

import os
import time
from typing import List, Dict, Any, Optional
from tqdm import tqdm

from run_vulnerability_detection import VulnerabilityDetectionEvaluator
from evolution_tracker import EvolutionTracker, StatisticalAnalyzer, PromptOptimizer
from sven_llm_client import sven_llm_query


class EnhancedVulnerabilityEvaluator(VulnerabilityDetectionEvaluator):
    """å¢å¼ºç‰ˆæ¼æ´æ£€æµ‹è¯„ä¼°å™¨"""
    
    def __init__(self, args):
        super().__init__(args)
        
        # åˆå§‹åŒ–è¿›åŒ–è·Ÿè¸ªç³»ç»Ÿ
        experiment_name = f"vuln_detect_{args.dataset}_{args.evo_mode}_{int(time.time())}"
        self.tracker = EvolutionTracker(args.output, experiment_name)
        self.analyzer = StatisticalAnalyzer(self.tracker)
        self.optimizer = PromptOptimizer(self.tracker, self.analyzer)
        
        # å¯åŠ¨å®éªŒè·Ÿè¸ª
        self.tracker.start_experiment({
            'dataset': args.dataset,
            'evo_mode': args.evo_mode,
            'popsize': args.popsize,
            'budget': args.budget,
            'sample_num': args.sample_num,
            'seed': args.seed
        })
        
        # å½“å‰ä»£æ•°
        self.current_generation = 0
        
        # å­˜å‚¨è¯¦ç»†ç»“æœ
        self.detailed_results_cache = {}
        
        self.logger.info(f"ğŸ”¬ Enhanced evaluator initialized with experiment: {experiment_name}")
    
    def forward(self, prompt_pre="", eval_src=None, ref_texts=None, output=None):
        """
        å¢å¼ºç‰ˆå‰å‘ä¼ æ’­ï¼ŒåŒ…å«è¯¦ç»†ç»“æœè®°å½•
        """
        start_time = time.time()
        
        if eval_src is None:
            eval_src = self.dev_src
        if ref_texts is None:
            ref_texts = self.dev_tgt
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompts = []
        for code_snippet in eval_src:
            full_prompt = self.create_vulnerability_prompt(code_snippet, prompt_pre)
            full_prompts.append(full_prompt)
        
        # ä½¿ç”¨LLMè¿›è¡Œæ¨ç†ï¼Œå¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
        predictions, detailed_results = self.get_llm_predictions_with_details(full_prompts, ref_texts)
        
        # æ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
        normalized_preds = self.normalize_predictions(predictions)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        scores = self.calculate_vulnerability_metrics(normalized_preds, ref_texts)
        
        # è®°å½•åˆ°è·Ÿè¸ªç³»ç»Ÿ
        prompt_id = self.tracker.log_prompt(
            generation=self.current_generation,
            prompt_text=prompt_pre,
            scores=scores,
            detailed_results=detailed_results,
            parent_id=None,  # TODO: åœ¨è¿›åŒ–ç®—æ³•ä¸­è®¾ç½®çˆ¶ID
            mutation_type=None  # TODO: åœ¨è¿›åŒ–ç®—æ³•ä¸­è®¾ç½®å˜å¼‚ç±»å‹
        )
        
        # ç¼“å­˜è¯¦ç»†ç»“æœ
        self.detailed_results_cache[prompt_pre] = {
            'prompt_id': prompt_id,
            'predictions': normalized_preds,
            'detailed_results': detailed_results,
            'scores': scores,
            'processing_time': time.time() - start_time
        }
        
        self.logger.info(f"ğŸ“Š Prompt evaluated - F1: {scores[3]:.4f}, Acc: {scores[0]:.4f}, Time: {time.time() - start_time:.2f}s")
        
        return {"hypos": normalized_preds, "scores": scores}
    
    def get_llm_predictions_with_details(self, prompts: List[str], true_labels: List[str]) -> tuple:
        """è·å–LLMé¢„æµ‹ç»“æœçš„åŒæ—¶è®°å½•è¯¦ç»†ä¿¡æ¯"""
        predictions = []
        detailed_results = []
        
        try:
            self.logger.info(f"ğŸ”® Starting enhanced batch prediction for {len(prompts)} prompts...")
            
            # ä½¿ç”¨SVENå®¢æˆ·ç«¯çš„æ‰¹é‡æŸ¥è¯¢åŠŸèƒ½
            responses = sven_llm_query(
                data=prompts,
                client=self.client,
                task=False,
                temperature=0.0,
                delay=0.1
            )
            
            for i, (response, true_label) in enumerate(zip(responses, true_labels)):
                start_pred_time = time.time()
                
                if response == "error":
                    self.logger.warning(f"âš ï¸ Error in query {i+1}, using default prediction")
                    prediction = "benign"
                    confidence = 0.0
                else:
                    prediction = response
                    # ç®€å•çš„ç½®ä¿¡åº¦ä¼°ç®—ï¼ˆåŸºäºå“åº”å†…å®¹ï¼‰
                    confidence = self._estimate_confidence(response)
                
                predictions.append(prediction)
                
                # è®°å½•è¯¦ç»†ç»“æœ
                detailed_results.append({
                    'input': prompts[i][:200] + '...' if len(prompts[i]) > 200 else prompts[i],
                    'predicted': prediction,
                    'true_label': true_label,
                    'correct': prediction.lower() in true_label.lower() or true_label.lower() in prediction.lower(),
                    'confidence': confidence,
                    'response_time': time.time() - start_pred_time,
                    'raw_response': response[:100] if isinstance(response, str) else str(response)[:100]
                })
                
        except Exception as e:
            self.logger.error(f"âŒ Enhanced batch prediction failed: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            predictions = self.get_llm_predictions(prompts)
            detailed_results = [
                {
                    'input': prompt[:100] + '...',
                    'predicted': pred,
                    'true_label': true_labels[i] if i < len(true_labels) else 'unknown',
                    'correct': False,
                    'confidence': 0.0,
                    'response_time': 0.0,
                    'raw_response': pred
                }
                for i, (prompt, pred) in enumerate(zip(prompts, predictions))
            ]
        
        return predictions, detailed_results
    
    def _estimate_confidence(self, response: str) -> float:
        """ç®€å•çš„ç½®ä¿¡åº¦ä¼°ç®—"""
        if not isinstance(response, str):
            return 0.0
        
        response_lower = response.lower()
        
        # åŸºäºå…³é”®è¯çš„ç½®ä¿¡åº¦
        high_confidence_words = ['definitely', 'clearly', 'obviously', 'certainly', 'vulnerable', 'safe']
        medium_confidence_words = ['likely', 'probably', 'seems', 'appears', 'might']
        low_confidence_words = ['maybe', 'possibly', 'unsure', 'unclear']
        
        if any(word in response_lower for word in high_confidence_words):
            return 0.9
        elif any(word in response_lower for word in medium_confidence_words):
            return 0.6
        elif any(word in response_lower for word in low_confidence_words):
            return 0.3
        else:
            return 0.5  # é»˜è®¤ç½®ä¿¡åº¦
    
    def start_generation(self, generation: int, population_prompts: List[str] = None):
        """å¼€å§‹æ–°çš„ä¸€ä»£"""
        self.current_generation = generation
        self.logger.info(f"ğŸ§¬ Starting generation {generation}")
        
        if population_prompts:
            self.logger.info(f"ğŸ“ Population size: {len(population_prompts)}")
    
    def end_generation(self, generation: int, population_scores: List[float]):
        """ç»“æŸå½“å‰ä»£ï¼Œè¿›è¡Œåˆ†æå’Œä¼˜åŒ–"""
        self.logger.info(f"ğŸ Ending generation {generation}")
        
        # è®°å½•ä»£æ•°ç»Ÿè®¡
        self.tracker.log_generation(generation, population_scores)
        
        # æ¯2ä»£è¿›è¡Œä¸€æ¬¡æ·±åº¦åˆ†æ
        if generation > 0 and generation % 2 == 0:
            self.perform_generation_analysis(generation)
        
        # æ¯3ä»£è¿›è¡Œä¸€æ¬¡ä¼˜åŒ–å»ºè®®
        if generation > 0 and generation % 3 == 0:
            self.generate_optimization_suggestions(generation)
    
    def perform_generation_analysis(self, generation: int):
        """æ‰§è¡Œä»£æ•°åˆ†æ"""
        self.logger.info(f"ğŸ” Performing analysis for generation {generation}")
        
        try:
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = self.analyzer.generate_comprehensive_report(generation)
            
            # ä¿å­˜åˆ†ææŠ¥å‘Š
            report_file = os.path.join(self.public_out_path, f"analysis_gen_{generation}.json")
            import json
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ“„ Analysis report saved: {report_file}")
            
            # è¾“å‡ºå…³é”®æ´å¯Ÿ
            if 'llm_insights' in report and report['llm_insights']:
                self.logger.info("ğŸ§  Key insights from LLM analysis:")
                insights_lines = report['llm_insights'].split('\n')[:5]  # å‰5è¡Œ
                for line in insights_lines:
                    if line.strip():
                        self.logger.info(f"   {line.strip()}")
                        
        except Exception as e:
            self.logger.error(f"âŒ Analysis failed for generation {generation}: {e}")
    
    def generate_optimization_suggestions(self, generation: int):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        self.logger.info(f"ğŸ’¡ Generating optimization suggestions for generation {generation}")
        
        try:
            # è·å–æœ€æ–°çš„åˆ†ææŠ¥å‘Š
            report = self.analyzer.generate_comprehensive_report(generation)
            
            # ç”Ÿæˆä¼˜åŒ–ç­–ç•¥
            strategies = self.optimizer.generate_optimization_strategies(report)
            
            # ä¿å­˜ç­–ç•¥
            strategies_file = os.path.join(self.public_out_path, f"strategies_gen_{generation}.txt")
            with open(strategies_file, 'w', encoding='utf-8') as f:
                f.write(f"Generation {generation} Optimization Strategies\n")
                f.write("=" * 50 + "\n\n")
                for i, strategy in enumerate(strategies, 1):
                    f.write(f"{i}. {strategy}\n\n")
            
            self.logger.info(f"ğŸ’¡ {len(strategies)} optimization strategies generated")
            self.logger.info(f"ğŸ“„ Strategies saved: {strategies_file}")
            
            # è¾“å‡ºå‰3ä¸ªç­–ç•¥
            for i, strategy in enumerate(strategies[:3], 1):
                self.logger.info(f"   Strategy {i}: {strategy[:100]}...")
                
        except Exception as e:
            self.logger.error(f"âŒ Strategy generation failed for generation {generation}: {e}")
    
    def optimize_population(self, prompts: List[str], generation: int) -> List[str]:
        """ä¼˜åŒ–ç§ç¾¤ï¼ˆå¯é€‰åŠŸèƒ½ï¼Œç”±è¿›åŒ–ç®—æ³•è°ƒç”¨ï¼‰"""
        self.logger.info(f"ğŸ”§ Optimizing population for generation {generation}")
        
        try:
            # è·å–æœ€æ–°çš„ä¼˜åŒ–ç­–ç•¥
            report = self.analyzer.generate_comprehensive_report(generation-1)
            strategies = self.optimizer.generate_optimization_strategies(report)
            
            if not strategies:
                self.logger.warning("âš ï¸ No optimization strategies available, returning original prompts")
                return prompts
            
            # æ‰¹é‡ä¼˜åŒ–
            optimized_prompts = self.optimizer.batch_optimize_population(prompts, strategies, generation)
            
            self.logger.info(f"âœ… Population optimization completed")
            return optimized_prompts
            
        except Exception as e:
            self.logger.error(f"âŒ Population optimization failed: {e}")
            return prompts
    
    def get_experiment_summary(self) -> Dict[str, Any]:
        """è·å–å®éªŒæ‘˜è¦"""
        summary = self.tracker.get_evolution_summary()
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        summary['experiment_name'] = self.tracker.experiment_name
        summary['current_generation'] = self.current_generation
        summary['cache_entries'] = len(self.detailed_results_cache)
        
        return summary
    
    def export_results(self):
        """å¯¼å‡ºå®Œæ•´ç»“æœ"""
        self.logger.info("ğŸ“¤ Exporting complete results...")
        
        try:
            # å¯¼å‡ºå®éªŒæ‘˜è¦
            summary = self.get_experiment_summary()
            summary_file = os.path.join(self.public_out_path, "experiment_summary.json")
            
            import json
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            # å¯¼å‡ºè¯¦ç»†ç¼“å­˜
            cache_file = os.path.join(self.public_out_path, "detailed_cache.json")
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.detailed_results_cache, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… Results exported to {self.public_out_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ Export failed: {e}")


# å·¥å‚å‡½æ•°
def create_enhanced_evaluator(args) -> EnhancedVulnerabilityEvaluator:
    """åˆ›å»ºå¢å¼ºç‰ˆè¯„ä¼°å™¨"""
    return EnhancedVulnerabilityEvaluator(args)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª Testing Enhanced Vulnerability Evaluator...")
    
    class MockArgs:
        def __init__(self):
            self.dataset = "sven"
            self.task = "vul_detection"
            self.output = "./test_enhanced_outputs/"
            self.seed = 42
            self.evo_mode = "de"
            self.popsize = 5
            self.budget = 3
            self.sample_num = 20
    
    args = MockArgs()
    
    try:
        evaluator = create_enhanced_evaluator(args)
        print("âœ… Enhanced evaluator created successfully!")
        
        # æ¨¡æ‹Ÿä¸€äº›è¯„ä¼°
        test_prompts = [
            "Analyze this code for vulnerabilities",
            "Check this code for security issues",
            "Review this code for potential threats"
        ]
        
        evaluator.start_generation(0, test_prompts)
        
        # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½å®é™…è¿è¡Œforwardï¼Œå› ä¸ºéœ€è¦çœŸå®çš„API
        print("ğŸ‰ Enhanced evaluator test completed!")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()