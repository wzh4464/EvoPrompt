#!/usr/bin/env python3
"""
EvoPrompt Main Entry - PrimeVul Layer-1 å¹¶å‘æ¼æ´åˆ†ç±»

åŠŸèƒ½:
1. ä» init/ æ–‡ä»¶å¤¹è¯»å–åˆå§‹åŒ– prompts
2. å¯¹ PrimeVul æ•°æ®é›†è¿›è¡Œ CWE å¤§ç±»åˆ†å±‚æ¼æ´æ£€æµ‹
3. æ¯ 16 æ¡ code ä¸ºä¸€ä¸ª batch è¿›è¡Œæ‰¹é‡å¤„ç†
4. Batch çº§åˆ«çš„åˆ†æå’Œåé¦ˆæœºåˆ¶æŒ‡å¯¼ prompt è¿›åŒ–
5. è¾“å‡ºæœ€ç»ˆ prompt å’Œå„ç±»åˆ«çš„ precision/recall/f1-score åˆ° result/ æ–‡ä»¶å¤¹
"""

import sys
import json
import time
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple

# æ·»åŠ  src è·¯å¾„
sys.path.insert(0, "src")

from evoprompt.data.sampler import sample_primevul_1percent
from evoprompt.data.dataset import PrimevulDataset
from evoprompt.data.cwe_categories import (
    CWE_MAJOR_CATEGORIES,
    map_cwe_to_major,
    canonicalize_category,
)
from evoprompt.llm.client import create_default_client
from evoprompt.algorithms.base import Individual, Population
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np


class BatchAnalyzer:
    """Batch çº§åˆ«çš„åˆ†æå™¨ï¼Œå¯¹æ¯”é¢„æµ‹ç»“æœå’Œ ground truth å¹¶ç”Ÿæˆåé¦ˆ"""

    def __init__(self, batch_size: int = 16):
        self.batch_size = batch_size
        self.analysis_history = []

    def analyze_batch(
        self,
        predictions: List[str],
        ground_truths: List[str],
        batch_idx: int
    ) -> Dict[str, Any]:
        """
        åˆ†æä¸€ä¸ª batch çš„é¢„æµ‹ç»“æœ

        Returns:
            analysis: åŒ…å«å‡†ç¡®ç‡ã€é”™è¯¯æ¨¡å¼ã€æ”¹è¿›å»ºè®®çš„å­—å…¸
        """
        correct = sum(p == g for p, g in zip(predictions, ground_truths))
        accuracy = correct / len(predictions) if predictions else 0.0

        # ç»Ÿè®¡é”™è¯¯ç±»å‹
        error_patterns = {}
        for pred, truth in zip(predictions, ground_truths):
            if pred != truth:
                error_key = f"{truth} -> {pred}"
                error_patterns[error_key] = error_patterns.get(error_key, 0) + 1

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = self._generate_improvement_suggestions(
            error_patterns, ground_truths, predictions
        )

        analysis = {
            "batch_idx": batch_idx,
            "batch_size": len(predictions),
            "correct": correct,
            "accuracy": accuracy,
            "error_patterns": error_patterns,
            "improvement_suggestions": improvement_suggestions,
            "timestamp": datetime.now().isoformat(),
        }

        self.analysis_history.append(analysis)
        return analysis

    def _generate_improvement_suggestions(
        self,
        error_patterns: Dict[str, int],
        ground_truths: List[str],
        predictions: List[str]
    ) -> List[str]:
        """æ ¹æ®é”™è¯¯æ¨¡å¼ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        if not error_patterns:
            suggestions.append("This batch achieved perfect accuracy. Maintain current approach.")
            return suggestions

        # åˆ†ææœ€å¸¸è§çš„é”™è¯¯
        sorted_errors = sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)

        for error_pattern, count in sorted_errors[:3]:  # åªå–å‰3ä¸ªæœ€å¸¸è§é”™è¯¯
            true_cat, pred_cat = error_pattern.split(" -> ")

            suggestion = (
                f"Improve detection of '{true_cat}' (misclassified as '{pred_cat}' {count} times). "
                f"Focus on distinguishing {true_cat} characteristics from {pred_cat}."
            )
            suggestions.append(suggestion)

        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„è¡¨ç°
        category_stats = {}
        for truth in set(ground_truths):
            category_stats[truth] = {
                "total": ground_truths.count(truth),
                "correct": sum(1 for p, g in zip(predictions, ground_truths) if g == truth and p == g)
            }

        # æ‰¾å‡ºè¡¨ç°æœ€å·®çš„ç±»åˆ«
        worst_category = None
        worst_accuracy = 1.0
        for cat, stats in category_stats.items():
            if stats["total"] > 0:
                acc = stats["correct"] / stats["total"]
                if acc < worst_accuracy:
                    worst_accuracy = acc
                    worst_category = cat

        if worst_category and worst_accuracy < 0.5:
            suggestions.append(
                f"Category '{worst_category}' has low accuracy ({worst_accuracy:.2%}). "
                f"Emphasize patterns specific to this vulnerability type."
            )

        return suggestions


class PromptEvolver:
    """åŸºäº Batch åˆ†æåé¦ˆçš„ Prompt è¿›åŒ–å™¨"""

    def __init__(self, llm_client, config: Dict[str, Any]):
        self.llm_client = llm_client
        self.config = config
        self.evolution_history = []

    def evolve_with_feedback(
        self,
        current_prompt: str,
        batch_analysis: Dict[str, Any],
        generation: int
    ) -> str:
        """æ ¹æ® batch åˆ†æåé¦ˆè¿›åŒ– prompt"""

        # å¦‚æœå‡†ç¡®ç‡å·²ç»å¾ˆé«˜ï¼Œä¸éœ€è¦æ”¹è¿›
        if batch_analysis["accuracy"] >= 0.95:
            return current_prompt

        # æ„å»ºè¿›åŒ–æŒ‡ä»¤
        improvement_text = "\n".join(
            f"- {sug}" for sug in batch_analysis["improvement_suggestions"]
        )

        error_text = "\n".join(
            f"- {pattern}: {count} occurrences"
            for pattern, count in batch_analysis["error_patterns"].items()
        )

        evolution_instruction = f"""
You are improving a vulnerability detection prompt based on batch analysis feedback.

Current Prompt:
{current_prompt}

Batch Analysis Results:
- Accuracy: {batch_analysis['accuracy']:.2%}
- Batch size: {batch_analysis['batch_size']}
- Correct predictions: {batch_analysis['correct']}

Common Error Patterns:
{error_text if error_text else "None - all predictions were correct"}

Improvement Suggestions:
{improvement_text}

Task: Create an improved prompt that:
1. Addresses the identified error patterns
2. Better distinguishes between the confused categories
3. Maintains the same output format (CWE major category or 'Benign')
4. Keeps the {{{{input}}}} placeholder for code insertion
5. Uses the following valid categories: {", ".join(CWE_MAJOR_CATEGORIES)}

Return ONLY the improved prompt text, nothing else:
"""

        try:
            improved_prompt = self.llm_client.generate(
                evolution_instruction,
                temperature=0.7,
                max_tokens=500
            )

            # éªŒè¯æ”¹è¿›åçš„ prompt
            if "{input}" in improved_prompt and len(improved_prompt.strip()) > 50:
                self.evolution_history.append({
                    "generation": generation,
                    "batch_idx": batch_analysis["batch_idx"],
                    "old_accuracy": batch_analysis["accuracy"],
                    "prompt": improved_prompt,
                    "timestamp": datetime.now().isoformat(),
                })
                return improved_prompt.strip()
            else:
                print(f"    âš ï¸ è¿›åŒ–åçš„ prompt æ— æ•ˆï¼Œä¿æŒåŸ prompt")
                return current_prompt

        except Exception as e:
            print(f"    âŒ Prompt è¿›åŒ–å¤±è´¥: {e}")
            return current_prompt


class PrimeVulLayer1Pipeline:
    """PrimeVul Layer-1 å¹¶å‘æ¼æ´åˆ†ç±»æµæ°´çº¿"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.batch_size = config.get("batch_size", 16)
        self.init_dir = Path("init")
        self.result_dir = Path("result")

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.init_dir.mkdir(exist_ok=True)
        self.result_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå®éªŒå­ç›®å½•
        self.exp_id = config.get("experiment_id", f"layer1_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.exp_dir = self.result_dir / self.exp_id
        self.exp_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–ç»„ä»¶
        self.llm_client = create_default_client()
        self.batch_analyzer = BatchAnalyzer(batch_size=self.batch_size)
        self.prompt_evolver = PromptEvolver(self.llm_client, config)

        print(f"âœ… åˆå§‹åŒ– PrimeVul Layer-1 Pipeline")
        print(f"   å®éªŒ ID: {self.exp_id}")
        print(f"   Batch å¤§å°: {self.batch_size}")
        print(f"   ç»“æœç›®å½•: {self.exp_dir}")

    def load_initial_prompts(self) -> List[str]:
        """ä» init/ æ–‡ä»¶å¤¹åŠ è½½åˆå§‹ prompts"""
        prompts_file = self.init_dir / "layer1_prompts.txt"

        if not prompts_file.exists():
            print(f"âš ï¸ æœªæ‰¾åˆ°åˆå§‹ prompts æ–‡ä»¶: {prompts_file}")
            print(f"   ä½¿ç”¨é»˜è®¤ prompts å¹¶ä¿å­˜åˆ° {prompts_file}")
            default_prompts = self._create_default_prompts()

            # ä¿å­˜é»˜è®¤ prompts
            with open(prompts_file, "w", encoding="utf-8") as f:
                f.write("# PrimeVul Layer-1 åˆå§‹åŒ– Prompts\n")
                f.write("# æ¯ä¸ª prompt ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”\n")
                f.write("# Prompt ä¸­å¿…é¡»åŒ…å« {input} å ä½ç¬¦\n\n")
                for i, prompt in enumerate(default_prompts, 1):
                    f.write(f"# Prompt {i}\n")
                    f.write(prompt)
                    f.write("\n\n" + "="*80 + "\n\n")

            return default_prompts

        # è¯»å– prompts
        with open(prompts_file, "r", encoding="utf-8") as f:
            content = f.read()

        # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²
        prompts = []
        for section in content.split("=" * 80):
            section = section.strip()
            if not section or section.startswith("#"):
                continue
            # ç§»é™¤æ³¨é‡Šè¡Œ
            lines = [line for line in section.split("\n") if not line.strip().startswith("#")]
            prompt = "\n".join(lines).strip()
            if prompt and "{input}" in prompt:
                prompts.append(prompt)

        print(f"âœ… ä» {prompts_file} åŠ è½½äº† {len(prompts)} ä¸ªåˆå§‹ prompts")
        return prompts if prompts else self._create_default_prompts()

    def _create_default_prompts(self) -> List[str]:
        """åˆ›å»ºé»˜è®¤çš„åˆå§‹ prompts"""
        categories_text = ", ".join(f"'{cat}'" for cat in CWE_MAJOR_CATEGORIES)

        return [
            f"""Analyze this code for security vulnerabilities and classify it into one of these CWE major categories: {categories_text}.
If no vulnerability is found, respond with 'Benign'.
Respond ONLY with the category name.

Code to analyze:
{{input}}

CWE Major Category:""",

            f"""You are a security expert analyzing code for vulnerabilities.
Classify the code into ONE of these categories: {categories_text}.
For secure code, respond with 'Benign'.
Output ONLY the category name, nothing else.

Code:
{{input}}

Category:""",

            f"""Security vulnerability classification task.
Categories: {categories_text}

Examine the code and identify the PRIMARY vulnerability type.
If the code is secure, respond with 'Benign'.
Response format: Category name only.

Code to analyze:
{{input}}

Result:""",
        ]

    def batch_predict(
        self,
        prompt: str,
        samples: List[Any],
        batch_idx: int
    ) -> Tuple[List[str], List[str]]:
        """æ‰¹é‡é¢„æµ‹ä¸€ä¸ª batch çš„æ ·æœ¬"""
        predictions = []
        ground_truths = []

        # å‡†å¤‡æ‰¹é‡æŸ¥è¯¢
        queries = []
        for sample in samples:
            code = sample.input_text
            query = prompt.format(input=code)
            queries.append(query)

            # è·å– ground truth
            ground_truth_binary = int(sample.target)
            cwe_codes = sample.metadata.get("cwe", [])

            if ground_truth_binary == 1 and cwe_codes:
                ground_truth_category = map_cwe_to_major(cwe_codes)
            else:
                ground_truth_category = "Benign"

            ground_truths.append(ground_truth_category)

        # æ‰¹é‡è°ƒç”¨ LLM
        print(f"      ğŸ” æ‰¹é‡é¢„æµ‹ {len(queries)} ä¸ªæ ·æœ¬...")
        try:
            responses = self.llm_client.batch_generate(
                queries,
                temperature=0.1,
                max_tokens=20,
                batch_size=min(8, len(queries)),
                concurrent=True
            )

            # è§„èŒƒåŒ–è¾“å‡º
            for response in responses:
                if response == "error":
                    predictions.append("Other")
                else:
                    predicted_category = canonicalize_category(response)
                    if predicted_category is None:
                        # å°è¯•ä»å“åº”ä¸­æå–
                        if "benign" in response.lower():
                            predicted_category = "Benign"
                        else:
                            predicted_category = "Other"
                    predictions.append(predicted_category)

        except Exception as e:
            print(f"      âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            predictions = ["Other"] * len(samples)

        return predictions, ground_truths

    def evaluate_prompt_on_dataset(
        self,
        prompt: str,
        dataset,
        generation: int,
        prompt_id: str,
        enable_evolution: bool = False
    ) -> Dict[str, Any]:
        """åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¯„ä¼° promptï¼Œä½¿ç”¨ batch å¤„ç†"""
        samples = dataset.get_samples()
        total_samples = len(samples)

        all_predictions = []
        all_ground_truths = []
        batch_analyses = []

        current_prompt = prompt
        num_batches = (total_samples + self.batch_size - 1) // self.batch_size

        print(f"    ğŸ“Š è¯„ä¼° prompt (å…± {num_batches} ä¸ª batches, {total_samples} ä¸ªæ ·æœ¬)")

        for batch_idx in range(num_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, total_samples)
            batch_samples = samples[start_idx:end_idx]

            print(f"      Batch {batch_idx + 1}/{num_batches} (æ ·æœ¬ {start_idx+1}-{end_idx})")

            # æ‰¹é‡é¢„æµ‹
            predictions, ground_truths = self.batch_predict(
                current_prompt, batch_samples, batch_idx
            )

            # åˆ†æ batch ç»“æœ
            batch_analysis = self.batch_analyzer.analyze_batch(
                predictions, ground_truths, batch_idx
            )

            print(f"        âœ“ å‡†ç¡®ç‡: {batch_analysis['accuracy']:.2%} ({batch_analysis['correct']}/{batch_analysis['batch_size']})")

            if batch_analysis["error_patterns"]:
                print(f"        âš ï¸ é”™è¯¯æ¨¡å¼: {len(batch_analysis['error_patterns'])} ç§")

            batch_analyses.append(batch_analysis)
            all_predictions.extend(predictions)
            all_ground_truths.extend(ground_truths)

            # æ ¹æ® batch åˆ†æè¿›åŒ– prompt (ä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹)
            if enable_evolution and batch_analysis["accuracy"] < 0.95:
                print(f"        ğŸ§¬ å°è¯•è¿›åŒ– prompt...")
                new_prompt = self.prompt_evolver.evolve_with_feedback(
                    current_prompt, batch_analysis, generation
                )
                if new_prompt != current_prompt:
                    print(f"        âœ… Prompt å·²è¿›åŒ–")
                    current_prompt = new_prompt

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        overall_accuracy = sum(p == g for p, g in zip(all_predictions, all_ground_truths)) / len(all_predictions)

        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            all_ground_truths,
            all_predictions,
            labels=CWE_MAJOR_CATEGORIES,
            output_dict=True,
            zero_division=0
        )

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(
            all_ground_truths,
            all_predictions,
            labels=CWE_MAJOR_CATEGORIES
        )

        return {
            "prompt_id": prompt_id,
            "generation": generation,
            "final_prompt": current_prompt,
            "accuracy": overall_accuracy,
            "total_samples": total_samples,
            "num_batches": num_batches,
            "batch_analyses": batch_analyses,
            "classification_report": report,
            "confusion_matrix": cm.tolist(),
            "predictions": all_predictions,
            "ground_truths": all_ground_truths,
        }

    def run_evolution(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è¿›åŒ–æµç¨‹"""
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹ PrimeVul Layer-1 å¹¶å‘æ¼æ´åˆ†ç±»")
        print("="*80 + "\n")

        # 1. å‡†å¤‡æ•°æ®
        print("ğŸ“ å‡†å¤‡æ•°æ®é›†...")
        primevul_dir = Path(self.config.get("primevul_dir", "./data/primevul/primevul"))
        sample_dir = Path(self.config.get("sample_dir", "./data/primevul_1percent_sample"))

        if not sample_dir.exists():
            print(f"   ç”Ÿæˆ 1% é‡‡æ ·æ•°æ®åˆ° {sample_dir}")
            sample_primevul_1percent(str(primevul_dir), str(sample_dir), seed=42)

        train_file = sample_dir / "train.txt"
        dev_file = sample_dir / "dev.txt"

        train_dataset = PrimevulDataset(str(train_file), "train")
        dev_dataset = PrimevulDataset(str(dev_file), "dev")

        print(f"   âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"   âœ… å¼€å‘é›†: {len(dev_dataset)} æ ·æœ¬")

        # 2. åŠ è½½åˆå§‹ prompts
        print("\nğŸ“ åŠ è½½åˆå§‹ prompts...")
        initial_prompts = self.load_initial_prompts()

        # 3. åˆå§‹è¯„ä¼°
        print(f"\nğŸ“Š åˆå§‹è¯„ä¼° ({len(initial_prompts)} ä¸ª prompts)...")
        population = []

        for i, prompt in enumerate(initial_prompts):
            print(f"\n  Prompt {i+1}/{len(initial_prompts)}")
            result = self.evaluate_prompt_on_dataset(
                prompt, dev_dataset, generation=0,
                prompt_id=f"initial_{i}", enable_evolution=False
            )
            individual = Individual(prompt)
            individual.fitness = result["accuracy"]
            population.append((individual, result))
            print(f"    âœ“ é€‚åº”åº¦: {individual.fitness:.4f}")

        # 4. è¿›åŒ–è¿‡ç¨‹
        max_generations = self.config.get("max_generations", 5)
        print(f"\nğŸ§¬ å¼€å§‹è¿›åŒ– (å…± {max_generations} ä»£)...")

        best_results = []

        for generation in range(1, max_generations + 1):
            print(f"\n{'='*80}")
            print(f"ğŸ“ˆ ç¬¬ {generation} ä»£è¿›åŒ–")
            print(f"{'='*80}\n")

            # é€‰æ‹©æœ€ä½³ä¸ªä½“
            population.sort(key=lambda x: x[0].fitness, reverse=True)
            best_individual, best_result = population[0]
            best_results.append(best_result)

            print(f"  å½“å‰æœ€ä½³é€‚åº”åº¦: {best_individual.fitness:.4f}")

            # åœ¨è®­ç»ƒé›†ä¸Šè¿›åŒ–æœ€ä½³ prompt
            print(f"\n  åœ¨è®­ç»ƒé›†ä¸Šè¿›åŒ–æœ€ä½³ prompt...")
            evolved_result = self.evaluate_prompt_on_dataset(
                best_individual.prompt,
                train_dataset,
                generation=generation,
                prompt_id=f"gen{generation}_best",
                enable_evolution=True
            )

            # åˆ›å»ºè¿›åŒ–åçš„ä¸ªä½“å¹¶åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°
            evolved_prompt = evolved_result["final_prompt"]
            if evolved_prompt != best_individual.prompt:
                print(f"\n  åœ¨å¼€å‘é›†ä¸Šè¯„ä¼°è¿›åŒ–åçš„ prompt...")
                eval_result = self.evaluate_prompt_on_dataset(
                    evolved_prompt,
                    dev_dataset,
                    generation=generation,
                    prompt_id=f"gen{generation}_evolved",
                    enable_evolution=False
                )

                evolved_individual = Individual(evolved_prompt)
                evolved_individual.fitness = eval_result["accuracy"]

                print(f"    è¿›åŒ–å‰é€‚åº”åº¦: {best_individual.fitness:.4f}")
                print(f"    è¿›åŒ–åé€‚åº”åº¦: {evolved_individual.fitness:.4f}")

                if evolved_individual.fitness > best_individual.fitness:
                    print(f"    âœ… æ¥å—è¿›åŒ–åçš„ prompt!")
                    population[0] = (evolved_individual, eval_result)
                else:
                    print(f"    âŒ ä¿ç•™åŸ prompt")

        # 5. æœ€ç»ˆç»“æœ
        population.sort(key=lambda x: x[0].fitness, reverse=True)
        best_individual, best_result = population[0]

        print(f"\n{'='*80}")
        print(f"ğŸ‰ è¿›åŒ–å®Œæˆ!")
        print(f"{'='*80}\n")
        print(f"  æœ€ç»ˆé€‚åº”åº¦: {best_individual.fitness:.4f}")

        # 6. ä¿å­˜ç»“æœ
        self.save_results(best_individual, best_result, best_results)

        return {
            "best_prompt": best_individual.prompt,
            "best_fitness": best_individual.fitness,
            "best_result": best_result,
            "evolution_history": best_results,
        }

    def save_results(
        self,
        best_individual: Individual,
        best_result: Dict[str, Any],
        evolution_history: List[Dict[str, Any]]
    ):
        """ä¿å­˜ç»“æœåˆ° result/ æ–‡ä»¶å¤¹"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ° {self.exp_dir}...")

        # 1. ä¿å­˜æœ€ç»ˆ prompt
        prompt_file = self.exp_dir / "final_prompt.txt"
        with open(prompt_file, "w", encoding="utf-8") as f:
            f.write(f"# æœ€ç»ˆä¼˜åŒ–çš„ Prompt (é€‚åº”åº¦: {best_individual.fitness:.4f})\n")
            f.write(f"# å®éªŒ ID: {self.exp_id}\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n\n")
            f.write(best_individual.prompt)
        print(f"  âœ“ {prompt_file}")

        # 2. ä¿å­˜åˆ†ç±»æŠ¥å‘Š (precision, recall, f1-score)
        report = best_result["classification_report"]
        metrics_file = self.exp_dir / "classification_metrics.json"
        with open(metrics_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ {metrics_file}")

        # 3. ä¿å­˜åˆ†ç±»æŠ¥å‘Šçš„æ˜“è¯»ç‰ˆæœ¬
        readable_report_file = self.exp_dir / "classification_report.txt"
        with open(readable_report_file, "w", encoding="utf-8") as f:
            f.write(f"PrimeVul Layer-1 åˆ†ç±»æŠ¥å‘Š\n")
            f.write(f"{'='*80}\n")
            f.write(f"å®éªŒ ID: {self.exp_id}\n")
            f.write(f"æœ€ç»ˆå‡†ç¡®ç‡: {best_individual.fitness:.4f}\n")
            f.write(f"æ€»æ ·æœ¬æ•°: {best_result['total_samples']}\n")
            f.write(f"Batch å¤§å°: {self.batch_size}\n")
            f.write(f"Batch æ€»æ•°: {best_result['num_batches']}\n\n")

            f.write(f"å„ç±»åˆ«æ€§èƒ½æŒ‡æ ‡:\n")
            f.write(f"{'-'*80}\n")
            f.write(f"{'Category':<25} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\n")
            f.write(f"{'-'*80}\n")

            for category in CWE_MAJOR_CATEGORIES:
                if category in report:
                    metrics = report[category]
                    f.write(f"{category:<25} {metrics['precision']:>10.4f} {metrics['recall']:>10.4f} "
                           f"{metrics['f1-score']:>10.4f} {metrics['support']:>10}\n")

            f.write(f"{'-'*80}\n")
            f.write(f"{'Overall (macro avg)':<25} {report['macro avg']['precision']:>10.4f} "
                   f"{report['macro avg']['recall']:>10.4f} {report['macro avg']['f1-score']:>10.4f} "
                   f"{report['macro avg']['support']:>10}\n")
            f.write(f"{'Overall (weighted avg)':<25} {report['weighted avg']['precision']:>10.4f} "
                   f"{report['weighted avg']['recall']:>10.4f} {report['weighted avg']['f1-score']:>10.4f} "
                   f"{report['weighted avg']['support']:>10}\n")

        print(f"  âœ“ {readable_report_file}")

        # 4. ä¿å­˜æ··æ·†çŸ©é˜µ
        confusion_file = self.exp_dir / "confusion_matrix.json"
        with open(confusion_file, "w", encoding="utf-8") as f:
            json.dump({
                "labels": CWE_MAJOR_CATEGORIES,
                "matrix": best_result["confusion_matrix"]
            }, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ {confusion_file}")

        # 5. ä¿å­˜ batch åˆ†æå†å²
        batch_history_file = self.exp_dir / "batch_analyses.jsonl"
        with open(batch_history_file, "w", encoding="utf-8") as f:
            for analysis in best_result["batch_analyses"]:
                f.write(json.dumps(analysis, ensure_ascii=False) + "\n")
        print(f"  âœ“ {batch_history_file}")

        # 6. ä¿å­˜å®Œæ•´çš„å®éªŒé…ç½®å’Œç»“æœ
        summary_file = self.exp_dir / "experiment_summary.json"
        summary = {
            "experiment_id": self.exp_id,
            "timestamp": datetime.now().isoformat(),
            "config": self.config,
            "best_fitness": best_individual.fitness,
            "best_prompt": best_individual.prompt,
            "total_samples": best_result["total_samples"],
            "num_batches": best_result["num_batches"],
            "batch_size": self.batch_size,
            "classification_report": report,
        }
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ {summary_file}")

        # 7. æ‰“å°æœ€ç»ˆæŠ¥å‘Šåˆ°æ§åˆ¶å°
        print(f"\nğŸ“Š æœ€ç»ˆåˆ†ç±»æ€§èƒ½:")
        print(f"{'-'*80}")
        print(f"{'Category':<25} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}")
        print(f"{'-'*80}")

        for category in CWE_MAJOR_CATEGORIES:
            if category in report:
                metrics = report[category]
                print(f"{category:<25} {metrics['precision']:>10.4f} {metrics['recall']:>10.4f} "
                     f"{metrics['f1-score']:>10.4f} {metrics['support']:>10.0f}")

        print(f"{'-'*80}")
        print(f"{'Overall (macro avg)':<25} {report['macro avg']['precision']:>10.4f} "
              f"{report['macro avg']['recall']:>10.4f} {report['macro avg']['f1-score']:>10.4f}")


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="EvoPrompt Main - PrimeVul Layer-1 å¹¶å‘æ¼æ´åˆ†ç±»")
    parser.add_argument("--batch-size", type=int, default=16, help="æ¯ä¸ª batch çš„æ ·æœ¬æ•°")
    parser.add_argument("--max-generations", type=int, default=5, help="æœ€å¤§è¿›åŒ–ä»£æ•°")
    parser.add_argument("--primevul-dir", type=str, default="./data/primevul/primevul",
                       help="PrimeVul æ•°æ®é›†ç›®å½•")
    parser.add_argument("--sample-dir", type=str, default="./data/primevul_1percent_sample",
                       help="é‡‡æ ·æ•°æ®ç›®å½•")
    parser.add_argument("--experiment-id", type=str, default=None,
                       help="å®éªŒ ID (é»˜è®¤è‡ªåŠ¨ç”Ÿæˆ)")

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®
    config = {
        "batch_size": args.batch_size,
        "max_generations": args.max_generations,
        "primevul_dir": args.primevul_dir,
        "sample_dir": args.sample_dir,
        "experiment_id": args.experiment_id,
    }

    # åˆ›å»ºå¹¶è¿è¡Œ pipeline
    pipeline = PrimeVulLayer1Pipeline(config)
    results = pipeline.run_evolution()

    print(f"\nâœ… å®éªŒå®Œæˆ!")
    print(f"ğŸ“‚ ç»“æœå·²ä¿å­˜åˆ°: {pipeline.exp_dir}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
