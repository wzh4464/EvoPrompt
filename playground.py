#!/usr/bin/env python3
"""
æµ‹è¯•æœ€ä½³promptåœ¨è®­ç»ƒé›†ä¸Šçš„æ³›åŒ–æ€§èƒ½
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, 'src')

from evoprompt.data.dataset import PrimevulDataset
from evoprompt.data.cwe_categories import map_cwe_to_major, canonicalize_category, CWE_MAJOR_CATEGORIES
from evoprompt.llm.client import create_default_client
from evoprompt.llm.async_client import AsyncLLMClient
import asyncio
import aiohttp


def load_sampling_stats(stats_file: str) -> Dict[str, Any]:
    """åŠ è½½é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯"""
    with open(stats_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_train_samples(train_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½è®­ç»ƒæ ·æœ¬"""
    samples = []
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                samples.append(json.loads(line))
    return samples


async def evaluate_prompt_on_samples_async(prompt: str, samples: List[Dict[str, Any]], 
                                         async_client: AsyncLLMClient, sample_limit: int = None) -> Dict[str, Any]:
    """åœ¨æ ·æœ¬ä¸Šå¼‚æ­¥è¯„ä¼°promptæ€§èƒ½"""
    if sample_limit:
        samples = samples[:sample_limit]
    
    print(f"ğŸ” å¼€å§‹å¼‚æ­¥è¯„ä¼° {len(samples)} ä¸ªæ ·æœ¬...")
    print(f"ğŸš€ ä½¿ç”¨å¹¶å‘å®¢æˆ·ç«¯ï¼Œæœ€å¤§å¹¶å‘æ•°: {async_client.max_concurrency}")
    
    correct = 0
    total = len(samples)
    category_results = {}
    cwe_results = {}
    
    # åˆå§‹åŒ–ç±»åˆ«ç»Ÿè®¡
    for category in CWE_MAJOR_CATEGORIES + ['Benign']:
        category_results[category] = {
            'total': 0,
            'correct': 0,
            'accuracy': 0.0,
            'predictions': {}
        }
    
    start_time = time.time()
    
    # å‡†å¤‡æ‰¹é‡æŸ¥è¯¢
    batch_queries = []
    batch_samples = []
    batch_metadata = []
    
    for idx, sample in enumerate(samples):
        code = sample.get('input_text', '')
        ground_truth_binary = int(sample.get('target', 0))
        
        # è·å–CWEä»£ç å’ŒçœŸå®ç±»åˆ«
        cwe_codes = sample.get('metadata', {}).get('cwe', [])
        if ground_truth_binary == 1 and cwe_codes:
            ground_truth_category = map_cwe_to_major(cwe_codes)
        else:
            ground_truth_category = "Benign"
        
        # æ„å»ºæŸ¥è¯¢
        query = prompt.format(input=code)
        batch_queries.append(query)
        batch_samples.append(sample)
        batch_metadata.append({
            'idx': idx,
            'ground_truth_binary': ground_truth_binary,
            'ground_truth_category': ground_truth_category,
            'cwe_codes': cwe_codes
        })
    
    # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹8ä¸ªå¹¶å‘è¯·æ±‚
    batch_size = 8
    total_batches = (total + batch_size - 1) // batch_size
    
    print(f"ğŸ“¦ åˆ†æ‰¹å¤„ç†: {total} ä¸ªæ ·æœ¬ï¼Œ{total_batches} æ‰¹ï¼Œæ¯æ‰¹ {batch_size} ä¸ª")
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total)
        current_batch_size = end_idx - start_idx
        
        print(f"   ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (æ ·æœ¬ {start_idx + 1}-{end_idx})")
        
        # è·å–å½“å‰æ‰¹æ¬¡çš„æŸ¥è¯¢
        current_queries = batch_queries[start_idx:end_idx]
        current_samples = batch_samples[start_idx:end_idx]
        current_metadata = batch_metadata[start_idx:end_idx]
        
        try:
            # å¹¶å‘è°ƒç”¨LLM
            prediction_texts = await async_client.batch_generate_async(
                current_queries,
                temperature=0.1,
                max_tokens=50
            )
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
            for i, (sample, metadata, prediction_text) in enumerate(zip(current_samples, current_metadata, prediction_texts)):
                if prediction_text == "error":
                    print(f"     âš ï¸ æ ·æœ¬ {metadata['idx'] + 1} é¢„æµ‹å¤±è´¥")
                    continue
                
                # è§„èŒƒåŒ–é¢„æµ‹ç»“æœ
                predicted_category = canonicalize_category(prediction_text)
                if predicted_category is None:
                    if any(word in prediction_text.lower() for word in ['vulnerable', 'vulnerability', 'vuln', 'exploit']):
                        predicted_category = "Other"
                    else:
                        predicted_category = "Benign"
                
                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
                is_correct = (predicted_category == metadata['ground_truth_category'])
                if is_correct:
                    correct += 1
                
                # æ›´æ–°ç±»åˆ«ç»Ÿè®¡
                ground_truth_category = metadata['ground_truth_category']
                if ground_truth_category not in category_results:
                    category_results[ground_truth_category] = {
                        'total': 0,
                        'correct': 0,
                        'accuracy': 0.0,
                        'predictions': {}
                    }
                
                cat_stats = category_results[ground_truth_category]
                cat_stats['total'] += 1
                if is_correct:
                    cat_stats['correct'] += 1
                
                # è®°å½•é¢„æµ‹åˆ†å¸ƒ
                if predicted_category not in cat_stats['predictions']:
                    cat_stats['predictions'][predicted_category] = 0
                cat_stats['predictions'][predicted_category] += 1
                
                # æ›´æ–°CWEç»Ÿè®¡
                for cwe in metadata['cwe_codes']:
                    if cwe not in cwe_results:
                        cwe_results[cwe] = {
                            'total': 0,
                            'correct': 0,
                            'accuracy': 0.0
                        }
                    cwe_results[cwe]['total'] += 1
                    if is_correct:
                        cwe_results[cwe]['correct'] += 1
                
                # æ˜¾ç¤ºä¸€äº›é”™è¯¯æ¡ˆä¾‹
                if not is_correct and metadata['idx'] < 100:  # åªæ˜¾ç¤ºå‰100ä¸ªé”™è¯¯æ¡ˆä¾‹
                    print(f"     âŒ æ ·æœ¬ {metadata['idx'] + 1} é¢„æµ‹é”™è¯¯:")
                    print(f"        çœŸå®: {ground_truth_category} | é¢„æµ‹: {predicted_category}")
                    print(f"        CWE: {metadata['cwe_codes']}")
                    print(f"        ä»£ç ç‰‡æ®µ: {sample.get('input_text', '')[:100]}...")
                    print(f"        LLMè¾“å‡º: {prediction_text}")
                    print()
            
            # æ˜¾ç¤ºæ‰¹æ¬¡è¿›åº¦
            processed_samples = (batch_idx + 1) * batch_size
            if processed_samples > total:
                processed_samples = total
            
            elapsed = time.time() - start_time
            avg_time = elapsed / processed_samples
            remaining = (total - processed_samples) * avg_time
            
            print(f"     ğŸ“Š æ‰¹æ¬¡å®Œæˆ: {processed_samples}/{total} ({processed_samples/total*100:.1f}%) "
                  f"| å·²ç”¨: {elapsed:.1f}s | é¢„è®¡å‰©ä½™: {remaining:.1f}s")
            
        except Exception as e:
            print(f"     âŒ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¤±è´¥: {e}")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
            continue
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    overall_accuracy = correct / total if total > 0 else 0
    
    # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
    for category in category_results:
        cat_stats = category_results[category]
        if cat_stats['total'] > 0:
            cat_stats['accuracy'] = cat_stats['correct'] / cat_stats['total']
    
    # è®¡ç®—CWEå‡†ç¡®ç‡
    for cwe in cwe_results:
        cwe_stats = cwe_results[cwe]
        if cwe_stats['total'] > 0:
            cwe_stats['accuracy'] = cwe_stats['correct'] / cwe_stats['total']
    
    return {
        'overall_accuracy': overall_accuracy,
        'correct_predictions': correct,
        'total_samples': total,
        'evaluation_time': total_time,
        'avg_time_per_sample': total_time / total if total > 0 else 0,
        'category_results': category_results,
        'cwe_results': cwe_results,
        'category_summary': {
            'best_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy'],
                reverse=True
            )[:5],
            'worst_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy']
            )[:5],
            'most_common_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['total'],
                reverse=True
            )[:5]
        },
        'cwe_summary': {
            'best_cwes': sorted(
                [(cwe, stats) for cwe, stats in cwe_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy'],
                reverse=True
            )[:10],
            'worst_cwes': sorted(
                [(cwe, stats) for cwe, stats in cwe_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy']
            )[:10]
        }
    }


def evaluate_prompt_on_samples(prompt: str, samples: List[Dict[str, Any]], 
                             llm_client, sample_limit: int = None) -> Dict[str, Any]:
    """åœ¨æ ·æœ¬ä¸Šè¯„ä¼°promptæ€§èƒ½ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰"""
    if sample_limit:
        samples = samples[:sample_limit]
    
    print(f"ğŸ” å¼€å§‹è¯„ä¼° {len(samples)} ä¸ªæ ·æœ¬...")
    
    correct = 0
    total = len(samples)
    category_results = {}
    cwe_results = {}
    
    # åˆå§‹åŒ–ç±»åˆ«ç»Ÿè®¡
    for category in CWE_MAJOR_CATEGORIES + ['Benign']:
        category_results[category] = {
            'total': 0,
            'correct': 0,
            'accuracy': 0.0,
            'predictions': {}
        }
    
    start_time = time.time()
    
    for idx, sample in enumerate(samples):
        try:
            # è·å–æ ·æœ¬ä¿¡æ¯
            code = sample.get('input_text', '')
            ground_truth_binary = int(sample.get('target', 0))
            
            # è·å–CWEä»£ç å’ŒçœŸå®ç±»åˆ«
            cwe_codes = sample.get('metadata', {}).get('cwe', [])
            if ground_truth_binary == 1 and cwe_codes:
                ground_truth_category = map_cwe_to_major(cwe_codes)
            else:
                ground_truth_category = "Benign"
            
            # æ„å»ºæŸ¥è¯¢
            query = prompt.format(input=code)
            
            # è°ƒç”¨LLM
            prediction_text = llm_client.generate(
                query, 
                temperature=0.1, 
                max_tokens=50
            )
            
            # è§„èŒƒåŒ–é¢„æµ‹ç»“æœ
            predicted_category = canonicalize_category(prediction_text)
            if predicted_category is None:
                if any(word in prediction_text.lower() for word in ['vulnerable', 'vulnerability', 'vuln', 'exploit']):
                    predicted_category = "Other"
                else:
                    predicted_category = "Benign"
            
            # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
            is_correct = (predicted_category == ground_truth_category)
            if is_correct:
                correct += 1
            
            # æ›´æ–°ç±»åˆ«ç»Ÿè®¡
            if ground_truth_category not in category_results:
                category_results[ground_truth_category] = {
                    'total': 0,
                    'correct': 0,
                    'accuracy': 0.0,
                    'predictions': {}
                }
            
            cat_stats = category_results[ground_truth_category]
            cat_stats['total'] += 1
            if is_correct:
                cat_stats['correct'] += 1
            
            # è®°å½•é¢„æµ‹åˆ†å¸ƒ
            if predicted_category not in cat_stats['predictions']:
                cat_stats['predictions'][predicted_category] = 0
            cat_stats['predictions'][predicted_category] += 1
            
            # æ›´æ–°CWEç»Ÿè®¡
            for cwe in cwe_codes:
                if cwe not in cwe_results:
                    cwe_results[cwe] = {
                        'total': 0,
                        'correct': 0,
                        'accuracy': 0.0
                    }
                cwe_results[cwe]['total'] += 1
                if is_correct:
                    cwe_results[cwe]['correct'] += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if (idx + 1) % 20 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / (idx + 1)
                remaining = (total - idx - 1) * avg_time
                print(f"   ğŸ“Š è¿›åº¦: {idx + 1}/{total} ({((idx + 1)/total)*100:.1f}%) "
                      f"| å·²ç”¨: {elapsed:.1f}s | é¢„è®¡å‰©ä½™: {remaining:.1f}s")
            
            # æ˜¾ç¤ºä¸€äº›é”™è¯¯æ¡ˆä¾‹
            if not is_correct and idx < 100:  # åªæ˜¾ç¤ºå‰100ä¸ªé”™è¯¯æ¡ˆä¾‹
                print(f"   âŒ æ ·æœ¬ {idx + 1} é¢„æµ‹é”™è¯¯:")
                print(f"      çœŸå®: {ground_truth_category} | é¢„æµ‹: {predicted_category}")
                print(f"      CWE: {cwe_codes}")
                print(f"      ä»£ç ç‰‡æ®µ: {code[:100]}...")
                print(f"      LLMè¾“å‡º: {prediction_text}")
                print()
                
        except Exception as e:
            print(f"   âš ï¸ æ ·æœ¬ {idx + 1} å¤„ç†å¤±è´¥: {e}")
            continue
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    overall_accuracy = correct / total if total > 0 else 0
    
    # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
    for category in category_results:
        cat_stats = category_results[category]
        if cat_stats['total'] > 0:
            cat_stats['accuracy'] = cat_stats['correct'] / cat_stats['total']
    
    # è®¡ç®—CWEå‡†ç¡®ç‡
    for cwe in cwe_results:
        cwe_stats = cwe_results[cwe]
        if cwe_stats['total'] > 0:
            cwe_stats['accuracy'] = cwe_stats['correct'] / cwe_stats['total']
    
    return {
        'overall_accuracy': overall_accuracy,
        'correct_predictions': correct,
        'total_samples': total,
        'evaluation_time': total_time,
        'avg_time_per_sample': total_time / total if total > 0 else 0,
        'category_results': category_results,
        'cwe_results': cwe_results,
        'category_summary': {
            'best_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy'],
                reverse=True
            )[:5],
            'worst_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy']
            )[:5],
            'most_common_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['total'],
                reverse=True
            )[:5]
        },
        'cwe_summary': {
            'best_cwes': sorted(
                [(cwe, stats) for cwe, stats in cwe_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy'],
                reverse=True
            )[:10],
            'worst_cwes': sorted(
                [(cwe, stats) for cwe, stats in cwe_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy']
            )[:10]
        }
    }
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    overall_accuracy = correct / total if total > 0 else 0
    
    # è®¡ç®—å„ç±»åˆ«å‡†ç¡®ç‡
    for category in category_results:
        cat_stats = category_results[category]
        if cat_stats['total'] > 0:
            cat_stats['accuracy'] = cat_stats['correct'] / cat_stats['total']
    
    # è®¡ç®—CWEå‡†ç¡®ç‡
    for cwe in cwe_results:
        cwe_stats = cwe_results[cwe]
        if cwe_stats['total'] > 0:
            cwe_stats['accuracy'] = cwe_stats['correct'] / cwe_stats['total']
    
    return {
        'overall_accuracy': overall_accuracy,
        'correct_predictions': correct,
        'total_samples': total,
        'evaluation_time': total_time,
        'avg_time_per_sample': total_time / total if total > 0 else 0,
        'category_results': category_results,
        'cwe_results': cwe_results,
        'category_summary': {
            'best_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy'],
                reverse=True
            )[:5],
            'worst_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy']
            )[:5],
            'most_common_categories': sorted(
                [(cat, stats) for cat, stats in category_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['total'],
                reverse=True
            )[:5]
        },
        'cwe_summary': {
            'best_cwes': sorted(
                [(cwe, stats) for cwe, stats in cwe_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy'],
                reverse=True
            )[:10],
            'worst_cwes': sorted(
                [(cwe, stats) for cwe, stats in cwe_results.items() if stats['total'] > 0],
                key=lambda x: x[1]['accuracy']
            )[:10]
        }
    }


def print_evaluation_results(results: Dict[str, Any]):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print("\n" + "="*80)
    print("ğŸ¯ PROMPT æ³›åŒ–æ€§èƒ½è¯„ä¼°ç»“æœ")
    print("="*80)
    
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½:")
    print(f"   å‡†ç¡®ç‡: {results['overall_accuracy']:.4f} ({results['correct_predictions']}/{results['total_samples']})")
    print(f"   è¯„ä¼°æ—¶é—´: {results['evaluation_time']:.2f}ç§’")
    print(f"   å¹³å‡æ¯æ ·æœ¬: {results['avg_time_per_sample']:.3f}ç§’")
    
    print(f"\nğŸ† è¡¨ç°æœ€ä½³çš„CWEå¤§ç±» (å‰5å):")
    for i, (category, stats) in enumerate(results['category_summary']['best_categories'], 1):
        print(f"   {i}. {category}: {stats['accuracy']:.4f} ({stats['correct']}/{stats['total']})")
    
    print(f"\nâŒ è¡¨ç°æœ€å·®çš„CWEå¤§ç±» (å‰5å):")
    for i, (category, stats) in enumerate(results['category_summary']['worst_categories'], 1):
        print(f"   {i}. {category}: {stats['accuracy']:.4f} ({stats['correct']}/{stats['total']})")
    
    print(f"\nğŸ“ˆ æ ·æœ¬æœ€å¤šçš„CWEå¤§ç±» (å‰5å):")
    for i, (category, stats) in enumerate(results['category_summary']['most_common_categories'], 1):
        print(f"   {i}. {category}: {stats['total']} æ ·æœ¬, å‡†ç¡®ç‡ {stats['accuracy']:.4f}")
    
    print(f"\nğŸ” å„ç±»åˆ«è¯¦ç»†ç»“æœ:")
    for category, stats in sorted(results['category_results'].items()):
        if stats['total'] > 0:
            print(f"   {category}: {stats['accuracy']:.4f} ({stats['correct']}/{stats['total']})")
            if stats['predictions']:
                pred_str = ", ".join([f"{pred}:{count}" for pred, count in stats['predictions'].items()])
                print(f"     é¢„æµ‹åˆ†å¸ƒ: {pred_str}")
    
    print(f"\nğŸ¯ è¡¨ç°æœ€ä½³çš„CWEä»£ç  (å‰10å):")
    for i, (cwe, stats) in enumerate(results['cwe_summary']['best_cwes'], 1):
        print(f"   {i}. {cwe}: {stats['accuracy']:.4f} ({stats['correct']}/{stats['total']})")
    
    print(f"\nâŒ è¡¨ç°æœ€å·®çš„CWEä»£ç  (å‰10å):")
    for i, (cwe, stats) in enumerate(results['cwe_summary']['worst_cwes'], 1):
        print(f"   {i}. {cwe}: {stats['accuracy']:.4f} ({stats['correct']}/{stats['total']})")


def save_evaluation_results(results: Dict[str, Any], output_file: str):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    print("ğŸ® Prompt æ³›åŒ–æ€§èƒ½æµ‹è¯• Playground (å¼‚æ­¥å¹¶å‘ç‰ˆæœ¬)")
    print("="*60)
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    print("ğŸ”§ åŠ è½½ç¯å¢ƒå˜é‡...")
    load_dotenv()
    
    # é…ç½®è·¯å¾„
    stats_file = "data/primevul_1percent_sample/sampling_stats.json"
    train_file = "data/primevul_1percent_sample/dev_sample.jsonl"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(stats_file):
        print(f"âŒ é‡‡æ ·ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨: {stats_file}")
        return 1
    
    if not os.path.exists(train_file):
        print(f"âŒ è®­ç»ƒæ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
        return 1
    
    # åŠ è½½é‡‡æ ·ç»Ÿè®¡
    print("ğŸ“Š åŠ è½½é‡‡æ ·ç»Ÿè®¡...")
    stats = load_sampling_stats(stats_file)
    print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {stats['sample_ratio']:.3f}")
    print(f"   è®­ç»ƒæ ·æœ¬: {stats['sampled_total']}")
    
    # åŠ è½½è®­ç»ƒæ ·æœ¬
    print("ğŸ“ åŠ è½½è®­ç»ƒæ ·æœ¬...")
    train_samples = load_train_samples(train_file)
    print(f"   å·²åŠ è½½: {len(train_samples)} ä¸ªæ ·æœ¬")
    
    # æ£€æŸ¥APIé…ç½®
    api_key = os.getenv("API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® API_KEY ç¯å¢ƒå˜é‡")
        print("   åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: API_KEY='your-api-key-here'")
        return 1
    
    # åˆ›å»ºå¼‚æ­¥LLMå®¢æˆ·ç«¯
    print("ğŸ¤– åˆå§‹åŒ–å¼‚æ­¥LLMå®¢æˆ·ç«¯...")
    async_client = AsyncLLMClient(
        api_key=api_key,
        max_concurrency=8,  # è®¾ç½®8ä¸ªå¹¶å‘è¯·æ±‚
        timeout=30
    )
    
    print(f"   âœ… å¹¶å‘å®¢æˆ·ç«¯å·²åˆ›å»ºï¼Œæœ€å¤§å¹¶å‘æ•°: {async_client.max_concurrency}")
    
    # æœ€ä½³prompt
    best_prompt = """You are an expert cybersecurity analyst with deep knowledge of CWE patterns and vulnerability detection. Conduct a comprehensive security assessment using a systematic multi-phase approach:

**Phase 1 - Attacker Mindset & Entry Point Analysis:**
- Identify the most attractive attack vectors and entry points an attacker would target first
- Examine user-controlled inputs, external interfaces, and data sources that could be weaponized
- Consider how vulnerabilities might be chained for maximum exploitation impact

**Phase 2 - CWE Pattern Recognition & Static Analysis:**
Systematically detect vulnerability patterns across major CWE categories:
- **Buffer Errors (CWE-120,119,787)**: overflows, underflows, bounds violations, memory corruption
- **Injection Flaws (CWE-78,79,89)**: SQL injection, command injection, XSS, code injection, format string attacks
- **Memory Management (CWE-416,415,401)**: use-after-free, double-free, memory leaks, improper cleanup
- **Pointer Dereference (CWE-476)**: null pointer dereference, invalid pointer usage, dangling pointers
- **Integer Errors (CWE-190,191)**: integer overflow, underflow, wraparound, signedness issues
- **Concurrency Issues (CWE-362)**: race conditions, synchronization problems, deadlocks
- **Path Traversal (CWE-22)**: directory traversal, path manipulation, file inclusion attacks
- **Cryptography Issues (CWE-327,326)**: weak algorithms, broken crypto, improper key management
- **Information Exposure (CWE-200)**: data leaks, privacy violations, sensitive info disclosure
- **Other Security Issues**: logic flaws, design weaknesses, implementation errors

**Phase 3 - Classification Decision:**
Based on the analysis, classify the code into the most appropriate CWE major category above. If no vulnerability is found, respond with 'Benign'.

Code: {input}

Security assessment:"""
    
    print(f"\nğŸ¯ å¼€å§‹è¯„ä¼°æœ€ä½³prompt...")
    print(f"Prompté•¿åº¦: {len(best_prompt)} å­—ç¬¦")
    
    # è¯¢é—®æ˜¯å¦é™åˆ¶æ ·æœ¬æ•°é‡
    sample_limit = input("\nğŸ”¢ è¾“å…¥è¦è¯„ä¼°çš„æ ·æœ¬æ•°é‡ (ç›´æ¥å›è½¦è¯„ä¼°å…¨éƒ¨): ").strip()
    if sample_limit:
        try:
            sample_limit = int(sample_limit)
            print(f"   å°†è¯„ä¼°å‰ {sample_limit} ä¸ªæ ·æœ¬")
        except ValueError:
            sample_limit = None
            print("   è¾“å…¥æ— æ•ˆï¼Œå°†è¯„ä¼°å…¨éƒ¨æ ·æœ¬")
    else:
        sample_limit = None
        print("   å°†è¯„ä¼°å…¨éƒ¨æ ·æœ¬")
    
    # æ‰§è¡Œå¼‚æ­¥è¯„ä¼°
    print(f"\nğŸš€ å¯åŠ¨å¼‚æ­¥å¹¶å‘è¯„ä¼°...")
    results = await evaluate_prompt_on_samples_async(best_prompt, train_samples, async_client, sample_limit)
    
    # æ˜¾ç¤ºç»“æœ
    print_evaluation_results(results)
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = f"playground_results_async_{timestamp}.json"
    save_evaluation_results(results, output_file)
    
    print(f"\nâœ… å¼‚æ­¥æ³›åŒ–æ€§èƒ½æµ‹è¯•å®Œæˆ!")
    return 0


def main():
    """ä¸»å‡½æ•°ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰"""
    print("ğŸ® Prompt æ³›åŒ–æ€§èƒ½æµ‹è¯• Playground (åŒæ­¥ç‰ˆæœ¬)")
    print("="*50)
    
    # é…ç½®è·¯å¾„
    stats_file = "data/primevul_1percent_sample/sampling_stats.json"
    train_file = "data/primevul_1percent_sample/dev_sample.jsonl"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(stats_file):
        print(f"âŒ é‡‡æ ·ç»Ÿè®¡æ–‡ä»¶ä¸å­˜åœ¨: {stats_file}")
        return 1
    
    if not os.path.exists(train_file):
        print(f"âŒ è®­ç»ƒæ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
        return 1
    
    # åŠ è½½é‡‡æ ·ç»Ÿè®¡
    print("ğŸ“Š åŠ è½½é‡‡æ ·ç»Ÿè®¡...")
    stats = load_sampling_stats(stats_file)
    print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"   é‡‡æ ·æ¯”ä¾‹: {stats['sample_ratio']:.3f}")
    print(f"   è®­ç»ƒæ ·æœ¬: {stats['sampled_total']}")
    
    # åŠ è½½è®­ç»ƒæ ·æœ¬
    print("ğŸ“ åŠ è½½è®­ç»ƒæ ·æœ¬...")
    train_samples = load_train_samples(train_file)
    print(f"   å·²åŠ è½½: {len(train_samples)} ä¸ªæ ·æœ¬")
    
    # æ£€æŸ¥APIé…ç½®
    api_key = os.getenv("API_KEY")
    if not api_key:
        print("âŒ è¯·è®¾ç½® API_KEY ç¯å¢ƒå˜é‡")
        print("   åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: API_KEY='your-api-key-here'")
        return 1
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    print("ğŸ¤– åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
    llm_client = create_default_client()
    
    # æœ€ä½³prompt
    best_prompt = """You are an expert cybersecurity analyst with deep knowledge of CWE patterns and vulnerability detection. Conduct a comprehensive security assessment using a systematic multi-phase approach:

**Phase 1 - Attacker Mindset & Entry Point Analysis:**
- Identify the most attractive attack vectors and entry points an attacker would target first
- Examine user-controlled inputs, external interfaces, and data sources that could be weaponized
- Consider how vulnerabilities might be chained for maximum exploitation impact

**Phase 2 - CWE Pattern Recognition & Static Analysis:**
Systematically detect vulnerability patterns across major CWE categories:
- **Buffer Errors (CWE-120,119,787)**: overflows, underflows, bounds violations, memory corruption
- **Injection Flaws (CWE-78,79,89)**: SQL injection, command injection, XSS, code injection, format string attacks

Code: {input}

Security assessment:"""
    
    print(f"\nğŸ¯ å¼€å§‹è¯„ä¼°æœ€ä½³prompt...")
    print(f"Prompté•¿åº¦: {len(best_prompt)} å­—ç¬¦")
    
    # è¯¢é—®æ˜¯å¦é™åˆ¶æ ·æœ¬æ•°é‡
    sample_limit = input("\nğŸ”¢ è¾“å…¥è¦è¯„ä¼°çš„æ ·æœ¬æ•°é‡ (ç›´æ¥å›è½¦è¯„ä¼°å…¨éƒ¨): ").strip()
    if sample_limit:
        try:
            sample_limit = int(sample_limit)
            print(f"   å°†è¯„ä¼°å‰ {sample_limit} ä¸ªæ ·æœ¬")
        except ValueError:
            sample_limit = None
            print("   è¾“å…¥æ— æ•ˆï¼Œå°†è¯„ä¼°å…¨éƒ¨æ ·æœ¬")
    else:
        sample_limit = None
        print("   å°†è¯„ä¼°å…¨éƒ¨æ ·æœ¬")
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluate_prompt_on_samples(best_prompt, train_samples, llm_client, sample_limit)
    
    # æ˜¾ç¤ºç»“æœ
    print_evaluation_results(results)
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = f"playground_results_{timestamp}.json"
    save_evaluation_results(results, output_file)
    
    print(f"\nâœ… æ³›åŒ–æ€§èƒ½æµ‹è¯•å®Œæˆ!")
    return 0


if __name__ == "__main__":
    # è¯¢é—®ç”¨æˆ·é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("ğŸ® é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å¼‚æ­¥å¹¶å‘æ¨¡å¼ (æ¨èï¼Œ8ä¸ªå¹¶å‘è¯·æ±‚)")
    print("2. åŒæ­¥æ¨¡å¼ (å…¼å®¹æ€§)")
    
    choice = input("\nè¯·é€‰æ‹© (1/2ï¼Œé»˜è®¤1): ").strip()
    
    if choice == "2":
        # åŒæ­¥æ¨¡å¼
        sys.exit(main())
    else:
        # å¼‚æ­¥æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        try:
            exit_code = asyncio.run(main_async())
            sys.exit(exit_code)
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
            sys.exit(1)
        except Exception as e:
            print(f"\nâŒ å¼‚æ­¥æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
