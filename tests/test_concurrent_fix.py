#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®æ”¹åçš„å¹¶å‘æ‰¹å¤„ç†åŠŸèƒ½
"""

import sys
from pathlib import Path

sys.path.insert(0, 'src')
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥scriptsæ¨¡å—
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_openai_client_concurrent():
    """æµ‹è¯•OpenAIå®¢æˆ·ç«¯å¹¶å‘åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•OpenAIå®¢æˆ·ç«¯å¹¶å‘åŠŸèƒ½...")
    
    from evoprompt.llm.client import create_default_client
    import time
    
    client = create_default_client()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¹¶å‘æ–¹æ³•
    if not hasattr(client, '_process_batch_concurrent'):
        print("   âŒ å®¢æˆ·ç«¯ç¼ºå°‘_process_batch_concurrentæ–¹æ³•")
        return False
    
    print("   âœ… å®¢æˆ·ç«¯åŒ…å«å¹¶å‘å¤„ç†æ–¹æ³•")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_prompts = [
        f"What is vulnerability {i}? One word answer." 
        for i in range(1, 6)  # 5ä¸ªprompt
    ]
    
    print(f"   ğŸ“ å‡†å¤‡äº† {len(test_prompts)} ä¸ªæµ‹è¯•prompt")
    
    # æµ‹è¯•é¡ºåºå¤„ç†
    print("   ğŸ”„ æµ‹è¯•é¡ºåºå¤„ç†...")
    start_time = time.time()
    
    try:
        sequential_results = client.batch_generate(
            test_prompts,
            batch_size=8,
            concurrent=False,  # é¡ºåºå¤„ç†
            max_tokens=10
        )
        sequential_time = time.time() - start_time
        sequential_success = sum(1 for r in sequential_results if r != "error")
        
        print(f"      â±ï¸ é¡ºåºå¤„ç†è€—æ—¶: {sequential_time:.2f}ç§’")
        print(f"      âœ… æˆåŠŸ: {sequential_success}/{len(test_prompts)}")
        
        # æµ‹è¯•å¹¶å‘å¤„ç†
        print("   ğŸš€ æµ‹è¯•å¹¶å‘å¤„ç†...")
        start_time = time.time()
        
        concurrent_results = client.batch_generate(
            test_prompts,
            batch_size=8, 
            concurrent=True,   # å¹¶å‘å¤„ç†
            max_tokens=10
        )
        concurrent_time = time.time() - start_time
        concurrent_success = sum(1 for r in concurrent_results if r != "error")
        
        print(f"      â±ï¸ å¹¶å‘å¤„ç†è€—æ—¶: {concurrent_time:.2f}ç§’")
        print(f"      âœ… æˆåŠŸ: {concurrent_success}/{len(test_prompts)}")
        
        # æ€§èƒ½å¯¹æ¯”
        if sequential_time > 0 and concurrent_time > 0:
            speedup = sequential_time / concurrent_time
            print(f"      ğŸ“ˆ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup > 1.2:
                print("      ğŸ‰ å¹¶å‘å¤„ç†æ˜æ˜¾æ›´å¿«ï¼")
            else:
                print("      â„¹ï¸ æ€§èƒ½æå‡ä¸æ˜æ˜¾ï¼ˆå¯èƒ½ç”±äºAPIæˆ–ç½‘ç»œé™åˆ¶ï¼‰")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_config_and_params():
    """æµ‹è¯•é…ç½®å’Œå‚æ•°ä¼ é€’"""
    print("\nâš™ï¸ æµ‹è¯•é…ç½®å’Œå‚æ•°ä¼ é€’...")
    
    from scripts.run_primevul_concurrent_optimized import create_optimized_config
    
    config = create_optimized_config()
    
    # æ£€æŸ¥å…³é”®é…ç½®
    enable_batch = config.get('enable_batch_processing', False)
    batch_size = config.get('llm_batch_size', 8)
    enable_concurrent = config.get('enable_concurrent', True)
    
    print(f"   ğŸ“Š é…ç½®æ£€æŸ¥:")
    print(f"      æ‰¹å¤„ç†å¯ç”¨: {enable_batch}")
    print(f"      æ‰¹å¤§å°: {batch_size}")
    print(f"      å¹¶å‘å¯ç”¨: {enable_concurrent}")
    
    if enable_batch and batch_size == 8 and enable_concurrent:
        print("   âœ… é…ç½®æ­£ç¡®")
        return True
    else:
        print("   âŒ é…ç½®æœ‰è¯¯")
        return False


def main():
    """è¿è¡Œæµ‹è¯•"""
    print("=== å¹¶å‘åŠŸèƒ½ä¿®å¤éªŒè¯ ===")
    print("éªŒè¯OpenAIå®¢æˆ·ç«¯ç°åœ¨æ”¯æŒå¹¶å‘æ‰¹å¤„ç†")
    print()
    
    tests = [
        ("é…ç½®å’Œå‚æ•°ä¼ é€’", test_config_and_params),
        ("OpenAIå®¢æˆ·ç«¯å¹¶å‘åŠŸèƒ½", test_openai_client_concurrent),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•: {test_name}")
        try:
            result = test_func()
            results[test_name] = result
        except Exception as e:
            print(f"   ğŸ’¥ æµ‹è¯•å¼‚å¸¸: {e}")
            results[test_name] = False
        print()
    
    # æ€»ç»“
    passed = sum(1 for r in results.values() if r)
    total = len(results)
    
    print("=== æµ‹è¯•æ€»ç»“ ===")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    print()
    print(f"æ€»ä½“ç»“æœ: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ ä¿®å¤æˆåŠŸï¼")
        print("âœ… OpenAIå®¢æˆ·ç«¯ç°åœ¨æ”¯æŒå¹¶å‘æ‰¹å¤„ç†")
        print("âœ… run_primevul_concurrent_optimized.py å°†ä½¿ç”¨å¹¶å‘æ¨¡å¼")
        print("\nğŸ“ ç°åœ¨åº”è¯¥çœ‹åˆ°çš„æ—¥å¿—ç‰¹å¾:")
        print("   â€¢ 'Using å¹¶å‘ batch processing' è€Œä¸æ˜¯ 'Using sequential'")
        print("   â€¢ 'ğŸš€ å¹¶å‘å¤„ç† X ä¸ªè¯·æ±‚'")
        print("   â€¢ 'ğŸ“Š å¹¶å‘è¿›åº¦: X/Y'")
        print("   â€¢ 'âœ… å¹¶å‘æ‰¹æ¬¡å®Œæˆ: X/Y æˆåŠŸ'")
        print("   â€¢ HTTPè¯·æ±‚æ—¶é—´é‡å è€Œä¸æ˜¯è¿ç»­")
        return 0
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤")
        return 1


if __name__ == "__main__":
    sys.exit(main())