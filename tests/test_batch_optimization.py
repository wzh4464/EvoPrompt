#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®æ”¹åçš„run_primevul_concurrent_optimized.pyä¸­çš„æ‰¹å¤„ç†åŠŸèƒ½
"""

import sys
import os
import tempfile
import json
from pathlib import Path

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, 'src')
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥scriptsæ¨¡å—
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_config_generation():
    """æµ‹è¯•é…ç½®ç”Ÿæˆæ˜¯å¦åŒ…å«æ‰¹å¤„ç†å‚æ•°"""
    print("ğŸ”§ æµ‹è¯•é…ç½®ç”Ÿæˆ...")
    
    # å¯¼å…¥å‡½æ•°
    from scripts.run_primevul_concurrent_optimized import create_optimized_config
    
    config = create_optimized_config()
    
    # æ£€æŸ¥æ‰¹å¤„ç†é…ç½®
    required_keys = [
        'llm_batch_size',
        'enable_batch_processing',
        'feedback_batch_size'
    ]
    
    missing_keys = []
    for key in required_keys:
        if key not in config:
            missing_keys.append(key)
    
    assert not missing_keys, f"ç¼ºå°‘æ‰¹å¤„ç†é…ç½®é”®: {missing_keys}"

    print(f"   âœ… æ‰¹å¤„ç†é…ç½®æ­£ç¡®:")
    print(f"      LLMæ‰¹å¤§å°: {config['llm_batch_size']}")
    print(f"      å¯ç”¨æ‰¹å¤„ç†: {config['enable_batch_processing']}")
    print(f"      åé¦ˆæ‰¹å¤§å°: {config['feedback_batch_size']}")


def test_llm_client_batch_support():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯æ˜¯å¦æ”¯æŒæ‰¹å¤„ç†"""
    print("ğŸ¤– æµ‹è¯•LLMå®¢æˆ·ç«¯æ‰¹å¤„ç†æ”¯æŒ...")

    from evoprompt.llm.client import create_default_client

    # åˆ›å»ºå®¢æˆ·ç«¯
    client = create_default_client()

    # æ£€æŸ¥æ˜¯å¦æœ‰batch_generateæ–¹æ³•
    assert hasattr(client, 'batch_generate'), "LLMå®¢æˆ·ç«¯ç¼ºå°‘batch_generateæ–¹æ³•"

    print("   âœ… LLMå®¢æˆ·ç«¯æ”¯æŒbatch_generateæ–¹æ³•")

    # æµ‹è¯•å°æ‰¹é‡è°ƒç”¨ï¼ˆä¸éœ€è¦çœŸå®APIï¼‰
    test_prompts = [
        "Test prompt 1: {input}",
        "Test prompt 2: {input}",
        "Test prompt 3: {input}"
    ]

    print("   ğŸ“ æ‰¹å¤„ç†æ–¹æ³•ç­¾åæ£€æŸ¥é€šè¿‡")


def test_function_signatures():
    """æµ‹è¯•å‡½æ•°ç­¾åæ˜¯å¦æ­£ç¡®æ›´æ–°"""
    print("ğŸ“‹ æµ‹è¯•å‡½æ•°ç­¾å...")

    # å¯¼å…¥ä¿®æ”¹åçš„å‡½æ•°
    from scripts.run_primevul_concurrent_optimized import (
        evaluate_on_dataset,
        sample_wise_feedback_training
    )

    import inspect

    # æ£€æŸ¥evaluate_on_datasetç­¾å
    sig = inspect.signature(evaluate_on_dataset)
    params = list(sig.parameters.keys())

    assert 'config' in params, "evaluate_on_datasetç¼ºå°‘configå‚æ•°"

    print("   âœ… evaluate_on_datasetç­¾åæ­£ç¡®")

    # æ£€æŸ¥sample_wise_feedback_trainingæ˜¯å¦èƒ½æ¥å—config
    sig = inspect.signature(sample_wise_feedback_training)
    params = list(sig.parameters.keys())

    assert 'config' in params, "sample_wise_feedback_trainingç¼ºå°‘configå‚æ•°"

    print("   âœ… sample_wise_feedback_trainingç­¾åæ­£ç¡®")


def test_batch_processing_logic():
    """æµ‹è¯•æ‰¹å¤„ç†é€»è¾‘"""
    print("âš¡ æµ‹è¯•æ‰¹å¤„ç†é€»è¾‘...")

    # æ¨¡æ‹Ÿé…ç½®
    config = {
        'enable_batch_processing': True,
        'llm_batch_size': 8,
        'feedback_batch_size': 10
    }

    # æµ‹è¯•æ‰¹å¤„ç†å‚æ•°æå–
    enable_batch = config.get('enable_batch_processing', False)
    llm_batch_size = config.get('llm_batch_size', 8)
    feedback_batch_size = config.get('feedback_batch_size', 10)

    assert enable_batch, "æ‰¹å¤„ç†æœªå¯ç”¨"
    assert llm_batch_size == 8, f"LLMæ‰¹å¤§å°é”™è¯¯: {llm_batch_size} != 8"
    assert feedback_batch_size == 10, f"åé¦ˆæ‰¹å¤§å°é”™è¯¯: {feedback_batch_size} != 10"

    print("   âœ… æ‰¹å¤„ç†å‚æ•°æå–æ­£ç¡®")
    print(f"      æ‰¹å¤„ç†å¯ç”¨: {enable_batch}")
    print(f"      LLMæ‰¹å¤§å°: {llm_batch_size}")
    print(f"      åé¦ˆæ‰¹å¤§å°: {feedback_batch_size}")


def test_import_and_basic_functionality():
    """æµ‹è¯•å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    print("ğŸ“¦ æµ‹è¯•å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½...")

    # æµ‹è¯•ä¸»è¦å‡½æ•°å¯¼å…¥
    from scripts.run_primevul_concurrent_optimized import (
        create_optimized_config,
        run_concurrent_evolution_with_feedback,
        evaluate_on_dataset,
        sample_wise_feedback_training,
        main
    )

    print("   âœ… æ‰€æœ‰ä¸»è¦å‡½æ•°å¯¼å…¥æˆåŠŸ")

    # æµ‹è¯•é…ç½®åˆ›å»º
    config = create_optimized_config()
    assert isinstance(config, dict), "é…ç½®åˆ›å»ºå¤±è´¥"

    print("   âœ… é…ç½®åˆ›å»ºæˆåŠŸ")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=== æ‰¹å¤„ç†ä¼˜åŒ–æµ‹è¯• ===")
    print()
    
    tests = [
        ("å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½", test_import_and_basic_functionality),
        ("é…ç½®ç”Ÿæˆ", test_config_generation),
        ("LLMå®¢æˆ·ç«¯æ‰¹å¤„ç†æ”¯æŒ", test_llm_client_batch_support),
        ("å‡½æ•°ç­¾å", test_function_signatures),
        ("æ‰¹å¤„ç†é€»è¾‘", test_batch_processing_logic),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•: {test_name}")
        try:
            result = test_func()
            results[test_name] = result
            if result:
                print(f"   âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"   âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"   ğŸ’¥ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results[test_name] = False
        print()
    
    # æ€»ç»“
    passed = sum(1 for r in results.values() if r)
    total = len(results)
    
    print("=== æµ‹è¯•æ€»ç»“ ===")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    print()
    print(f"æ€»ä½“ç»“æœ: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ‰¹å¤„ç†ä¼˜åŒ–å·²æˆåŠŸé›†æˆã€‚")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())