#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®æ”¹åçš„run_primevul_concurrent_optimized.pyä¸­çš„æ‰¹å¤„ç†åŠŸèƒ½
"""

import sys
import os
import tempfile
import json
from pathlib import Path

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, 'src')
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥scriptsæ¨¡å—
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_config_generation():
    """æµ‹è¯•é…ç½®ç”Ÿæˆæ˜¯å¦åŒ…å«æ‰¹å¤„ç†å‚æ•°"""
    print("ğŸ”§ æµ‹è¯•é…ç½®ç”Ÿæˆ...")
    
    # å¯¼å…¥å‡½æ•°
    from scripts.run_primevul_concurrent_optimized import create_optimized_config
    
    config = create_optimized_config()
    
    # æ£€æŸ¥æ‰¹å¤„ç†é…ç½®
    required_keys = [
        'llm_batch_size',
        'enable_batch_processing',
        'feedback_batch_size'
    ]
    
    missing_keys = []
    for key in required_keys:
        if key not in config:
            missing_keys.append(key)
    
    if missing_keys:
        print(f"   âŒ ç¼ºå°‘æ‰¹å¤„ç†é…ç½®é”®: {missing_keys}")
        return False
    
    print(f"   âœ… æ‰¹å¤„ç†é…ç½®æ­£ç¡®:")
    print(f"      LLMæ‰¹å¤§å°: {config['llm_batch_size']}")
    print(f"      å¯ç”¨æ‰¹å¤„ç†: {config['enable_batch_processing']}")
    print(f"      åé¦ˆæ‰¹å¤§å°: {config['feedback_batch_size']}")
    
    return True


def test_llm_client_batch_support():
    """æµ‹è¯•LLMå®¢æˆ·ç«¯æ˜¯å¦æ”¯æŒæ‰¹å¤„ç†"""
    print("ğŸ¤– æµ‹è¯•LLMå®¢æˆ·ç«¯æ‰¹å¤„ç†æ”¯æŒ...")
    
    try:
        from evoprompt.llm.client import create_default_client
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = create_default_client()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰batch_generateæ–¹æ³•
        if not hasattr(client, 'batch_generate'):
            print("   âŒ LLMå®¢æˆ·ç«¯ç¼ºå°‘batch_generateæ–¹æ³•")
            return False
        
        print("   âœ… LLMå®¢æˆ·ç«¯æ”¯æŒbatch_generateæ–¹æ³•")
        
        # æµ‹è¯•å°æ‰¹é‡è°ƒç”¨ï¼ˆä¸éœ€è¦çœŸå®APIï¼‰
        test_prompts = [
            "Test prompt 1: {input}",
            "Test prompt 2: {input}",
            "Test prompt 3: {input}"
        ]
        
        print("   ğŸ“ æ‰¹å¤„ç†æ–¹æ³•ç­¾åæ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ LLMå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_function_signatures():
    """æµ‹è¯•å‡½æ•°ç­¾åæ˜¯å¦æ­£ç¡®æ›´æ–°"""
    print("ğŸ“‹ æµ‹è¯•å‡½æ•°ç­¾å...")
    
    try:
        # å¯¼å…¥ä¿®æ”¹åçš„å‡½æ•°
        from scripts.run_primevul_concurrent_optimized import (
            evaluate_on_dataset,
            sample_wise_feedback_training
        )
        
        import inspect
        
        # æ£€æŸ¥evaluate_on_datasetç­¾å
        sig = inspect.signature(evaluate_on_dataset)
        params = list(sig.parameters.keys())
        
        if 'config' not in params:
            print("   âŒ evaluate_on_datasetç¼ºå°‘configå‚æ•°")
            return False
        
        print("   âœ… evaluate_on_datasetç­¾åæ­£ç¡®")
        
        # æ£€æŸ¥sample_wise_feedback_trainingæ˜¯å¦èƒ½æ¥å—config
        sig = inspect.signature(sample_wise_feedback_training)
        params = list(sig.parameters.keys())
        
        if 'config' not in params:
            print("   âŒ sample_wise_feedback_trainingç¼ºå°‘configå‚æ•°")
            return False
        
        print("   âœ… sample_wise_feedback_trainingç­¾åæ­£ç¡®")
        
        return True
        
    except Exception as e:
        print(f"   âŒ å‡½æ•°ç­¾åæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_batch_processing_logic():
    """æµ‹è¯•æ‰¹å¤„ç†é€»è¾‘"""
    print("âš¡ æµ‹è¯•æ‰¹å¤„ç†é€»è¾‘...")
    
    try:
        # æ¨¡æ‹Ÿé…ç½®
        config = {
            'enable_batch_processing': True,
            'llm_batch_size': 8,
            'feedback_batch_size': 10
        }
        
        # æµ‹è¯•æ‰¹å¤„ç†å‚æ•°æå–
        enable_batch = config.get('enable_batch_processing', False)
        llm_batch_size = config.get('llm_batch_size', 8)
        feedback_batch_size = config.get('feedback_batch_size', 10)
        
        if not enable_batch:
            print("   âŒ æ‰¹å¤„ç†æœªå¯ç”¨")
            return False
            
        if llm_batch_size != 8:
            print(f"   âŒ LLMæ‰¹å¤§å°é”™è¯¯: {llm_batch_size} != 8")
            return False
            
        if feedback_batch_size != 10:
            print(f"   âŒ åé¦ˆæ‰¹å¤§å°é”™è¯¯: {feedback_batch_size} != 10")
            return False
        
        print("   âœ… æ‰¹å¤„ç†å‚æ•°æå–æ­£ç¡®")
        print(f"      æ‰¹å¤„ç†å¯ç”¨: {enable_batch}")
        print(f"      LLMæ‰¹å¤§å°: {llm_batch_size}")
        print(f"      åé¦ˆæ‰¹å¤§å°: {feedback_batch_size}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ‰¹å¤„ç†é€»è¾‘æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_import_and_basic_functionality():
    """æµ‹è¯•å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    print("ğŸ“¦ æµ‹è¯•å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½...")
    
    try:
        # æµ‹è¯•ä¸»è¦å‡½æ•°å¯¼å…¥
        from scripts.run_primevul_concurrent_optimized import (
            create_optimized_config,
            run_concurrent_evolution_with_feedback,
            evaluate_on_dataset,
            sample_wise_feedback_training,
            main
        )
        
        print("   âœ… æ‰€æœ‰ä¸»è¦å‡½æ•°å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•é…ç½®åˆ›å»º
        config = create_optimized_config()
        if not isinstance(config, dict):
            print("   âŒ é…ç½®åˆ›å»ºå¤±è´¥")
            return False
            
        print("   âœ… é…ç½®åˆ›å»ºæˆåŠŸ")
        
        return True
        
    except ImportError as e:
        print(f"   âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"   âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=== æ‰¹å¤„ç†ä¼˜åŒ–æµ‹è¯• ===")
    print()
    
    tests = [
        ("å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½", test_import_and_basic_functionality),
        ("é…ç½®ç”Ÿæˆ", test_config_generation),
        ("LLMå®¢æˆ·ç«¯æ‰¹å¤„ç†æ”¯æŒ", test_llm_client_batch_support),
        ("å‡½æ•°ç­¾å", test_function_signatures),
        ("æ‰¹å¤„ç†é€»è¾‘", test_batch_processing_logic),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•: {test_name}")
        try:
            result = test_func()
            results[test_name] = result
            if result:
                print(f"   âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"   âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"   ğŸ’¥ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results[test_name] = False
        print()
    
    # æ€»ç»“
    passed = sum(1 for r in results.values() if r)
    total = len(results)
    
    print("=== æµ‹è¯•æ€»ç»“ ===")
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    print()
    print(f"æ€»ä½“ç»“æœ: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ‰¹å¤„ç†ä¼˜åŒ–å·²æˆåŠŸé›†æˆã€‚")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())