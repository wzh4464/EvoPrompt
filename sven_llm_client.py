#!/usr/bin/env python3
"""
SVEN-style LLM Client for EvoPrompt
åŸºäºSVENçš„APIè°ƒç”¨æ–¹å¼ï¼Œæ”¯æŒå¤šç§API baseå’Œç¯å¢ƒé…ç½®
"""

import json
import os
import requests
import time
from typing import List, Dict, Optional, Union
from pathlib import Path


def load_env_vars():
    """Load environment variables from .env file"""
    env_path = Path(__file__).parent / '.env'
    if env_path.exists():
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()


# Load environment variables
load_env_vars()

# API Configuration from environment
API_BASE = os.getenv("API_BASE_URL", "https://newapi.pockgo.com/v1")
API_KEY = os.getenv("API_KEY", "")
BACKUP_API_BASE = os.getenv("BACKUP_API_BASE_URL", "https://newapi.aicohere.org/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "kimi-k2-0711-preview")


class SVENLLMClient:
    """SVENé£æ ¼çš„LLMå®¢æˆ·ç«¯"""
    
    def __init__(self, api_base: str = None, api_key: str = None, model_name: str = None):
        self.api_base = api_base or API_BASE
        self.api_key = api_key or API_KEY
        self.model_name = model_name or MODEL_NAME
        self.backup_api_base = BACKUP_API_BASE
        
        if not self.api_key:
            raise ValueError("API_KEY not found. Please set it in .env file or environment variable.")
        
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        })
    
    def _make_request(self, messages: List[Dict], temperature: float = 0.1, max_tokens: int = None) -> str:
        """å‘é€APIè¯·æ±‚"""
        data = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature
        }
        if max_tokens is not None:
            data["max_tokens"] = max_tokens
        
        # å°è¯•ä¸»APIï¼Œå¤±è´¥åˆ™å°è¯•å¤‡ç”¨API
        apis_to_try = [self.api_base, self.backup_api_base]
        
        for api_base in apis_to_try:
            try:
                response = self.session.post(
                    f"{api_base}/chat/completions",
                    json=data,
                    timeout=300
                )
                response.raise_for_status()
                
                result = response.json()
                return result['choices'][0]['message']['content'].strip()
                
            except Exception as e:
                print(f"API call failed for {api_base}: {e}")
                if api_base == apis_to_try[-1]:  # æœ€åä¸€ä¸ªAPIä¹Ÿå¤±è´¥äº†
                    raise Exception(f"All API endpoints failed. Last error: {e}")
                continue  # å°è¯•ä¸‹ä¸€ä¸ªAPI
        
        raise Exception("No API endpoints available")
    
    def query_single(self, prompt: str, temperature: float = 0.1, max_tokens: int = None) -> str:
        """å•æ¬¡æŸ¥è¯¢"""
        messages = [{"role": "user", "content": prompt}]
        return self._make_request(messages, temperature, max_tokens)
    
    def query_batch(self, prompts: List[str], temperature: float = 0.1, max_tokens: int = None, 
                   delay: float = 0.1, batch_size: int = 8, concurrent: bool = False) -> List[str]:
        """æ‰¹é‡æŸ¥è¯¢ï¼Œæ”¯æŒåˆ†æ‰¹å¤„ç†å’Œå¹¶å‘é€‰é¡¹"""
        results = []
        total_prompts = len(prompts)
        
        # æŒ‰batch_sizeåˆ†ç»„å¤„ç†
        for batch_start in range(0, total_prompts, batch_size):
            batch_end = min(batch_start + batch_size, total_prompts)
            batch_prompts = prompts[batch_start:batch_end]
            
            print(f"Processing batch {batch_start//batch_size + 1}/{(total_prompts + batch_size - 1)//batch_size}: prompts {batch_start+1}-{batch_end}")
            
            if concurrent and len(batch_prompts) > 1:
                # å¹¶å‘å¤„ç†æ¨¡å¼
                print(f"  ğŸš€ å¹¶å‘å¤„ç† {len(batch_prompts)} ä¸ªè¯·æ±‚")
                batch_results = self._process_batch_concurrent(batch_prompts, temperature, max_tokens)
            else:
                # é¡ºåºå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                print(f"  ğŸ”„ é¡ºåºå¤„ç† {len(batch_prompts)} ä¸ªè¯·æ±‚")
                batch_results = []
                for i, prompt in enumerate(batch_prompts):
                    try:
                        result = self.query_single(prompt, temperature, max_tokens)
                        batch_results.append(result)
                        
                        # è¿›åº¦æ˜¾ç¤º
                        global_idx = batch_start + i + 1
                        if global_idx % 10 == 0:
                            print(f"Processed {global_idx}/{total_prompts} queries")
                        
                        # é€Ÿç‡é™åˆ¶
                        if delay > 0:
                            time.sleep(delay)
                            
                    except Exception as e:
                        print(f"Query {batch_start + i + 1} failed: {e}")
                        batch_results.append("error")
            
            results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´çš„çŸ­æš‚ä¼‘æ¯
            if batch_end < total_prompts and delay > 0:
                print(f"Batch {batch_start//batch_size + 1} completed, brief rest...")
                time.sleep(delay * 2)
        
        return results
    
    def _process_batch_concurrent(self, prompts: List[str], temperature: float, max_tokens: int) -> List[str]:
        """å¹¶å‘å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„è¯·æ±‚"""
        import threading
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def query_with_retry(prompt):
            """å¸¦é‡è¯•çš„å•æ¬¡æŸ¥è¯¢"""
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    return self.query_single(prompt, temperature, max_tokens)
                except Exception as e:
                    if attempt == max_retries - 1:
                        print(f"    âš ï¸ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        return "error"
                    time.sleep(0.5 * (attempt + 1))  # é€’å¢å»¶æ—¶
        
        results = ["error"] * len(prompts)  # é¢„åˆ†é…ç»“æœæ•°ç»„
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=min(len(prompts), 8)) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(query_with_retry, prompt): i 
                for i, prompt in enumerate(prompts)
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                    results[index] = result
                except Exception as e:
                    print(f"    âŒ å¹¶å‘è¯·æ±‚ {index + 1} å¼‚å¸¸: {e}")
                    results[index] = "error"
                
                completed += 1
                if completed % 4 == 0 or completed == len(prompts):
                    print(f"    ğŸ“Š å¹¶å‘è¿›åº¦: {completed}/{len(prompts)}")
        
        success_count = sum(1 for r in results if r != "error")
        print(f"    âœ… å¹¶å‘æ‰¹æ¬¡å®Œæˆ: {success_count}/{len(prompts)} æˆåŠŸ")
        
        return results
    
    def paraphrase(self, sentence: Union[str, List[str]], temperature: float = 0.7) -> Union[str, List[str]]:
        """é‡æ–°è¡¨è¿°å¥å­"""
        if isinstance(sentence, list):
            prompts = [
                f"Generate a variation of the following instruction while keeping the semantic meaning.\nInput: {s}\nOutput:"
                for s in sentence
            ]
            return self.query_batch(prompts, temperature=temperature)
        else:
            prompt = f"Generate a variation of the following instruction while keeping the semantic meaning.\nInput: {sentence}\nOutput:"
            return self.query_single(prompt, temperature=temperature)


def sven_llm_init(api_base: str = None, api_key: str = None, model_name: str = None) -> SVENLLMClient:
    """åˆå§‹åŒ–SVENé£æ ¼çš„LLMå®¢æˆ·ç«¯"""
    return SVENLLMClient(api_base, api_key, model_name)


def sven_llm_query(data: Union[str, List[str]], client: SVENLLMClient, task: bool = False, 
                  temperature: float = 0.1, batch_size: int = 8, concurrent: bool = False, **kwargs) -> Union[str, List[str]]:
    """
    SVENé£æ ¼çš„LLMæŸ¥è¯¢å‡½æ•°
    
    Args:
        data: å•ä¸ªæç¤ºæˆ–æç¤ºåˆ—è¡¨
        client: SVEN LLMå®¢æˆ·ç«¯
        task: æ˜¯å¦ä¸ºä»»åŠ¡å¯¼å‘ï¼ˆä¼šæˆªæ–­å¤šæ®µå›ç­”ï¼‰
        temperature: æ¸©åº¦å‚æ•°
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º8
        concurrent: æ˜¯å¦åœ¨æ‰¹æ¬¡å†…å¹¶å‘å¤„ç†ï¼Œé»˜è®¤False
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        å•ä¸ªå›ç­”æˆ–å›ç­”åˆ—è¡¨
    """
    if isinstance(data, list):
        results = client.query_batch(data, temperature=temperature, batch_size=batch_size, concurrent=concurrent, **kwargs)
        if task:
            # ä»»åŠ¡å¯¼å‘ï¼Œåªå–ç¬¬ä¸€æ®µ
            results = [str(r).strip().split("\n\n")[0] for r in results]
        return results
    else:
        result = client.query_single(data, temperature=temperature, **kwargs)
        if task:
            result = result.split("\n\n")[0]
        return result


# å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒä¸åŸEvoPromptæ¥å£ä¸€è‡´
def llm_init(**kwargs) -> SVENLLMClient:
    """å…¼å®¹æ€§å‡½æ•°"""
    return sven_llm_init()


def llm_query(data, client, type=None, task=False, batch_size=8, concurrent=True, **config):
    """å…¼å®¹æ€§å‡½æ•°"""
    return sven_llm_query(data, client, task=task, batch_size=batch_size, concurrent=concurrent, **config)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("Testing SVEN LLM Client...")
    
    try:
        client = sven_llm_init()
        
        # å•æ¬¡æŸ¥è¯¢æµ‹è¯•
        test_prompt = "Explain what a buffer overflow vulnerability is in one sentence."
        result = sven_llm_query(test_prompt, client)
        print(f"Single query result: {result}")
        
        # æ‰¹é‡æŸ¥è¯¢æµ‹è¯•
        test_prompts = [
            "What is SQL injection?",
            "What is XSS?",
            "What is CSRF?"
        ]
        results = sven_llm_query(test_prompts, client)
        print(f"Batch query results: {results}")
        
        print("SVEN LLM Client test completed successfully!")
        
    except Exception as e:
        print(f"Test failed: {e}")