"""å¤šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡ - ä¸“ä¸ºä¸‰å±‚æ¼æ´æ£€æµ‹è®¾è®¡

æ”¯æŒä¸‰ç§F1è®¡ç®—æ–¹å¼:
1. Macro-F1: æ‰€æœ‰ç±»åˆ«åŒç­‰é‡è¦ (æ¨èç”¨äºä¸å¹³è¡¡æ•°æ®)
2. Weighted-F1: æŒ‰æ ·æœ¬æ•°é‡åŠ æƒ
3. Micro-F1: å…¨å±€è®¡ç®— (ç­‰åŒäºaccuracy)

é€‚ç”¨åœºæ™¯: æ¼æ´æ£€æµ‹ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
"""

from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import json


@dataclass
class ClassMetrics:
    """å•ä¸ªç±»åˆ«çš„è¯„ä¼°æŒ‡æ ‡"""

    class_name: str
    tp: int = 0  # True Positives
    fp: int = 0  # False Positives
    tn: int = 0  # True Negatives
    fn: int = 0  # False Negatives
    support: int = 0  # è¯¥ç±»åˆ«çš„å®é™…æ ·æœ¬æ•°

    @property
    def precision(self) -> float:
        """ç²¾ç¡®ç‡ = TP / (TP + FP)"""
        if self.tp + self.fp == 0:
            return 0.0
        return self.tp / (self.tp + self.fp)

    @property
    def recall(self) -> float:
        """å¬å›ç‡ = TP / (TP + FN)"""
        if self.tp + self.fn == 0:
            return 0.0
        return self.tp / (self.tp + self.fn)

    @property
    def f1_score(self) -> float:
        """F1åˆ†æ•° = 2 * (precision * recall) / (precision + recall)"""
        if self.precision + self.recall == 0:
            return 0.0
        return 2 * (self.precision * self.recall) / (self.precision + self.recall)

    @property
    def accuracy(self) -> float:
        """å‡†ç¡®ç‡ = (TP + TN) / (TP + TN + FP + FN)"""
        total = self.tp + self.tn + self.fp + self.fn
        if total == 0:
            return 0.0
        return (self.tp + self.tn) / total

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "class": self.class_name,
            "support": self.support,
            "precision": round(self.precision, 4),
            "recall": round(self.recall, 4),
            "f1_score": round(self.f1_score, 4),
            "accuracy": round(self.accuracy, 4),
            "tp": self.tp,
            "fp": self.fp,
            "tn": self.tn,
            "fn": self.fn,
        }


@dataclass
class MultiClassMetrics:
    """å¤šåˆ†ç±»è¯„ä¼°æŒ‡æ ‡

    æ”¯æŒä¸‰ç§F1è®¡ç®—æ–¹å¼:
    - Macro-F1: æ‰€æœ‰ç±»åˆ«å¹³ç­‰å¯¹å¾… (æ¨èç”¨äºæ¼æ´æ£€æµ‹)
    - Weighted-F1: æŒ‰æ ·æœ¬æ•°åŠ æƒ
    - Micro-F1: å…¨å±€è®¡ç®—
    """

    # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    class_metrics: Dict[str, ClassMetrics] = field(default_factory=dict)

    # æ··æ·†çŸ©é˜µ: predicted -> actual -> count
    confusion_matrix: Dict[str, Dict[str, int]] = field(
        default_factory=lambda: defaultdict(lambda: defaultdict(int))
    )

    # æ€»æ ·æœ¬æ•°
    total_samples: int = 0

    # æ­£ç¡®é¢„æµ‹æ•°
    correct_predictions: int = 0

    def add_prediction(self, predicted: str, actual: str):
        """æ·»åŠ ä¸€ä¸ªé¢„æµ‹ç»“æœ

        Args:
            predicted: é¢„æµ‹çš„ç±»åˆ«
            actual: å®é™…çš„ç±»åˆ«
        """
        self.total_samples += 1

        # æ›´æ–°æ··æ·†çŸ©é˜µ
        self.confusion_matrix[predicted][actual] += 1

        # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹
        if predicted == actual:
            self.correct_predictions += 1

        # åˆå§‹åŒ–ç±»åˆ«æŒ‡æ ‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if predicted not in self.class_metrics:
            self.class_metrics[predicted] = ClassMetrics(class_name=predicted)
        if actual not in self.class_metrics:
            self.class_metrics[actual] = ClassMetrics(class_name=actual)

        # æ›´æ–°æ¯ä¸ªç±»åˆ«çš„TP/FP/TN/FN
        # æ³¨æ„: åœ¨å¤šåˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨One-vs-Restç­–ç•¥
        for class_name in set(list(self.class_metrics.keys())):
            metrics = self.class_metrics[class_name]

            # å®é™…æ˜¯è¯¥ç±» + é¢„æµ‹ä¹Ÿæ˜¯è¯¥ç±» = TP
            if actual == class_name and predicted == class_name:
                metrics.tp += 1
            # å®é™…ä¸æ˜¯è¯¥ç±» + é¢„æµ‹æ˜¯è¯¥ç±» = FP
            elif actual != class_name and predicted == class_name:
                metrics.fp += 1
            # å®é™…æ˜¯è¯¥ç±» + é¢„æµ‹ä¸æ˜¯è¯¥ç±» = FN
            elif actual == class_name and predicted != class_name:
                metrics.fn += 1
            # å®é™…ä¸æ˜¯è¯¥ç±» + é¢„æµ‹ä¹Ÿä¸æ˜¯è¯¥ç±» = TN
            else:  # actual != class_name and predicted != class_name
                metrics.tn += 1

            # æ›´æ–°support (è¯¥ç±»çš„å®é™…æ ·æœ¬æ•°)
            if actual == class_name:
                metrics.support += 1

    def compute_macro_f1(self) -> float:
        """è®¡ç®—Macro-F1 (å®å¹³å‡)

        æ‰€æœ‰ç±»åˆ«åŒç­‰é‡è¦ï¼Œä¸è€ƒè™‘æ ·æœ¬æ•°é‡
        æ¨èç”¨äºä¸å¹³è¡¡æ•°æ®é›† (å¦‚æ¼æ´æ£€æµ‹)

        Formula: mean(F1_i for all classes i)
        """
        if not self.class_metrics:
            return 0.0

        f1_scores = [m.f1_score for m in self.class_metrics.values()]
        return sum(f1_scores) / len(f1_scores)

    def compute_weighted_f1(self) -> float:
        """è®¡ç®—Weighted-F1 (åŠ æƒå¹³å‡)

        æŒ‰æ ·æœ¬æ•°é‡åŠ æƒï¼Œæ ·æœ¬å¤šçš„ç±»åˆ«æƒé‡å¤§

        Formula: sum(F1_i * support_i) / total_samples
        """
        if self.total_samples == 0:
            return 0.0

        weighted_sum = sum(
            m.f1_score * m.support
            for m in self.class_metrics.values()
        )
        return weighted_sum / self.total_samples

    def compute_micro_f1(self) -> float:
        """è®¡ç®—Micro-F1 (å¾®å¹³å‡)

        å…¨å±€è®¡ç®—TP/FP/FNï¼Œåœ¨å¤šåˆ†ç±»ä¸­ç­‰åŒäºaccuracy

        Formula: 2 * (P_micro * R_micro) / (P_micro + R_micro)
        """
        # å…¨å±€TP = æ­£ç¡®é¢„æµ‹æ•°
        global_tp = self.correct_predictions

        # å…¨å±€FP + FN = é”™è¯¯é¢„æµ‹æ•°
        global_fp_fn = self.total_samples - self.correct_predictions

        if global_tp + global_fp_fn == 0:
            return 0.0

        # Micro-F1 = accuracy (åœ¨å¤šåˆ†ç±»ä¸­)
        return global_tp / self.total_samples

    def compute_macro_precision(self) -> float:
        """è®¡ç®—Macro-Precision (å®å¹³å‡ç²¾ç¡®ç‡)"""
        if not self.class_metrics:
            return 0.0

        precisions = [m.precision for m in self.class_metrics.values()]
        return sum(precisions) / len(precisions)

    def compute_macro_recall(self) -> float:
        """è®¡ç®—Macro-Recall (å®å¹³å‡å¬å›ç‡)"""
        if not self.class_metrics:
            return 0.0

        recalls = [m.recall for m in self.class_metrics.values()]
        return sum(recalls) / len(recalls)

    def compute_weighted_precision(self) -> float:
        """è®¡ç®—Weighted-Precision (åŠ æƒç²¾ç¡®ç‡)"""
        if self.total_samples == 0:
            return 0.0

        weighted_sum = sum(
            m.precision * m.support
            for m in self.class_metrics.values()
        )
        return weighted_sum / self.total_samples

    def compute_weighted_recall(self) -> float:
        """è®¡ç®—Weighted-Recall (åŠ æƒå¬å›ç‡)"""
        if self.total_samples == 0:
            return 0.0

        weighted_sum = sum(
            m.recall * m.support
            for m in self.class_metrics.values()
        )
        return weighted_sum / self.total_samples

    @property
    def accuracy(self) -> float:
        """æ€»ä½“å‡†ç¡®ç‡"""
        if self.total_samples == 0:
            return 0.0
        return self.correct_predictions / self.total_samples

    def get_per_class_metrics(self) -> Dict[str, Dict]:
        """è·å–æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡"""
        return {
            class_name: metrics.to_dict()
            for class_name, metrics in self.class_metrics.items()
        }

    def get_classification_report(self) -> Dict:
        """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š (ç±»ä¼¼sklearn.metrics.classification_report)

        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        report = {
            "per_class_metrics": self.get_per_class_metrics(),
            "macro_avg": {
                "precision": round(self.compute_macro_precision(), 4),
                "recall": round(self.compute_macro_recall(), 4),
                "f1_score": round(self.compute_macro_f1(), 4),
                "support": self.total_samples,
            },
            "weighted_avg": {
                "precision": round(self.compute_weighted_precision(), 4),
                "recall": round(self.compute_weighted_recall(), 4),
                "f1_score": round(self.compute_weighted_f1(), 4),
                "support": self.total_samples,
            },
            "micro_avg": {
                "precision": round(self.accuracy, 4),  # Micro = Accuracy
                "recall": round(self.accuracy, 4),
                "f1_score": round(self.compute_micro_f1(), 4),
                "support": self.total_samples,
            },
            "overall": {
                "accuracy": round(self.accuracy, 4),
                "total_samples": self.total_samples,
                "correct_predictions": self.correct_predictions,
            }
        }

        return report

    def print_report(self, layer_name: str = ""):
        """æ‰“å°æ ¼å¼åŒ–çš„åˆ†ç±»æŠ¥å‘Š

        Args:
            layer_name: å±‚çº§åç§° (å¦‚ "Layer 1", "Layer 2")
        """
        report = self.get_classification_report()

        header = f"Classification Report"
        if layer_name:
            header += f" - {layer_name}"

        print("\n" + "=" * 70)
        print(header)
        print("=" * 70)

        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        print(f"\n{'Class':<20} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}")
        print("-" * 70)

        for class_name, metrics in sorted(report["per_class_metrics"].items()):
            print(
                f"{class_name:<20} "
                f"{metrics['precision']:>10.4f} "
                f"{metrics['recall']:>10.4f} "
                f"{metrics['f1_score']:>10.4f} "
                f"{metrics['support']:>10}"
            )

        print("-" * 70)

        # æ‰“å°æ±‡æ€»æŒ‡æ ‡
        print(f"\n{'Metric':<20} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}")
        print("-" * 70)

        for avg_type in ["macro_avg", "weighted_avg", "micro_avg"]:
            avg_name = avg_type.replace("_", " ").title()
            avg = report[avg_type]
            print(
                f"{avg_name:<20} "
                f"{avg['precision']:>10.4f} "
                f"{avg['recall']:>10.4f} "
                f"{avg['f1_score']:>10.4f} "
                f"{avg['support']:>10}"
            )

        print("-" * 70)
        print(f"\nAccuracy: {report['overall']['accuracy']:.4f}")
        print(f"Total Samples: {report['overall']['total_samples']}")
        print()

    def get_confusion_matrix_dict(self) -> Dict:
        """è·å–æ··æ·†çŸ©é˜µ (å¯åºåˆ—åŒ–æ ¼å¼)"""
        return {
            pred: dict(actual_counts)
            for pred, actual_counts in self.confusion_matrix.items()
        }

    def print_confusion_matrix(self):
        """æ‰“å°æ··æ·†çŸ©é˜µ"""
        if not self.confusion_matrix:
            print("No predictions recorded.")
            return

        # è·å–æ‰€æœ‰ç±»åˆ«
        all_classes = sorted(set(
            list(self.confusion_matrix.keys()) +
            [actual for counts in self.confusion_matrix.values() for actual in counts.keys()]
        ))

        print("\n" + "=" * 70)
        print("Confusion Matrix")
        print("=" * 70)
        print("\nRows: Predicted, Columns: Actual\n")

        # æ‰“å°è¡¨å¤´
        header = f"{'Predicted':<15}"
        for cls in all_classes:
            header += f"{cls:>12}"
        print(header)
        print("-" * len(header))

        # æ‰“å°æ¯ä¸€è¡Œ
        for pred_class in all_classes:
            row = f"{pred_class:<15}"
            for actual_class in all_classes:
                count = self.confusion_matrix.get(pred_class, {}).get(actual_class, 0)
                row += f"{count:>12}"
            print(row)

        print()


def compute_layered_metrics(
    predictions: List[Tuple[str, str]],
    layer_name: str = ""
) -> MultiClassMetrics:
    """è®¡ç®—å±‚çº§åŒ–çš„å¤šåˆ†ç±»æŒ‡æ ‡

    Args:
        predictions: List of (predicted, actual) tuples
        layer_name: å±‚çº§åç§° (ç”¨äºæ‰“å°)

    Returns:
        MultiClassMetricså¯¹è±¡
    """
    metrics = MultiClassMetrics()

    for predicted, actual in predictions:
        metrics.add_prediction(predicted, actual)

    return metrics


def compare_averaging_methods(metrics: MultiClassMetrics) -> Dict:
    """å¯¹æ¯”ä¸‰ç§F1è®¡ç®—æ–¹æ³•

    Args:
        metrics: MultiClassMetricså¯¹è±¡

    Returns:
        åŒ…å«ä¸‰ç§æ–¹æ³•çš„å¯¹æ¯”ç»“æœ
    """
    comparison = {
        "macro_f1": {
            "value": round(metrics.compute_macro_f1(), 4),
            "description": "æ‰€æœ‰ç±»åˆ«åŒç­‰é‡è¦ (æ¨èç”¨äºä¸å¹³è¡¡æ•°æ®)",
            "formula": "mean(F1_i for all classes)",
        },
        "weighted_f1": {
            "value": round(metrics.compute_weighted_f1(), 4),
            "description": "æŒ‰æ ·æœ¬æ•°é‡åŠ æƒ (æ ·æœ¬å¤šçš„ç±»åˆ«æƒé‡å¤§)",
            "formula": "sum(F1_i * support_i) / total_samples",
        },
        "micro_f1": {
            "value": round(metrics.compute_micro_f1(), 4),
            "description": "å…¨å±€è®¡ç®— (ç­‰åŒäºaccuracy)",
            "formula": "global_TP / total_samples",
        },
        "accuracy": {
            "value": round(metrics.accuracy, 4),
            "description": "æ€»ä½“å‡†ç¡®ç‡",
        }
    }

    return comparison


def print_averaging_comparison(metrics: MultiClassMetrics):
    """æ‰“å°ä¸‰ç§F1æ–¹æ³•çš„å¯¹æ¯”"""
    comparison = compare_averaging_methods(metrics)

    print("\n" + "=" * 70)
    print("F1 Averaging Methods Comparison")
    print("=" * 70)

    for method, info in comparison.items():
        print(f"\n{method.upper().replace('_', '-')}:")
        print(f"  Value: {info['value']:.4f}")
        print(f"  Description: {info['description']}")
        if 'formula' in info:
            print(f"  Formula: {info['formula']}")

    print("\n" + "=" * 70)
    print("ğŸ’¡ æ¨èç”¨äºæ¼æ´æ£€æµ‹: Macro-F1")
    print("   åŸå› : å¼ºåˆ¶æ¨¡å‹åœ¨æ‰€æœ‰ç±»åˆ«(åŒ…æ‹¬å°‘æ•°ç±»)ä¸Šéƒ½è¡¨ç°å¥½")
    print("=" * 70)
