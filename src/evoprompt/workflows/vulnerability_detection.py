"""Complete workflow for vulnerability detection using EvoPrompt."""

import os
import json
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

from ..core.evolution import EvolutionEngine
from ..core.evaluator import Evaluator, EvaluationResult
from ..core.prompt_tracker import PromptTracker, EvolutionLogger
from ..data.dataset import create_dataset, prepare_primevul_data
from ..algorithms.differential import DifferentialEvolution
from ..algorithms.genetic import GeneticAlgorithm
from ..llm.client import create_llm_client
from ..metrics.base import AccuracyMetric
from ..data.cwe_categories import CWE_MAJOR_CATEGORIES, map_cwe_to_major
from ..utils.response_parsing import (
    extract_cwe_major,
    extract_vulnerability_label,
)


class VulnerabilityDetectionWorkflow:
    """Complete workflow for vulnerability detection prompt optimization."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.setup_directories()
        self.setup_logging()
        self.setup_prompt_tracker()
        # CWE大类模式开关
        self.use_cwe_major = bool(self.config.get("use_cwe_major", False))
        
    def setup_directories(self):
        """Setup output directories."""
        self.output_dir = Path(self.config.get("output_dir", "./outputs/vulnerability_detection"))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create experiment ID
        self.experiment_id = self.config.get("experiment_id") or f"vuldet_{int(time.time())}"
        self.exp_dir = self.output_dir / self.experiment_id
        self.exp_dir.mkdir(exist_ok=True)
        
    def setup_logging(self):
        """Setup logging."""
        self.logger = EvolutionLogger(
            output_dir=str(self.output_dir),
            experiment_id=self.experiment_id
        )
        
    def setup_prompt_tracker(self):
        """Setup prompt tracking."""
        self.prompt_tracker = PromptTracker(
            output_dir=str(self.output_dir),
            experiment_id=self.experiment_id
        )
        self.prompt_tracker.set_config(self.config)
        
    def prepare_data(self) -> tuple:
        """Prepare dataset for training."""
        dataset_name = self.config["dataset"]
        data_dir = self.config.get("data_dir", f"./data/{dataset_name}")
        
        self.logger.info(f"Preparing {dataset_name} dataset...")
        
        if dataset_name.lower() == "primevul":
            # Convert Primevul JSONL to tab-separated format
            primevul_dir = self.config.get("primevul_dir", "./data/primevul/primevul")
            output_data_dir = f"./data/vul_detection/primevul"
            
            dev_file, test_file = prepare_primevul_data(primevul_dir, output_data_dir)
            self.logger.info(f"Prepared Primevul data: dev={dev_file}, test={test_file}")
            
        else:
            # Use existing data structure
            dev_file = self.config.get("dev_file") or f"{data_dir}/dev.txt"
            test_file = self.config.get("test_file") or f"{data_dir}/test.txt"
        
        # Create datasets
        dev_dataset = create_dataset(dataset_name, dev_file, "dev")
        test_dataset = create_dataset(dataset_name, test_file, "test")
        
        self.logger.info(f"Loaded datasets: dev={len(dev_dataset)}, test={len(test_dataset)}")
        
        return dev_dataset, test_dataset
        
    def create_components(self, dev_dataset):
        """Create evolution components."""
        # Create LLM client (SVEN-compatible)
        llm_client = create_llm_client(
            llm_type=self.config.get("llm_type", "sven")
        )
        
        # Create metric
        metric = AccuracyMetric()
        
        # filled prompts 路径
        filled_prompts_file = str(self.exp_dir / "filled_prompts.jsonl")

        # Create evaluator with prompt tracking
        evaluator = VulnerabilityEvaluator(
            dataset=dev_dataset,
            metric=metric,
            llm_client=llm_client,
            prompt_tracker=self.prompt_tracker,
            sample_size=self.config.get("sample_size", 100),
            use_cwe_major=self.use_cwe_major,
            filled_prompts_file=filled_prompts_file
        )
        
        # Create algorithm
        algorithm_type = self.config.get("algorithm", "de").lower()
        algorithm_config = {
            "population_size": self.config.get("population_size", 20),
            "max_generations": self.config.get("max_generations", 10),
            "mutation_rate": self.config.get("mutation_rate", 0.1),
        }
        
        if algorithm_type == "de":
            algorithm = DifferentialEvolution(algorithm_config)
        elif algorithm_type == "ga":
            algorithm_config["crossover_rate"] = self.config.get("crossover_rate", 0.8)
            algorithm = GeneticAlgorithm(algorithm_config)
        else:
            raise ValueError(f"Unknown algorithm: {algorithm_type}")
            
        self.logger.info(f"Created {algorithm_type.upper()} algorithm with config: {algorithm_config}")
        
        return llm_client, evaluator, algorithm
        
    def get_initial_prompts(self) -> List[str]:
        """Get initial prompts for vulnerability detection."""
        # Default vulnerability detection prompts
        default_prompts = [
            "Analyze the following code and determine if it contains a security vulnerability. Respond with 'vulnerable' if there is a vulnerability, or 'benign' if the code is safe:\n\nCode: {input}\n\nAnalysis:",
            
            "You are a security expert. Examine this code snippet for potential vulnerabilities. Answer 'vulnerable' if you find security issues, 'benign' otherwise:\n\n{input}\n\nYour assessment:",
            
            "Review the code below for security vulnerabilities such as buffer overflows, injection attacks, or other security flaws. Respond 'vulnerable' or 'benign':\n\n{input}\n\nSecurity assessment:",
            
            "As a cybersecurity analyst, evaluate this code for security vulnerabilities. Classify as 'vulnerable' if unsafe, 'benign' if secure:\n\nCode to analyze:\n{input}\n\nClassification:",
            
            "Perform a security analysis on the following code. Look for common vulnerabilities and respond with 'vulnerable' if found, 'benign' if not:\n\n{input}\n\nResult:"
        ]
        
        # 如果启用CWE大类模式，则在尾部固定一条只输出大类的要求
        if self.use_cwe_major:
            suffix = (
                "\n\nNow, ONLY output the CWE major category from the following list that best matches the vulnerability, "
                "or 'Benign' if no vulnerability: "
                + ", ".join(CWE_MAJOR_CATEGORIES)
                + ". Output just the category name."
            )
            default_prompts = [p + suffix for p in default_prompts]
        
        # Try to load custom prompts if specified   
        custom_prompts_file = self.config.get("initial_prompts_file")
        if custom_prompts_file and os.path.exists(custom_prompts_file):
            try:
                with open(custom_prompts_file, 'r', encoding='utf-8') as f:
                    custom_prompts = [line.strip() for line in f if line.strip()]
                if custom_prompts:
                    self.logger.info(f"Loaded {len(custom_prompts)} custom initial prompts")
                    # 同样追加固定尾句
                    if self.use_cwe_major:
                        custom_prompts = [p + suffix for p in custom_prompts]
                    return custom_prompts
            except Exception as e:
                self.logger.warning(f"Failed to load custom prompts: {e}")
        
        self.logger.info(f"Using {len(default_prompts)} default initial prompts")
        return default_prompts
        
    def run_evolution(self) -> Dict[str, Any]:
        """Run the complete evolution workflow."""
        self.logger.info("Starting vulnerability detection prompt evolution...")
        self.logger.info(f"Experiment ID: {self.experiment_id}")
        # 保存 meta prompt
        if self.config.get("meta_prompt"):
            meta_path = self.exp_dir / "meta_prompt.txt"
            try:
                with open(meta_path, 'w', encoding='utf-8') as f:
                    f.write(str(self.config["meta_prompt"]).strip() + '\n')
            except Exception as e:
                self.logger.warning(f"Failed to save meta prompt: {e}")
        # Prepare data
        dev_dataset, test_dataset = self.prepare_data()
        
        # Create components
        llm_client, evaluator, algorithm = self.create_components(dev_dataset)
        
        # Get initial prompts
        initial_prompts = self.get_initial_prompts()
        
        # Log initial prompts
        for i, prompt in enumerate(initial_prompts):
            self.prompt_tracker.log_prompt(
                prompt=prompt,
                generation=0,
                individual_id=f"init_{i}",
                operation="initialization"
            )
        
        # Create evolution engine
        engine = EvolutionEngine(
            algorithm=algorithm,
            evaluator=evaluator,
            llm_client=llm_client,
            config=self.config
        )
        
        # Run evolution
        self.logger.info("Starting evolution process...")
        results = engine.evolve(initial_prompts=initial_prompts)
        
        # Save results
        self.save_results(results, test_dataset, llm_client)
        
        return results
        
    def save_results(self, results: Dict[str, Any], test_dataset, llm_client):
        """Save evolution results."""
        # 再次确保 meta prompt 单独保存，可冗余
        if self.config.get("meta_prompt"):
            meta_path = self.exp_dir / "meta_prompt.txt"
            try:
                with open(meta_path, 'w', encoding='utf-8') as f:
                    f.write(str(self.config["meta_prompt"]).strip() + '\n')
            except Exception as e:
                self.logger.warning(f"Failed to save meta prompt: {e}")
        # Save prompt tracker summary
        self.prompt_tracker.save_summary(results)
        
        # Export top prompts
        top_prompts_file = self.exp_dir / "top_prompts.txt"
        self.prompt_tracker.export_prompts_by_fitness(str(top_prompts_file), top_k=10)
        
        # Evaluate best prompt on test set
        if results.get("best_prompt"):
            self.evaluate_on_test_set(results["best_prompt"], test_dataset, llm_client)
            
        # Save final results
        results_file = self.exp_dir / "final_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"Results saved to {self.exp_dir}")
        
    def evaluate_on_test_set(self, best_prompt: str, test_dataset, llm_client):
        """Evaluate best prompt on test set."""
        self.logger.info("Evaluating best prompt on test set...")
        
        evaluator = VulnerabilityEvaluator(
            dataset=test_dataset,
            metric=AccuracyMetric(),
            llm_client=llm_client,
            prompt_tracker=None,  # Don't track test evaluation
            sample_size=self.config.get("test_sample_size", 200),
            use_cwe_major=self.use_cwe_major,
        )
        
        result = evaluator.evaluate(best_prompt)
        self.logger.info(f"Test set performance: {result.score:.4f}")
        
        # Save test results
        test_results = {
            "prompt": best_prompt,
            "test_accuracy": result.score,
            "test_details": result.details
        }
        
        test_file = self.exp_dir / "test_results.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)


class VulnerabilityEvaluator(Evaluator):
    """Specialized evaluator for vulnerability detection with prompt tracking and filled prompt recording."""
    
    def __init__(
        self,
        dataset,
        metric,
        llm_client,
        prompt_tracker: Optional[PromptTracker] = None,
        sample_size: int = 100,
        use_cwe_major: bool = False,
        filled_prompts_file: Optional[str] = None  # 新增
    ):
        super().__init__(dataset, metric, llm_client)
        self.prompt_tracker = prompt_tracker
        self.sample_size = sample_size
        self.evaluation_count = 0
        self.use_cwe_major = use_cwe_major
        self.filled_prompts_file = filled_prompts_file
        
    def evaluate(self, prompt: str, generation: int = 0) -> EvaluationResult:
        """Evaluate prompt with tracking and记录所有filled prompt实例."""
        self.evaluation_count += 1
        # Get samples
        samples = self.dataset.get_samples(self.sample_size)
        predictions = []
        targets = []
        filled_examples = []
        for idx, sample in enumerate(samples):
            # Format prompt
            formatted_prompt = self._format_vulnerability_prompt(prompt, sample.input_text)
            instance = {
                "template": prompt,
                "filled": formatted_prompt,
                "sample_id": getattr(sample, 'id', idx),
                "generation": generation,
                "target": getattr(sample, 'target', None)
            }
            filled_examples.append(instance)
            # Get prediction
            try:
                response = self.llm_client.generate(formatted_prompt, max_tokens=16, temperature=0.1)
                if self.use_cwe_major:
                    prediction = extract_cwe_major(response)
                else:
                    prediction = extract_vulnerability_label(response)
            except Exception as e:
                print(f"LLM error: {e}")
                prediction = "Benign" if self.use_cwe_major else "0"
            predictions.append(prediction)
            # 目标
            if self.use_cwe_major:
                if str(sample.target) == "1":
                    cwe_codes = sample.metadata.get('cwe', []) if hasattr(sample, 'metadata') else []
                    major = map_cwe_to_major(cwe_codes) if cwe_codes else "Other"
                    targets.append(major)
                else:
                    targets.append("Benign")
            else:
                targets.append(sample.target)
        # 写入filled prompt实例
        if self.filled_prompts_file and filled_examples:
            try:
                with open(self.filled_prompts_file, 'a', encoding='utf-8') as f:
                    for item in filled_examples:
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
            except Exception as e:
                print(f"⚠️ 填充prompt样例保存失败: {e}")
        # Calculate score
        score = self.metric.compute(predictions, targets)
        # Log to tracker if available
        if self.prompt_tracker:
            self.prompt_tracker.log_prompt(
                prompt=prompt,
                fitness=score,
                generation=generation,
                individual_id=f"eval_{self.evaluation_count}",
                operation="evaluation",
                metadata={
                    "sample_size": len(samples),
                    "predictions_sample": predictions[:5],
                    "targets_sample": targets[:5]
                }
            )
        return EvaluationResult(
            score=score,
            details={
                "num_samples": len(samples),
                "predictions": predictions[:10],
                "targets": targets[:10],
                "accuracy": score
            }
        )
        
    def _format_vulnerability_prompt(self, prompt: str, code: str) -> str:
        """Format prompt for vulnerability detection."""
        return prompt.replace("{input}", code)
        

def run_vulnerability_detection_workflow(config_file: str = None, **kwargs) -> Dict[str, Any]:
    """Run vulnerability detection workflow with configuration."""
    
    # Default configuration
    default_config = {
        "dataset": "primevul",
        "algorithm": "de",
        "population_size": 20,
        "max_generations": 10,
        "mutation_rate": 0.1,
        "llm_type": "gpt-3.5-turbo",
        "sample_size": 100,
        "test_sample_size": 200,
        "output_dir": "./outputs/vulnerability_detection",
    }
    
    # Load config from file if provided
    if config_file and os.path.exists(config_file):
        with open(config_file, 'r') as f:
            file_config = json.load(f)
        default_config.update(file_config)
    
    # Override with kwargs
    default_config.update(kwargs)
    
    # Create and run workflow
    workflow = VulnerabilityDetectionWorkflow(default_config)
    return workflow.run_evolution()


if __name__ == "__main__":
    # Example usage
    config = {
        "dataset": "primevul",
        "algorithm": "de",
        "population_size": 10,
        "max_generations": 5,
        "sample_size": 50,
        "llm_type": "sven",
        "primevul_dir": "./data/primevul/primevul"
    }
    
    results = run_vulnerability_detection_workflow(**config)
    print(f"Evolution completed! Best fitness: {results.get('best_fitness', 'N/A')}")