"""Complete workflow for vulnerability detection using EvoPrompt."""

import os
import json
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

from ..core.evolution import EvolutionEngine
from ..core.evaluator import Evaluator, EvaluationResult
from ..core.prompt_tracker import PromptTracker, EvolutionLogger
from ..data.dataset import create_dataset, prepare_primevul_data
from ..algorithms.differential import DifferentialEvolution
from ..algorithms.genetic import GeneticAlgorithm
from ..llm.client import create_llm_client
from ..metrics.base import AccuracyMetric


class VulnerabilityDetectionWorkflow:
    """Complete workflow for vulnerability detection prompt optimization."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.setup_directories()
        self.setup_logging()
        self.setup_prompt_tracker()
        
    def setup_directories(self):
        """Setup output directories."""
        self.output_dir = Path(self.config.get("output_dir", "./outputs/vulnerability_detection"))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create experiment ID
        self.experiment_id = self.config.get("experiment_id") or f"vuldet_{int(time.time())}"
        self.exp_dir = self.output_dir / self.experiment_id
        self.exp_dir.mkdir(exist_ok=True)
        
    def setup_logging(self):
        """Setup logging."""
        self.logger = EvolutionLogger(
            output_dir=str(self.output_dir),
            experiment_id=self.experiment_id
        )
        
    def setup_prompt_tracker(self):
        """Setup prompt tracking."""
        self.prompt_tracker = PromptTracker(
            output_dir=str(self.output_dir),
            experiment_id=self.experiment_id
        )
        self.prompt_tracker.set_config(self.config)
        
    def prepare_data(self) -> tuple:
        """Prepare dataset for training."""
        dataset_name = self.config["dataset"]
        data_dir = self.config.get("data_dir", f"./data/{dataset_name}")
        
        self.logger.info(f"Preparing {dataset_name} dataset...")
        
        if dataset_name.lower() == "primevul":
            # Convert Primevul JSONL to tab-separated format
            primevul_dir = self.config.get("primevul_dir", "./data/primevul/primevul")
            output_data_dir = f"./data/vul_detection/primevul"
            
            dev_file, test_file = prepare_primevul_data(primevul_dir, output_data_dir)
            self.logger.info(f"Prepared Primevul data: dev={dev_file}, test={test_file}")
            
        else:
            # Use existing data structure
            dev_file = self.config.get("dev_file") or f"{data_dir}/dev.txt"
            test_file = self.config.get("test_file") or f"{data_dir}/test.txt"
        
        # Create datasets
        dev_dataset = create_dataset(dataset_name, dev_file, "dev")
        test_dataset = create_dataset(dataset_name, test_file, "test")
        
        self.logger.info(f"Loaded datasets: dev={len(dev_dataset)}, test={len(test_dataset)}")
        
        return dev_dataset, test_dataset
        
    def create_components(self, dev_dataset):
        """Create evolution components."""
        # Create LLM client (SVEN-compatible)
        llm_client = create_llm_client(
            llm_type=self.config.get("llm_type", "sven")
        )
        
        # Create metric
        metric = AccuracyMetric()
        
        # Create evaluator with prompt tracking
        evaluator = VulnerabilityEvaluator(
            dataset=dev_dataset,
            metric=metric,
            llm_client=llm_client,
            prompt_tracker=self.prompt_tracker,
            sample_size=self.config.get("sample_size", 100)
        )
        
        # Create algorithm
        algorithm_type = self.config.get("algorithm", "de").lower()
        algorithm_config = {
            "population_size": self.config.get("population_size", 20),
            "max_generations": self.config.get("max_generations", 10),
            "mutation_rate": self.config.get("mutation_rate", 0.1),
        }
        
        if algorithm_type == "de":
            algorithm = DifferentialEvolution(algorithm_config)
        elif algorithm_type == "ga":
            algorithm_config["crossover_rate"] = self.config.get("crossover_rate", 0.8)
            algorithm = GeneticAlgorithm(algorithm_config)
        else:
            raise ValueError(f"Unknown algorithm: {algorithm_type}")
            
        self.logger.info(f"Created {algorithm_type.upper()} algorithm with config: {algorithm_config}")
        
        return llm_client, evaluator, algorithm
        
    def get_initial_prompts(self) -> List[str]:
        """Get initial prompts for vulnerability detection."""
        # Default vulnerability detection prompts
        default_prompts = [
            "Analyze the following code and determine if it contains a security vulnerability. Respond with 'vulnerable' if there is a vulnerability, or 'benign' if the code is safe:\\n\\nCode: {input}\\n\\nAnalysis:",
            
            "You are a security expert. Examine this code snippet for potential vulnerabilities. Answer 'vulnerable' if you find security issues, 'benign' otherwise:\\n\\n{input}\\n\\nYour assessment:",
            
            "Review the code below for security vulnerabilities such as buffer overflows, injection attacks, or other security flaws. Respond 'vulnerable' or 'benign':\\n\\n{input}\\n\\nSecurity assessment:",
            
            "As a cybersecurity analyst, evaluate this code for security vulnerabilities. Classify as 'vulnerable' if unsafe, 'benign' if secure:\\n\\nCode to analyze:\\n{input}\\n\\nClassification:",
            
            "Perform a security analysis on the following code. Look for common vulnerabilities and respond with 'vulnerable' if found, 'benign' if not:\\n\\n{input}\\n\\nResult:"
        ]
        
        # Try to load custom prompts if specified   
        custom_prompts_file = self.config.get("initial_prompts_file")
        if custom_prompts_file and os.path.exists(custom_prompts_file):
            try:
                with open(custom_prompts_file, 'r', encoding='utf-8') as f:
                    custom_prompts = [line.strip() for line in f if line.strip()]
                if custom_prompts:
                    self.logger.info(f"Loaded {len(custom_prompts)} custom initial prompts")
                    return custom_prompts
            except Exception as e:
                self.logger.warning(f"Failed to load custom prompts: {e}")
        
        self.logger.info(f"Using {len(default_prompts)} default initial prompts")
        return default_prompts
        
    def run_evolution(self) -> Dict[str, Any]:
        """Run the complete evolution workflow."""
        self.logger.info("Starting vulnerability detection prompt evolution...")
        self.logger.info(f"Experiment ID: {self.experiment_id}")
        
        # Prepare data
        dev_dataset, test_dataset = self.prepare_data()
        
        # Create components
        llm_client, evaluator, algorithm = self.create_components(dev_dataset)
        
        # Get initial prompts
        initial_prompts = self.get_initial_prompts()
        
        # Log initial prompts
        for i, prompt in enumerate(initial_prompts):
            self.prompt_tracker.log_prompt(
                prompt=prompt,
                generation=0,
                individual_id=f"init_{i}",
                operation="initialization"
            )
        
        # Create evolution engine
        engine = EvolutionEngine(
            algorithm=algorithm,
            evaluator=evaluator,
            llm_client=llm_client,
            config=self.config
        )
        
        # Run evolution
        self.logger.info("Starting evolution process...")
        results = engine.evolve(initial_prompts=initial_prompts)
        
        # Save results
        self.save_results(results, test_dataset, llm_client)
        
        return results
        
    def save_results(self, results: Dict[str, Any], test_dataset, llm_client):
        """Save evolution results."""
        # Save prompt tracker summary
        self.prompt_tracker.save_summary(results)
        
        # Export top prompts
        top_prompts_file = self.exp_dir / "top_prompts.txt"
        self.prompt_tracker.export_prompts_by_fitness(str(top_prompts_file), top_k=10)
        
        # Evaluate best prompt on test set
        if results.get("best_prompt"):
            self.evaluate_on_test_set(results["best_prompt"], test_dataset, llm_client)
            
        # Save final results
        results_file = self.exp_dir / "final_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"Results saved to {self.exp_dir}")
        
    def evaluate_on_test_set(self, best_prompt: str, test_dataset, llm_client):
        """Evaluate best prompt on test set."""
        self.logger.info("Evaluating best prompt on test set...")
        
        evaluator = VulnerabilityEvaluator(
            dataset=test_dataset,
            metric=AccuracyMetric(),
            llm_client=llm_client,
            prompt_tracker=None,  # Don't track test evaluation
            sample_size=self.config.get("test_sample_size", 200)
        )
        
        result = evaluator.evaluate(best_prompt)
        self.logger.info(f"Test set performance: {result.score:.4f}")
        
        # Save test results
        test_results = {
            "prompt": best_prompt,
            "test_accuracy": result.score,
            "test_details": result.details
        }
        
        test_file = self.exp_dir / "test_results.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)


class VulnerabilityEvaluator(Evaluator):
    """Specialized evaluator for vulnerability detection with prompt tracking."""
    
    def __init__(
        self,
        dataset,
        metric,
        llm_client,
        prompt_tracker: Optional[PromptTracker] = None,
        sample_size: int = 100
    ):
        super().__init__(dataset, metric, llm_client)
        self.prompt_tracker = prompt_tracker
        self.sample_size = sample_size
        self.evaluation_count = 0
        
    def evaluate(self, prompt: str, generation: int = 0) -> EvaluationResult:
        """Evaluate prompt with tracking."""
        self.evaluation_count += 1
        
        # Get samples
        samples = self.dataset.get_samples(self.sample_size)
        predictions = []
        targets = []
        
        for sample in samples:
            # Format prompt
            formatted_prompt = self._format_vulnerability_prompt(prompt, sample.input_text)
            
            # Get prediction
            try:
                response = self.llm_client.generate(formatted_prompt, max_tokens=10, temperature=0.1)
                prediction = self._extract_vulnerability_label(response)
            except Exception as e:
                print(f"LLM error: {e}")
                prediction = "benign"  # Default to benign on error
                
            predictions.append(prediction)
            targets.append(sample.target)
            
        # Calculate score
        score = self.metric.compute(predictions, targets)
        
        # Log to tracker if available
        if self.prompt_tracker:
            self.prompt_tracker.log_prompt(
                prompt=prompt,
                fitness=score,
                generation=generation,
                individual_id=f"eval_{self.evaluation_count}",
                operation="evaluation",
                metadata={
                    "sample_size": len(samples),
                    "predictions_sample": predictions[:5],
                    "targets_sample": targets[:5]
                }
            )
        
        return EvaluationResult(
            score=score,
            details={
                "num_samples": len(samples),
                "predictions": predictions[:10],
                "targets": targets[:10],
                "accuracy": score
            }
        )
        
    def _format_vulnerability_prompt(self, prompt: str, code: str) -> str:
        """Format prompt for vulnerability detection."""
        return prompt.replace("{input}", code)
        
    def _extract_vulnerability_label(self, response: str) -> str:
        """Extract vulnerability label from LLM response."""
        response = response.lower().strip()
        
        if "vulnerable" in response:
            return "1"
        elif "benign" in response:
            return "0"
        else:
            # Default based on first character or keyword
            if response.startswith("1") or "yes" in response:
                return "1"
            else:
                return "0"


def run_vulnerability_detection_workflow(config_file: str = None, **kwargs) -> Dict[str, Any]:
    """Run vulnerability detection workflow with configuration."""
    
    # Default configuration
    default_config = {
        "dataset": "primevul",
        "algorithm": "de",
        "population_size": 20,
        "max_generations": 10,
        "mutation_rate": 0.1,
        "llm_type": "gpt-3.5-turbo",
        "sample_size": 100,
        "test_sample_size": 200,
        "output_dir": "./outputs/vulnerability_detection",
    }
    
    # Load config from file if provided
    if config_file and os.path.exists(config_file):
        with open(config_file, 'r') as f:
            file_config = json.load(f)
        default_config.update(file_config)
    
    # Override with kwargs
    default_config.update(kwargs)
    
    # Create and run workflow
    workflow = VulnerabilityDetectionWorkflow(default_config)
    return workflow.run_evolution()


if __name__ == "__main__":
    # Example usage
    config = {
        "dataset": "primevul",
        "algorithm": "de",
        "population_size": 10,
        "max_generations": 5,
        "sample_size": 50,
        "llm_type": "sven",
        "primevul_dir": "./data/primevul/primevul"
    }
    
    results = run_vulnerability_detection_workflow(**config)
    print(f"Evolution completed! Best fitness: {results.get('best_fitness', 'N/A')}")